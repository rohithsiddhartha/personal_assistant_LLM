{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Max length 512\n",
      "Embeddings shape: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_embeddings(texts, model_name='bert-base-uncased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    max_length = tokenizer.model_max_length\n",
    "    print(\"Model Max length\", max_length)\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize and split text into chunks\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=False)\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "\n",
    "        # Create chunks of max_length\n",
    "        chunks = [input_ids[i:i + max_length] for i in range(0, len(input_ids), max_length)]\n",
    "\n",
    "        chunk_embeddings = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_input = {'input_ids': chunk.unsqueeze(0)}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**chunk_input)\n",
    "            chunk_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "            chunk_embeddings.append(chunk_embedding)\n",
    "\n",
    "        # Combine chunk embeddings\n",
    "        combined_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "        all_embeddings.append(combined_embedding)\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    texts = [\"This is a long text that needs to be tokenized and embedded without truncation.\"]\n",
    "    embeddings = get_embeddings(texts)\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Max length 512\n",
      "Embeddings shape: (1, 768)\n",
      "Decoded text: ['this is a long text that needs to be tokenized and embedded without truncation.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_embeddings(texts, model_name='bert-base-uncased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    max_length = tokenizer.model_max_length\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_decoded_texts = []\n",
    "    print(\"Model Max length\", max_length)\n",
    "\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize and split text into chunks\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=False)\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "\n",
    "        # Create chunks of max_length\n",
    "        chunks = [input_ids[i:i + max_length] for i in range(0, len(input_ids), max_length)]\n",
    "\n",
    "        chunk_embeddings = []\n",
    "        chunk_texts = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_input = {'input_ids': chunk.unsqueeze(0)}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**chunk_input)\n",
    "            chunk_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "            chunk_embeddings.append(chunk_embedding)\n",
    "\n",
    "            # Decode the chunk back to text\n",
    "            decoded_chunk = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "            chunk_texts.append(decoded_chunk)\n",
    "\n",
    "        # Combine chunk embeddings\n",
    "        combined_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "        all_embeddings.append(combined_embedding)\n",
    "        all_decoded_texts.append(' '.join(chunk_texts))\n",
    "\n",
    "    return np.vstack(all_embeddings), all_decoded_texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    texts = [\"This is a long text that needs to be tokenized and embedded without truncation.\"]\n",
    "    embeddings, decoded_texts = get_embeddings(texts)\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "    print(\"Decoded text:\", decoded_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Max length: 512\n",
      "[tensor([  101,  2023,  2003,  1037,  2146,  3793,  2008,  3791,  2000,  2022,\n",
      "        19204,  3550,  1998, 11157,  2302, 19817,  4609, 10719,  1012,  9345,\n",
      "        19771,  1024,  2875,  2488,  9312,  3001,  2005,  9932,  6074,  6904,\n",
      "        23736,  2243,  1024,  2019,  3784, 12317, 15756,  2897,  3559,  2079,\n",
      "         7526, 16913, 11475,  7368,  2191,  1058, 19062,  4275,  2062, 21425,\n",
      "         2000,  1037,  2529,  1029, 23208,  5107,  4512,  2389,  6074,  3081,\n",
      "        10791,  2529,  1011,  9932,  2399,  2009,  3138,  2048,  2000, 17609,\n",
      "         1024,  2875,  3399,  1997,  9932,  1005,  1055,  2568,   102])]\n",
      "Embeddings shape: (1, 768)\n",
      "Embeddings: [[-2.89555043e-01  5.51846884e-02  1.08932123e-01 -4.20793444e-02\n",
      "   1.14392862e-01  5.98948859e-02  2.37213328e-01  3.37278917e-02\n",
      "   6.56338260e-02 -2.67120004e-01  6.07445017e-02 -5.81723712e-02\n",
      "  -2.59414941e-01 -3.88526917e-02  9.47184116e-02  4.02795434e-01\n",
      "   5.88367647e-03 -1.34157985e-02 -2.71942079e-01  2.05526903e-01\n",
      "   1.98624402e-01  4.14048940e-01  1.50652593e-02  5.35788536e-01\n",
      "   4.11051482e-01 -3.64426710e-02  8.86803493e-02  2.19370082e-01\n",
      "  -1.18666276e-01 -7.42021427e-02  3.01478118e-01  3.31757396e-01\n",
      "  -1.75395295e-01 -4.81991649e-01 -2.53675073e-01  4.88422439e-02\n",
      "  -2.32545301e-01 -1.21280022e-01 -3.79482135e-02  3.06622088e-01\n",
      "  -6.27159357e-01 -7.92460963e-02 -1.79929644e-01  1.62276804e-01\n",
      "  -2.88527429e-01 -3.12600255e-01  1.83397550e-02 -2.36705154e-01\n",
      "   2.32627138e-01 -1.53303370e-01 -8.84912252e-01  2.78207690e-01\n",
      "  -9.79594588e-02  5.37734628e-02  2.25072652e-02  7.13398337e-01\n",
      "   2.91931748e-01 -5.67767501e-01 -1.79288730e-01 -2.32106119e-01\n",
      "   3.11843324e-02 -6.39884546e-02  1.63648531e-01 -2.76526928e-01\n",
      "   2.82575693e-02  7.18692169e-02  2.06678547e-02  3.01431984e-01\n",
      "  -7.10697114e-01  2.14109376e-01 -3.58619094e-01 -1.20548576e-01\n",
      "  -3.54522169e-01 -6.89242175e-03 -5.11453450e-01 -1.00288466e-01\n",
      "  -2.32523344e-02  3.07401478e-01  2.87990998e-02 -2.13905618e-01\n",
      "  -3.20671946e-01  1.50620699e-01 -1.54223979e-01  2.32482672e-01\n",
      "   4.12449509e-01  2.90184289e-01  7.99146742e-02  4.84123230e-01\n",
      "  -4.70458597e-01  2.31693342e-01  9.03746337e-02 -3.04723471e-01\n",
      "   4.91868034e-02  1.07230224e-01  6.74561262e-01  6.13102317e-02\n",
      "   1.17806591e-01  1.76554695e-01 -9.78405699e-02  3.15079451e-01\n",
      "   2.94280231e-01 -4.33531225e-01  1.59697592e-01  6.18270077e-02\n",
      "  -2.03044161e-01 -1.48851678e-01  9.46789142e-03  5.88586293e-02\n",
      "  -1.01567730e-01  2.48021305e-01  1.57012910e-01 -5.22598252e-02\n",
      "   3.66946161e-02 -4.53795999e-01 -1.70711994e-01 -1.88222472e-02\n",
      "  -7.96103328e-02  2.12872997e-01  1.15403645e-01  1.04632705e-01\n",
      "   1.78685457e-01  3.55067849e-01  1.78371072e-01  1.02905953e+00\n",
      "   3.44721571e-04  1.36525318e-01 -1.77698314e-01  2.87120163e-01\n",
      "   2.36960202e-01 -1.67587876e-01  3.05001915e-01  3.21725667e-01\n",
      "   2.17464715e-01 -2.40246087e-01 -2.07785890e-01  2.53534764e-01\n",
      "  -5.20270132e-02 -1.76719412e-01 -4.58178788e-01  5.86025640e-02\n",
      "  -3.15583527e-01 -2.58217871e-01  5.52150786e-01  1.28988653e-01\n",
      "   3.40468794e-01  4.57251593e-02 -1.03474341e-01 -6.47897646e-02\n",
      "   1.29866019e-01  1.51677400e-01 -2.90419022e-03  9.88170132e-02\n",
      "  -2.15673104e-01 -2.70682603e-01 -2.57846028e-01  2.96531111e-01\n",
      "  -9.85588953e-02  1.89201623e-01 -1.23312816e-01  4.64162901e-02\n",
      "   2.04684421e-01  2.67432258e-02 -1.94451809e-01  5.65588009e-03\n",
      "   5.41910939e-02 -2.85776615e-01  9.11672115e-02  4.82621461e-01\n",
      "   2.29616910e-01  7.86388367e-02  8.36868435e-02 -3.17285478e-01\n",
      "   5.55369496e-01 -1.33812606e-01  1.65954363e-02 -1.84324533e-01\n",
      "   2.67470181e-01 -1.41374275e-01  2.62599677e-01  2.91293353e-01\n",
      "  -7.42504299e-01  3.46714228e-01 -6.47065267e-02 -2.23659292e-01\n",
      "   1.49828672e-01 -1.24712162e-01  2.18529493e-01 -3.90754968e-01\n",
      "  -6.91603944e-02 -1.11103073e-01 -2.08348721e-01 -1.55129299e-01\n",
      "  -3.75047684e-01 -6.67880848e-02  5.10751188e-01 -1.69123128e-01\n",
      "  -2.37702280e-01  1.35983437e-01 -2.67957628e-01  1.02751032e-02\n",
      "   6.35321587e-02 -2.06456020e-01  1.77867357e-02  1.99545696e-02\n",
      "  -1.06642962e-01 -4.55825984e-01 -1.58778578e-01 -3.65638286e-01\n",
      "  -6.19723022e-01  1.35455042e-01 -3.68557721e-01  3.83307070e-01\n",
      "   6.08005337e-02 -1.75835833e-01  8.75987113e-02  1.62810087e-01\n",
      "  -9.81901586e-02  2.73909837e-01  9.34443921e-02 -2.63884999e-02\n",
      "   3.10193241e-01 -9.42136422e-02 -5.54919958e-01  4.01060998e-01\n",
      "  -1.61182448e-01  8.23977113e-01  4.86005813e-01 -4.63676453e-01\n",
      "   7.34786868e-01  1.83528095e-01  1.49161309e-01 -2.04885006e-01\n",
      "   4.31407303e-01 -2.27333292e-01 -6.94147050e-02  5.15899546e-02\n",
      "  -1.10192552e-01 -2.46221110e-01  2.40938976e-01 -9.17914212e-02\n",
      "  -2.16959164e-01  3.79398406e-01  3.92536938e-01  1.37147158e-01\n",
      "   1.35460952e-02 -4.90641683e-01 -1.44580737e-01  1.77257374e-01\n",
      "  -2.16978684e-01 -1.87570512e-01 -7.01228082e-01 -8.84969085e-02\n",
      "  -2.28106678e-01 -3.55607152e-01 -1.47578925e-01 -7.66443685e-02\n",
      "  -2.21362144e-01 -3.07564735e-01 -2.32100114e-01  5.54043055e-01\n",
      "   2.26728484e-01  3.57591003e-01 -6.91071805e-03  3.23390646e-04\n",
      "  -1.88610941e-01 -5.89301527e-01 -1.76687658e-01  4.31329152e-03\n",
      "   6.66644037e-01  3.32077354e-01  1.81599170e-01 -4.46592510e-01\n",
      "   1.34676835e-02  4.46964353e-01 -2.43292898e-01 -4.43484932e-01\n",
      "   1.89269394e-01  7.19339848e-02 -1.68867022e-01  7.66637623e-02\n",
      "   4.55144309e-02  4.53896970e-01 -3.69624764e-01 -2.98440945e-03\n",
      "   6.00937940e-03 -2.80986965e-01  3.85834068e-01  3.36950785e-03\n",
      "  -1.95978582e-01 -2.11259380e-01 -3.79575305e-02  2.94163018e-01\n",
      "  -5.12514293e-01 -2.58655071e-01  3.40180367e-01  4.22318608e-01\n",
      "   3.14467669e-01  1.10553004e-01  1.34067535e-01 -3.29325229e-01\n",
      "  -2.68759698e-01 -4.18699384e-01  1.79054946e-01  1.68328807e-01\n",
      "  -2.46438712e-01  1.35950953e-01 -7.67418519e-02 -2.34360591e-01\n",
      "  -4.22653389e+00 -1.51689246e-01  2.39961654e-01 -4.10124175e-02\n",
      "   1.14473730e-01  7.49004334e-02 -3.52061652e-02 -2.57994622e-01\n",
      "  -3.04427713e-01  9.50686708e-02 -3.01378191e-01 -2.54530132e-01\n",
      "  -2.17106223e-01  3.76822650e-01  1.41782716e-01  2.86865905e-02\n",
      "  -6.85332045e-02 -2.72140592e-01 -2.06532568e-01  3.20578188e-01\n",
      "  -1.95873886e-01 -4.03577477e-01 -3.91336763e-03  6.91931620e-02\n",
      "   5.29947178e-03  1.93015158e-01 -2.83779889e-01  3.57041787e-03\n",
      "  -5.05813994e-02 -1.59502119e-01 -2.53321249e-02 -1.82580665e-01\n",
      "  -3.45385522e-02  2.05425695e-01  2.28802904e-01  2.38997281e-01\n",
      "  -1.16216898e-01 -1.53447360e-01  6.80320486e-02 -1.82115838e-01\n",
      "  -6.96620196e-02 -1.71670422e-01  2.40227729e-01 -9.10336226e-02\n",
      "   6.30149543e-01 -4.39313948e-01 -5.53408377e-02  5.60132776e-06\n",
      "   1.17123351e-01 -1.12891935e-01 -1.82607859e-01  6.58741146e-02\n",
      "  -2.04057425e-01  2.02620421e-02 -1.75017212e-02 -1.90164536e-01\n",
      "   5.17671347e-01  3.57096016e-01 -1.60371270e-02 -1.99416131e-01\n",
      "   2.76422441e-01 -2.63567805e-01 -2.11307406e-01 -4.12696838e-01\n",
      "  -5.89414500e-02 -9.76720974e-02 -6.78454697e-01 -4.48525935e-01\n",
      "   1.59668289e-02 -7.91402012e-02 -3.46565157e-01  2.84229189e-01\n",
      "  -4.92318839e-01 -7.08670199e-01 -3.93305570e-02 -3.87269557e-01\n",
      "   1.76250771e-01 -2.81550020e-01  2.50916213e-01 -9.41010639e-02\n",
      "  -4.12978977e-01 -6.41968369e-01  1.48684412e-01 -2.08380684e-01\n",
      "  -1.05044328e-01 -4.33986038e-01  1.44861132e-01 -4.02153246e-02\n",
      "  -2.82770425e-01 -2.73285389e-01  2.18583167e-01  4.19750452e-01\n",
      "   1.90996021e-01  1.74930647e-01  2.45941073e-01  2.64801055e-01\n",
      "   3.82124841e-01 -2.09172472e-01  2.51833647e-01 -3.53670716e-02\n",
      "   1.99886799e-01 -4.17296886e-01  4.87988830e-01 -3.10032815e-01\n",
      "  -3.97167504e-02  3.87380183e-01 -3.46416801e-01 -5.81459105e-02\n",
      "   4.96998221e-01  1.45694194e-03  3.70875653e-03 -1.93551853e-01\n",
      "   5.45464933e-01 -2.90634811e-01 -8.01301450e-02  2.94610560e-01\n",
      "  -6.16997816e-02  5.84120512e-01  1.19054891e-01 -1.84419811e-01\n",
      "  -2.41270840e-01  4.41027373e-01 -1.65839970e-01 -1.69474825e-01\n",
      "  -2.18006954e-01  1.50939599e-01  1.83535926e-02  1.72244295e-01\n",
      "  -1.10753048e-02 -3.07693809e-01 -3.33794840e-02 -3.41982216e-01\n",
      "  -1.75730318e-01  2.22976878e-01  3.84019464e-01 -3.10642928e-01\n",
      "   1.52841611e-02  2.20237207e-03 -1.28103673e-01 -6.33101165e-02\n",
      "   3.36058974e-01  2.15121090e-01  2.81743377e-01 -4.84508902e-01\n",
      "  -2.28461996e-01  1.36631638e-01 -1.12640828e-01 -6.94241673e-02\n",
      "  -5.69373928e-02  9.77435037e-02 -4.80990291e-01 -3.08263719e-01\n",
      "  -2.17177674e-01 -3.31967264e-01  3.98929387e-01 -4.12508324e-02\n",
      "   2.09438130e-01 -1.72096357e-01  1.30853597e-02 -4.43632573e-01\n",
      "   2.84141153e-01  1.16278440e-01  1.57885343e-01  8.19846019e-02\n",
      "   1.50795346e-02  3.05283964e-01 -1.11944839e-01 -3.94716114e-02\n",
      "   3.65207195e-02  4.63473052e-02 -2.09506243e-01 -3.74977261e-01\n",
      "   3.47994536e-01 -3.99069697e-01 -1.41408304e-02  4.05575812e-01\n",
      "   3.42362911e-01 -2.04331681e-01 -3.43647212e-01  4.52553272e-01\n",
      "   9.14707631e-02 -6.05848767e-02 -8.56190249e-02 -4.17628288e-02\n",
      "   3.90388399e-01  4.18598615e-02  3.17015380e-01 -2.61624813e-01\n",
      "  -5.85430786e-02  9.08967778e-02 -1.27220407e-01  5.48049808e-01\n",
      "   1.74010560e-01 -3.08409184e-01 -2.38247812e-01  8.40135515e-02\n",
      "   1.86353043e-01 -1.57030031e-01  1.44254521e-01 -4.73971777e-02\n",
      "   2.13451356e-01 -1.53371155e-01 -3.87623399e-01  1.20524660e-01\n",
      "   5.89707820e-03 -4.53806281e-01  1.42958388e-01  1.65979251e-01\n",
      "  -1.21529251e-01  4.66314927e-02 -6.27530932e-01 -2.45012894e-01\n",
      "  -4.46603805e-01 -3.00094068e-01  2.24952981e-01 -1.09789304e-01\n",
      "  -1.21125944e-01 -1.22886799e-01 -4.73080784e-01  2.94314116e-01\n",
      "  -2.37389460e-01 -2.03041226e-01 -5.64910062e-02  3.26421082e-01\n",
      "  -2.60329008e-01 -5.55668958e-02  3.77642401e-02 -1.76258327e-03\n",
      "  -4.12995160e-01 -3.22291665e-02 -3.62683348e-02 -9.45868850e-01\n",
      "  -3.45752505e-03  2.39031464e-01 -8.91940445e-02 -2.17808813e-01\n",
      "  -1.40604541e-01 -3.67502272e-01 -3.68035887e-03 -3.81094545e-01\n",
      "  -1.67977557e-01  1.12185925e-01 -1.57101050e-01 -2.41403893e-01\n",
      "   2.90113807e-01 -4.36426997e-02 -1.65708810e-01  2.89960176e-01\n",
      "  -2.98582256e-01  1.63360402e-01 -5.69062717e-02  2.62549501e-02\n",
      "  -1.81977361e-01 -9.56802666e-02  1.04108639e-01 -2.09989011e-01\n",
      "  -2.18848005e-01 -1.03977442e-01 -3.62764031e-01 -1.27094761e-01\n",
      "  -7.39461035e-02 -2.95952410e-01 -2.32735842e-01  1.51025010e-02\n",
      "   1.92804053e-01  2.70078629e-01  3.63240123e-01 -1.87155515e-01\n",
      "   1.65810645e-01 -3.21199596e-01  4.29234020e-02 -3.15338582e-01\n",
      "  -1.14810482e-01 -6.59554005e-02 -1.76321760e-01 -2.02215627e-01\n",
      "   1.77694887e-01 -3.66734108e-03  3.03267419e-01 -3.30638260e-01\n",
      "  -7.85695985e-02  4.98383828e-02 -5.98271154e-02 -6.50906935e-02\n",
      "   2.31066957e-01 -3.84608448e-01  6.45493641e-02  2.02436224e-01\n",
      "   1.16086848e-01 -1.00054294e-01  1.99845955e-01 -6.43915832e-02\n",
      "  -1.90359424e-03  2.70735294e-01 -2.95956850e-01  9.60397422e-02\n",
      "   4.97004300e-01  3.12896311e-01  1.20117016e-01  5.65681234e-02\n",
      "   1.16214007e-01  1.09798491e-01  3.09697449e-01  2.28431776e-01\n",
      "   2.54024584e-02  5.19330287e-03  3.13018858e-02 -3.84949356e-01\n",
      "  -7.35563859e-02 -8.69492292e-02 -2.36512363e-01 -8.73645917e-02\n",
      "   6.06627762e-01  3.58747452e-01 -5.50024748e-01  1.50788516e-01\n",
      "   2.14191638e-02 -1.43671766e-01  3.71316701e-01  1.14541069e-01\n",
      "  -1.94587544e-01  1.83650449e-01  5.41824460e-01 -7.10985288e-02\n",
      "   7.68326223e-02  2.55332619e-01 -2.38416985e-01 -6.09177016e-02\n",
      "  -2.56343901e-01  4.44353521e-01 -3.20495525e-03  6.64315939e-01\n",
      "  -4.90650088e-02  3.47024977e-01 -1.12161793e-01 -2.63155587e-02\n",
      "   1.44918397e-01 -3.40287015e-02  1.05926849e-01 -1.24522829e-02\n",
      "   2.26701885e-01  6.79881424e-02  3.03280493e-03  2.94876873e-01\n",
      "   2.56892070e-02  2.80552894e-01  5.70382118e-01  2.23455504e-01\n",
      "   1.44496113e-01  6.37901425e-01  2.49980971e-01  3.70863855e-01\n",
      "   1.85214117e-01  9.47633460e-02  1.28573328e-01  2.73173582e-02\n",
      "   3.47926706e-01  3.47979188e-01 -1.50908887e-01  5.51545501e-01\n",
      "   9.35200527e-02  2.51016259e-01  6.44529700e-01 -7.51246333e-01\n",
      "  -2.47073159e-01  3.41533214e-01  2.30977237e-01 -6.51058629e-02\n",
      "   1.31078273e-01  4.23603132e-02 -1.06668979e-01  2.51586974e-01\n",
      "  -3.62501979e-01 -1.16772868e-01 -7.26955831e-02  3.60334694e-01\n",
      "   1.93254743e-02 -1.71818599e-01 -3.21566135e-01  1.03199117e-01\n",
      "  -1.41323775e-01  6.43701479e-02 -1.93136960e-01 -1.54669285e-01\n",
      "  -4.92947549e-01 -3.48759532e-01 -1.61603883e-01  3.25216055e-01\n",
      "  -8.04916322e-02 -2.75205135e-01 -2.86860913e-01  1.51883855e-01\n",
      "   3.71219575e-01 -1.16216637e-01  5.12174591e-02 -2.49465480e-02\n",
      "   1.11635014e-01  9.64282081e-02  4.35036957e-01 -1.03505896e-02\n",
      "   1.93455294e-01 -1.54464334e-01 -2.69043267e-01 -4.54458296e-02\n",
      "   2.91467980e-02 -5.06183691e-02  4.80353720e-02  2.98900366e-01\n",
      "  -2.40730286e-01 -1.08188808e-01  2.10084796e-01  1.10735953e-01\n",
      "  -5.48424184e-01 -3.35793337e-03 -3.31109203e-02  2.83696920e-01\n",
      "  -2.55061104e-03  2.26467222e-01  1.28042866e-02  4.06067036e-02\n",
      "  -3.39962959e-01 -4.01826710e-01  4.02541012e-01  1.81044593e-01\n",
      "  -9.41579044e-02  1.67724028e-01  2.04218686e-01  4.80212756e-02\n",
      "  -1.84226066e-01 -6.66242391e-02  5.59993275e-02  3.05632204e-01\n",
      "  -1.06070694e-02  8.55197832e-02 -8.25271383e-03 -2.07184970e-01\n",
      "  -3.83279473e-01  2.49675676e-01 -1.32921105e-02  1.05675362e-01\n",
      "  -5.03341816e-02  1.74083367e-01  3.43557596e-01  1.37089819e-01\n",
      "   2.03521922e-01  2.17903301e-01 -6.14781752e-02 -3.20310861e-01\n",
      "  -1.10265717e-01 -4.17657495e-01  4.52300720e-02 -1.78855024e-02\n",
      "  -1.35821700e-01 -4.69521046e-01 -1.09541684e-01  7.80205950e-02\n",
      "   9.49794799e-03 -2.94912606e-01 -1.76292852e-01  3.86135042e-01]]\n",
      "Token IDs [array([  101,  2023,  2003,  1037,  2146,  3793,  2008,  3791,  2000,\n",
      "        2022, 19204,  3550,  1998, 11157,  2302, 19817,  4609, 10719,\n",
      "        1012,  9345, 19771,  1024,  2875,  2488,  9312,  3001,  2005,\n",
      "        9932,  6074,  6904, 23736,  2243,  1024,  2019,  3784, 12317,\n",
      "       15756,  2897,  3559,  2079,  7526, 16913, 11475,  7368,  2191,\n",
      "        1058, 19062,  4275,  2062, 21425,  2000,  1037,  2529,  1029,\n",
      "       23208,  5107,  4512,  2389,  6074,  3081, 10791,  2529,  1011,\n",
      "        9932,  2399,  2009,  3138,  2048,  2000, 17609,  1024,  2875,\n",
      "        3399,  1997,  9932,  1005,  1055,  2568,   102])]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def encode_text(texts, model_name='bert-base-uncased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    max_length = tokenizer.model_max_length\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_token_ids = []\n",
    "\n",
    "    print(\"Model Max length:\", max_length)\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize and split text into chunks\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=False)\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "\n",
    "        # Create chunks of max_length\n",
    "        chunks = [input_ids[i:i + max_length] for i in range(0, len(input_ids), max_length)]\n",
    "        print(chunks)\n",
    "\n",
    "        chunk_embeddings = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_input = {'input_ids': chunk.unsqueeze(0)}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**chunk_input)\n",
    "            chunk_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "            chunk_embeddings.append(chunk_embedding)\n",
    "\n",
    "            all_token_ids.append(chunk.numpy())\n",
    "\n",
    "        # Combine chunk embeddings\n",
    "        combined_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "        all_embeddings.append(combined_embedding)\n",
    "\n",
    "    return np.vstack(all_embeddings), all_token_ids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    texts = [\"\"\"This is a long text that needs to be tokenized and embedded without truncation.     EvalAI: Towards Better Evaluation Systems for AI Agents\n",
    "      \n",
    "\n",
    "       Fabrik: An Online Collaborative Neural Network Editor\n",
    "      \n",
    "\n",
    "       Do explanation modalities make VQA models more predictable to a human?\n",
    "      \n",
    "\n",
    "       Evaluating Visual Conversational Agents via Cooperative Human-AI Games\n",
    "      \n",
    "\n",
    "       It Takes Two to Tango: Towards Theory of AI's Mind\"\"\"]\n",
    "    embeddings, token_ids = encode_text(texts)\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "    print(\"Embeddings:\", embeddings)\n",
    "    print(\"Token IDs\", token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded tezt: \n",
      " this is a long text that needs to be tokenized and embedded without truncation. evalai : towards better evaluation systems for ai agents fabrik : an online collaborative neural network editor do explanation modalities make vqa models more predictable to a human? evaluating visual conversational agents via cooperative human - ai games it takes two to tango : towards theory of ai's mind\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def decode_text(token_ids, model_name='bert-base-uncased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    all_decoded_texts = []\n",
    "\n",
    "    for chunk_ids in token_ids:\n",
    "        decoded_chunk = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        all_decoded_texts.append(decoded_chunk)\n",
    "\n",
    "    return ' '.join(all_decoded_texts)\n",
    "\n",
    "\n",
    "decoded_text = decode_text(token_ids)\n",
    "print(\"decoded tezt: \\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def encode_texts(texts, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Encode a list of texts using a sentence transformer model.\n",
    "    \n",
    "    Args:\n",
    "        texts (list of str): The texts to encode.\n",
    "        model_name (str): The name of the sentence transformer model to use.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The embeddings of the texts.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"*100]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(len(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "for k in embeddings:\n",
    "    print(len(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunking.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Extract text (You can integrate your HTML/PDF extraction logic here)\n",
    "def extract_text(file_path):\n",
    "    # Dummy implementation\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Chunk text using LangChain\n",
    "def chunk_text(text):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "#     chunks = []\n",
    "#     for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "#         chunks.append(text[i:i + chunk_size])\n",
    "#     return chunks\n",
    "\n",
    "# Get embeddings using the updated function\n",
    "\n",
    "# Save chunks and embeddings to CSV\n",
    "def save_to_csv(file_path, chunks, embeddings):\n",
    "    data = {\n",
    "        \"Index\": list(range(len(chunks))),\n",
    "        \"Text\": chunks,\n",
    "        \"Embeddings\": [emb.tolist() for emb in embeddings]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to files\n",
    "    file_paths = [\"path/to/your/file1.txt\", \"path/to/your/file2.txt\"]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        text = extract_text(file_path)\n",
    "        chunks = chunk_text(text)\n",
    "        embeddings, decoded_texts = get_embeddings(chunks)\n",
    "        save_to_csv(\"chunks_and_embeddings.csv\", chunks, embeddings)\n",
    "        print(f\"Chunks and embeddings from {file_path} saved to CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary profile text processin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def extract_text(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        return text\n",
    "    \n",
    "    def encode_texts(self, texts):\n",
    "        embeddings = self.model.encode(texts, convert_to_tensor=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def save_to_csv(self, file_path, texts, embeddings):\n",
    "        data = {\n",
    "            \"Index\": list(range(len(texts))),\n",
    "            \"Text\": texts,\n",
    "            \"Embeddings\": [emb.tolist() for emb in embeddings]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(file_path, index=False)\n",
    "    \n",
    "    def process_and_save_text(self, master_text, output_csv):\n",
    "        chunks = self.text_splitter.split_text(master_text)\n",
    "        embeddings = self.encode_texts(chunks)\n",
    "        self.save_to_csv(output_csv, chunks, embeddings)\n",
    "        print(f\"Chunks and embeddings saved to {output_csv}\")\n",
    "    \n",
    "    def process_directory(self, directory, output_csv):\n",
    "        all_texts = []\n",
    "        all_embeddings = []\n",
    "        master_text = ''\n",
    "        self.text_splitter = CharacterTextSplitter(chunk_size=300, \n",
    "                                                   chunk_overlap=100,\n",
    "                                                #    separator=\" \")\n",
    "                                                   separator=\"\\n\")\n",
    "\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\"summary.txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    text = self.extract_text(file_path)\n",
    "                    print(f\"Processing {file_path}...\")\n",
    "\n",
    "                    master_text +=f'\\n {text} \\n'\n",
    "\n",
    "        self.process_and_save_text(master_text, output_csv)\n",
    "        print(f\"All chunks and embeddings saved to {output_csv}\")\n",
    "        return master_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all text files to create chunks and embeddings...\n",
      "Processing rohithprofile/user_profile_summary.txt...\n",
      "Chunks and embeddings saved to rohithprofile/summary_profile.csv\n",
      "All chunks and embeddings saved to rohithprofile/summary_profile.csv\n",
      "Database is created at rohithprofile/summary_profile.csv\n"
     ]
    }
   ],
   "source": [
    "textprocessor = TextProcessor()\n",
    "user_dir = \"rohithprofile/\"\n",
    "output_csv = os.path.join(user_dir, \"summary_profile.csv\")\n",
    "print(\"Processing all text files to create chunks and embeddings...\")\n",
    "textprocessor.process_directory(user_dir, output_csv)\n",
    "print(f\"Database is created at {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable set.\n",
      "Environment variable value: some_value\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
