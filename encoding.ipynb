{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def encode_texts(texts, model_name='all-MiniLM-L6-v2'):\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_csv(file_path, texts, embeddings):\n",
    "\n",
    "    data = {\n",
    "        \"Index\": list(range(len(texts))),\n",
    "        \"Text\": texts,\n",
    "        \"Embeddings\": [emb.tolist() for emb in embeddings]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.info())\n",
    "    df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(file_path, texts, embeddings):\n",
    "    # Serialize embeddings as JSON\n",
    "    serialized_embeddings = [json.dumps(emb.tolist()) for emb in embeddings]\n",
    "\n",
    "    data = {\n",
    "        \"Index\": list(range(len(texts))),\n",
    "        \"Text\": texts,\n",
    "        \"Embeddings\": serialized_embeddings\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.info())\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_save_text(file_path, output_csv):\n",
    "    text = extract_text(file_path)\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    embeddings = encode_texts(chunks)\n",
    "    \n",
    "    save_to_csv(output_csv, chunks, embeddings)\n",
    "    print(f\"Chunks and embeddings saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 706, which is longer than the specified 500\n",
      "Created a chunk of size 2283, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Index       15 non-null     int64 \n",
      " 1   Text        15 non-null     object\n",
      " 2   Embeddings  15 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 492.0+ bytes\n",
      "None\n",
      "Chunks and embeddings saved to extracted_html_files/saivineethkumar.github.io/chunks_and_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file_path = \"extracted_html_files/saivineethkumar.github.io/extracted_text.txt\"\n",
    "\n",
    "output_csv_path = \"extracted_html_files/saivineethkumar.github.io/chunks_and_embeddings.csv\"\n",
    "\n",
    "process_and_save_text(input_file_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Index       15 non-null     int64 \n",
      " 1   Text        15 non-null     object\n",
      " 2   Embeddings  15 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 492.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = pd.read_csv('extracted_html_files/saivineethkumar.github.io/chunks_and_embeddings.csv')\n",
    "file.info()\n",
    "# file.shape, file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FAISS TEsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ### PARAGRAPHS ###\n",
      "\n",
      "\n",
      "       I am a seasoned software developer driven by a passion for creating transformative products that positively impact the world\n",
      "Distance: 1.4239776134490967\n",
      "********************\n",
      "\n",
      "Text: - \n",
      "\n",
      "\n",
      "-\n",
      "Distance: 1.5956523418426514\n",
      "********************\n",
      "\n",
      "Text: A seasoned software developer with a knack for crafting innovative solutions, I am currently pursuing a Master's in Computer Science from UMass Amherst to further enhance my expertise in the ever-evolving realm of technology. With a passion for problem-solving and a keen eye for detail, I am eager to transition into a full-time software engineer role starting in May 2024. My academic background has equipped me with a solid foundation in programming languages, algorithms, and data structures, while my practical experience has honed my ability to design, develop, and test complex software systems. I thrive in collaborative environments and am always eager to learn new technologies.\n",
      "Distance: 1.5979716777801514\n",
      "********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "csv_file = 'extracted_html_files/saivineethkumar.github.io/chunks_and_embeddings.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "# print(df.info())\n",
    "texts = df['Text'].tolist() \n",
    "embeddings = df[\"Embeddings\"].apply(eval).apply(np.array).tolist()\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "# embeddings = model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "# Step 3: Build a FAISS Index\n",
    "# print(len(embeddings),  type(list(embeddings[0])), embeddings[0])\n",
    "d = len(embeddings[0]) # Dimension of embeddings\n",
    "index = faiss.IndexFlatL2(d)  # Using L2 (Euclidean) distance\n",
    "# print(type(embeddings))\n",
    "index.add(np.array(embeddings)) # Add embeddings to the index\n",
    "\n",
    "# Function to perform search and retrieve original text\n",
    "def search(query_text, texts, k=3):\n",
    "    query_embedding = model.encode([query_text], convert_to_tensor=False)\n",
    "    D, I = index.search(query_embedding, k)  # Perform search\n",
    "    return [texts[i] for i in I[0]], D[0]\n",
    "\n",
    "# Example query\n",
    "query = \"Tell me something about me\"\n",
    "similar_texts, distances = search(query, texts)\n",
    "\n",
    "for text, distance in zip(similar_texts, distances):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Distance: {distance}\")\n",
    "    print(\"*\"*20)\n",
    "    print()\n",
    "# print(\"Similar Texts:\", similar_texts)\n",
    "# print(\"Distances:\", distances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ### PARAGRAPHS ###\n",
      "\n",
      "\n",
      "       I am a seasoned software developer driven by a passion for creating transformative products that positively impact the world\n",
      "Distance: 1.4239776134490967\n",
      "****************************************\n",
      "\n",
      "Text: - \n",
      "\n",
      "\n",
      "-\n",
      "Distance: 1.5956523418426514\n",
      "****************************************\n",
      "\n",
      "Text: A seasoned software developer with a knack for crafting innovative solutions, I am currently pursuing a Master's in Computer Science from UMass Amherst to further enhance my expertise in the ever-evolving realm of technology. With a passion for problem-solving and a keen eye for detail, I am eager to transition into a full-time software engineer role starting in May 2024. My academic background has equipped me with a solid foundation in programming languages, algorithms, and data structures, while my practical experience has honed my ability to design, develop, and test complex software systems. I thrive in collaborative environments and am always eager to learn new technologies.\n",
      "Distance: 1.5979716777801514\n",
      "****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load data\n",
    "csv_file = 'extracted_html_files/saivineethkumar.github.io/chunks_and_embeddings.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "texts = df['Text'].tolist()\n",
    "embeddings = df[\"Embeddings\"].apply(eval).apply(np.array).tolist()\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Build a FAISS Index using Euclidean distance\n",
    "d = len(embeddings[0])  # Dimension of embeddings\n",
    "index = faiss.IndexFlatL2(d)  # Using L2 (Euclidean) distance\n",
    "index.add(np.array(embeddings))  # Add embeddings to the index\n",
    "\n",
    "# Function to perform search and retrieve original text\n",
    "def search_euclidean(query_text, texts, k=3):\n",
    "    query_embedding = model.encode([query_text], convert_to_tensor=False)\n",
    "    D, I = index.search(query_embedding, k)  # Perform search\n",
    "    return [texts[i] for i in I[0]], D[0]\n",
    "\n",
    "# Example query\n",
    "query = \"Tell me something about me\"\n",
    "similar_texts, distances = search_euclidean(query, texts)\n",
    "\n",
    "for text, distance in zip(similar_texts, distances):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Distance: {distance}\")\n",
    "    print(\"*\"*40)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/mem0_env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load data\n",
    "csv_file = 'extracted_html_files/saivineethkumar.github.io/chunks_and_embeddings.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "texts = df['Text'].tolist()\n",
    "embeddings = df[\"Embeddings\"].apply(eval).apply(np.array).tolist()\n",
    "\n",
    "# Normalize embeddings\n",
    "embeddings = np.array([embedding / np.linalg.norm(embedding) for embedding in embeddings])\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Build a FAISS Index using Inner Product (dot product)\n",
    "d = len(embeddings[0])  # Dimension of embeddings\n",
    "index = faiss.IndexFlatIP(d)  # Using Inner Product\n",
    "index.add(embeddings)  # Add embeddings to the index\n",
    "\n",
    "# Function to perform search and retrieve original text\n",
    "def search_inner_product(query_text, texts, k=3):\n",
    "    query_embedding = model.encode([query_text], convert_to_tensor=False)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)  # Normalize the query embedding\n",
    "    D, I = index.search(query_embedding, k)  # Perform search\n",
    "    return [texts[i] for i in I[0]], D[0]\n",
    "\n",
    "# Example query\n",
    "query = \"Tell me something about me\"\n",
    "similar_texts, distances = search_inner_product(query, texts)\n",
    "\n",
    "for text, distance in zip(similar_texts, distances):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Distance: {distance}\")\n",
    "    print(\"*\"*40)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(csv_file):\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    embeddings = df[\"Embeddings\"].apply(eval).apply(np.array).tolist()\n",
    "    texts = df[\"Text\"].tolist()\n",
    "    return texts, np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(query, model_name='all-MiniLM-L6-v2'):\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    return query_embedding.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_similar_texts(query_embedding, texts, embeddings, k=3):\n",
    "\n",
    "    # faiss.normalize_L2(embeddings)\n",
    "    # faiss.normalize_L2(query_embedding)\n",
    "    # index = faiss.IndexFlatIP(embeddings.shape[1])  # Inner Product (cosine similarity)\n",
    "\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    print(D, I)\n",
    "    top_k_texts = [texts[i] for i in I[0]]\n",
    "    return top_k_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4210975 1.5832176 1.6176374]] [[0 4 1]]\n",
      "Top 3 similar texts:\n",
      "Rank 1: ### PARAGRAPHS ###\n",
      "\n",
      "\n",
      "       I am a seasoned software developer driven by a passion for creating transformative products that positively impact the world\n",
      "Rank 2: ### HEADINGS ###\n",
      "\n",
      "\n",
      "      Contact Me\n",
      "     \n",
      "\n",
      "       About\n",
      "      \n",
      "\n",
      "      Experience\n",
      "     \n",
      "\n",
      "       Skills\n",
      "      \n",
      "\n",
      "      Projects\n",
      "     \n",
      "\n",
      "        Why hire me?  ↓\n",
      "       \n",
      "\n",
      "        Hobbies  ↓\n",
      "       \n",
      "\n",
      "          Software Engineer, Samsung Research\n",
      "         \n",
      "\n",
      "          SWE Intern, Samsung Research\n",
      "         \n",
      "\n",
      "          SWE Intern, Tika-Data\n",
      "         \n",
      "\n",
      "         Languages\n",
      "        \n",
      "\n",
      "         Tools / Frameworks\n",
      "        \n",
      "\n",
      "         Machine Learning / Data Science\n",
      "        \n",
      "\n",
      "         Miscellaneous\n",
      "Rank 3: A seasoned software developer with a knack for crafting innovative solutions, I am currently pursuing a Master's in Computer Science from UMass Amherst to further enhance my expertise in the ever-evolving realm of technology. With a passion for problem-solving and a keen eye for detail, I am eager to transition into a full-time software engineer role starting in May 2024. My academic background has equipped me with a solid foundation in programming languages, algorithms, and data structures, while my practical experience has honed my ability to design, develop, and test complex software systems. I thrive in collaborative environments and am always eager to learn new technologies.\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'extracted_html_files/saivineethkumar.github.io/chunks_and_embeddings.csv'\n",
    "texts, embeddings = load_embeddings(csv_file)\n",
    "\n",
    "# Handle user query\n",
    "query = \"Hello tell me something about me\"\n",
    "query_embedding = encode_query(query)\n",
    "\n",
    "# Find top 10 similar texts\n",
    "top_k_texts = find_top_k_similar_texts(query_embedding, texts, embeddings, k=3)\n",
    "\n",
    "# Output the results\n",
    "print(\"Top 3 similar texts:\")\n",
    "for i, text in enumerate(top_k_texts):\n",
    "    print(f\"Rank {i + 1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_embeddings(csv_file):\n",
    "    print(\"loadembeddings\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    embeddings = df[\"Embeddings\"].apply(eval).apply(np.array).tolist()\n",
    "    texts = df[\"Text\"].tolist()\n",
    "    return texts, np.ascontiguousarray(embeddings, dtype=np.float32)\n",
    "\n",
    "def encode_query(query, model_name='all-MiniLM-L6-v2'):\n",
    "    print(\"encode_query\")\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    return np.ascontiguousarray(query_embedding.cpu().numpy(), dtype=np.float32)\n",
    "\n",
    "def find_top_k_similar_texts(query_embedding, texts, embeddings, k=3, metric='euclidean'):\n",
    "    print(\"find_toop\")\n",
    "    if metric == 'cosine':\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])  # Inner Product (cosine similarity)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])  # Euclidean distance\n",
    "\n",
    "    index.add(embeddings)\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    top_k_texts = [texts[i] for i in I[0]]\n",
    "    return top_k_texts, D[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadembeddings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_file = 'extracted_html_files/saivineethkumar.github.io/chunks_and_embeddings.csv'\n",
    "texts, embeddings = load_embeddings(csv_file)\n",
    "\n",
    "query = \"Hello tell me something about me\"\n",
    "# query_embedding = encode_query(query)\n",
    "\n",
    "# Choose the distance metric 'cosine' or 'euclidean'\n",
    "metric = 'cosine'  \n",
    "# metric = 'euclidean' \n",
    "\n",
    "# top_k_texts, distances = find_top_k_similar_texts(query_embedding, texts, embeddings, k=3, metric=metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_query\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "query = \"who am I\"\n",
    "query_embedding = encode_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
