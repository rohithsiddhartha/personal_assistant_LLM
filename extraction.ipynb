{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5e8730bcaed3407bc187064859005a28'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedchain import App\n",
    "app = App()\n",
    "app.add('Mem0 Interview Task.pdf', data_type='pdf_file')\n",
    "app.add('https://saivineethkumar.github.io/', data_type='web_page')\n",
    "app.add('https://taranjeet.co/about/', data_type='web_page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 21:59:46,189 - 8572832768 - embedchain.py-embedchain:545 - WARNING: Starting from v0.1.125 the return type of query method will be changed to tuple containing `answer`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Taranjeet has a diverse professional background, starting as a software engineer at companies like Paytm and Gradeup. He then transitioned into product management, working as a Senior Product Manager at Khatabook. Additionally, he has experience in remote work, contributing to projects like an educational product for individuals with autism at Inclusys and developing frontend components for LabXchange at OpenCraft. Taranjeet has also made significant contributions to open source projects, co-authoring a chapter on NLP and organizing hackathons. His entrepreneurial endeavors include building an AI App Store with a large user base and creating various successful apps.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.query(\"Who is taranjeet. Give me detailed understanding of his professional experience?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      " 1.\n",
      "First step is to set up the assistant which is up and running & then needs to be\n",
      "personalized with the next content that is added.\n",
      "2.\n",
      "User details and preferences are open ended ? or do we get proper inputs from the user\n",
      "on what he likes and what he doesn’t? Or do we have to extract user profile from the\n",
      "unstructured input that that they share and generate a user profile based on the inputs\n",
      "Ans: As I understand this needs to be done by AI when we pass the unstructured data to\n",
      "the user where it can parse the info and create a personalized user profile\n",
      "3.\n",
      "Then this profile which is stored should be used for custom response generation. -> This\n",
      "profile should be saved in memory and should be constantly updated.\n",
      "https://github.com/Shubhamsaboo/awesome-llm-apps/blob/main/local_chatgpt_with_memory/lo\n",
      "cal_chatgpt_memory.py\n",
      "What does it mean by custom retrieval logic? Contextual information here means whatever is\n",
      "the memory of LLM from that fetch the user preferences for the query asked and then generate\n",
      "the response to user queries.\n",
      "Deliverables:\n",
      "Once a new model is launched you should be able to upload unstructured data -> create custom\n",
      "user profile -> For every query retrieve most relevant preferences based on the user profile and\n",
      "prepare the response? The retrieval here is from memory right??\n",
      "What do you mean by any open source library? Likes of embed chain as well where building a\n",
      "RAG system is one highly effective.\n",
      "Streamlit/Shiny for UI Design.\n",
      "API functionality, Load, data types for unstructured data - unittest\n",
      "Vector DB, FAISS - can store.\n",
      "Memory Store - MongoDB\n",
      "For pdf unstructured library\n",
      "Questions and Answers\n",
      "Question: Can I use the likes of OpenAI Models or something local?\n",
      "Answer: Yes, you are free to use any model.\n",
      "Question: Currently, can we restrict the scope to PDFs and blogs? If not, what are the\n",
      "different unstructured data sources the Assistant should handle?\n",
      "Answer: Sure, restricting to PDFs and blogs is fine for the task.\n",
      "Question: What specific user details and preferences should the AI Assistant extract and\n",
      "store from the unstructured data?\n",
      "Answer: User details and preferences are kind of open-ended, and this is where you can get a\n",
      "lot creative.\n",
      "Question: Are there any restrictions or guidelines on which open-source libraries can be\n",
      "used for this project?\n",
      "Answer: There is no restriction on using open-source libraries. Feel free to use any library of\n",
      "your choice.\n",
      "Bonus:\n",
      "Question: Do you have any specific requirements or expectations for the custom retrieval\n",
      "logic?\n",
      "Answer: No specific requirements. Again, we want you to be smart and creative about how to do\n",
      "the retrieval of memories.\n",
      "Question: Should the retrieval logic prioritize recent interactions (chat history based) or a\n",
      "specific type of user information (user information retrieval from data uploaded)? Or\n",
      "should it focus on profile-relevant information extraction?\n",
      "Answer: Great question. The candidate is expected to take a call on this based on their\n",
      "understanding of how humans think about memory.\n",
      "Mem0 Interview Project\n",
      "Goal\n",
      "Develop a personalized AI Assistant that can understand and remember user details and\n",
      "preferences from unstructured data sources, providing a tailored conversational experience.\n",
      "Requirements\n",
      "●\n",
      "Data Ingestion: The AI Assistant should allow users to input various unstructured data\n",
      "sources, such as blog post content, personal bio etc.\n",
      "●\n",
      "User Profiling: From the ingested data, the AI Assistant must deduce relevant user details\n",
      "and preferences, creating a personalized user profile.\n",
      "●\n",
      "Contextual Response Generation: During conversations, the AI Assistant should recall and\n",
      "leverage the user's profile information to provide contextualized and personalized\n",
      "responses.\n",
      "●\n",
      "End-to-End Functionality: Implement a chat CLI where the user inputs natural language\n",
      "queries, which can perform the following tasks:\n",
      "○\n",
      "Save something in memory\n",
      "○\n",
      "Deduce memory from unstructured text\n",
      "○\n",
      "Update memory\n",
      "○\n",
      "Delete memory\n",
      "Process the user input with your algorithm and return relevant responses.\n",
      "Bonus\n",
      "●\n",
      "Custom Retrieval Logic: Develop a unique logic for fetching the most relevant contextual\n",
      "information in response to user queries.\n",
      "●\n",
      "UI Design: Showcase your design prowess by creating a custom UI to show off your full\n",
      "stack skills.\n",
      "●\n",
      "Innovation Encouraged: We urge you to think outside the box. Impress us with your\n",
      "creativity and technical skills. The sky is merely the starting point.\n",
      "●\n",
      "Unit tests: Add unit tests if you can. We would love to see unit tests for your functionality.\n",
      "Deliverables\n",
      "●\n",
      "CLI interface: Develop a fully functional CLI where a user can interact with the AI assistant,\n",
      "ask questions, and store relevant information in memory for personalization.\n",
      "●\n",
      "Architecture Diagram: Provide a screenshot of an architecture diagram, created with tools\n",
      "like Excalidraw or using pen paper, detailing your strategy.\n",
      "●\n",
      "Code Quality: Submit well-documented code as a GitHub repository, accompanied by a\n",
      "comprehensive README. Ensure your code is of high quality.\n",
      "●\n",
      "GitHub Repository: Send us a link to your GitHub repository so that we can checkout the\n",
      "code.\n",
      "●\n",
      "Areas of improvements: While we understand that this is just an assignment, tell us what\n",
      "approaches you might have taken if you had one month to work on this problem\n",
      "statement to improve the memory system.\n",
      "Getting started\n",
      "●\n",
      "Feel free to use any open source library of your choice.\n",
      "Timeline\n",
      "●\n",
      "We have tested and found that the project can be fully finished in less than 72 hours.\n",
      "●\n",
      "Nevertheless, what matters is the final quality of the deliverable and showing a well\n",
      "thought methodology, proper working and task completion accuracy.\n",
      "●\n",
      "You have 4 days to submit the project from when you start (please send an email to\n",
      "founders@mem0.ai when you start the project to start tracking).\n",
      "ID\n",
      "Name\n",
      "Age\n",
      "Department\n",
      "Salary\n",
      "101\n",
      "Alice\n",
      "25\n",
      "HR\n",
      "50000\n",
      "102\n",
      "Bob\n",
      "30\n",
      "Engineering\n",
      "70000\n",
      "103\n",
      "Charlie\n",
      "35\n",
      "Marketing\n",
      "60000\n",
      "104\n",
      "David\n",
      "40\n",
      "Sales\n",
      "65000\n",
      "105\n",
      "Eve\n",
      "28\n",
      "Finance\n",
      "72000\n",
      "ID\n",
      "Name Age Departmen\n",
      "t\n",
      "Salar\n",
      "y\n",
      "101 Alice\n",
      "25\n",
      "HR\n",
      "50000\n",
      "102 Bob\n",
      "30\n",
      "Engineerin\n",
      "g\n",
      "70000\n",
      "103 Charli\n",
      "e\n",
      "35\n",
      "Marketing\n",
      "60000\n",
      "104 David 40\n",
      "Sales\n",
      "65000\n",
      "105 Eve\n",
      "28\n",
      "Finance\n",
      "72000\n",
      "\n",
      "Images extracted to directory: extracted_images\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_images(pdf_path, image_dir):\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_number in range(len(doc)):\n",
    "        for img_index, img in enumerate(doc.get_page_images(page_number)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_path = f\"{image_dir}/page{page_number+1}_img{img_index+1}.{image_ext}\"\n",
    "            with open(image_path, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "\n",
    "# pdf_path = \"Mem0 Interview Task.pdf\"  # Path to the uploaded PDF\n",
    "# pdf_path = \"image_text.pdf\"  # Path to the uploaded PDF\n",
    "pdf_path = \"text_image_table.pdf\"  # Path to the uploaded PDF\n",
    "image_dir = \"extracted_images\"\n",
    "text = extract_text(pdf_path)\n",
    "print(\"Extracted Text:\\n\", text)\n",
    "extract_images(pdf_path, image_dir)\n",
    "print(f\"Images extracted to directory: {image_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc1']],\n",
       " 'distances': [[1.2882779836654663]],\n",
       " 'metadatas': [[{'source': 'local'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Mem0 Interview Task.pdf']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      " 1.\n",
      "First step is to set up the assistant which is up and running & then needs to be\n",
      "personalized with the next content that is added.\n",
      "2.\n",
      "User details and preferences are open ended ? or do we get proper inputs from the user\n",
      "on what he likes and what he doesn’t? Or do we have to extract user profile from the\n",
      "unstructured input that that they share and generate a user profile based on the inputs\n",
      "Ans: As I understand this needs to be done by AI when we pass the unstructured data to\n",
      "the user where it can parse the info and create a personalized user profile\n",
      "3.\n",
      "Then this profile which is stored should be used for custom response generation. -> This\n",
      "profile should be saved in memory and should be constantly updated.\n",
      "https://github.com/Shubhamsaboo/awesome-llm-apps/blob/main/local_chatgpt_with_memory/lo\n",
      "cal_chatgpt_memory.py\n",
      "What does it mean by custom retrieval logic? Contextual information here means whatever is\n",
      "the memory of LLM from that fetch the user preferences for the query asked and then generate\n",
      "the response to user queries.\n",
      "Deliverables:\n",
      "Once a new model is launched you should be able to upload unstructured data -> create custom\n",
      "user profile -> For every query retrieve most relevant preferences based on the user profile and\n",
      "prepare the response? The retrieval here is from memory right??\n",
      "What do you mean by any open source library? Likes of embed chain as well where building a\n",
      "RAG system is one highly effective.\n",
      "Streamlit/Shiny for UI Design.\n",
      "API functionality, Load, data types for unstructured data - unittest\n",
      "Vector DB, FAISS - can store.\n",
      "Memory Store - MongoDB\n",
      "For pdf unstructured library\n",
      "Questions and Answers\n",
      "Question: Can I use the likes of OpenAI Models or something local?\n",
      "Answer: Yes, you are free to use any model.\n",
      "Question: Currently, can we restrict the scope to PDFs and blogs? If not, what are the\n",
      "different unstructured data sources the Assistant should handle?\n",
      "Answer: Sure, restricting to PDFs and blogs is fine for the task.\n",
      "Question: What specific user details and preferences should the AI Assistant extract and\n",
      "store from the unstructured data?\n",
      "Answer: User details and preferences are kind of open-ended, and this is where you can get a\n",
      "lot creative.\n",
      "Question: Are there any restrictions or guidelines on which open-source libraries can be\n",
      "used for this project?\n",
      "Answer: There is no restriction on using open-source libraries. Feel free to use any library of\n",
      "your choice.\n",
      "Bonus:\n",
      "Question: Do you have any specific requirements or expectations for the custom retrieval\n",
      "logic?\n",
      "Answer: No specific requirements. Again, we want you to be smart and creative about how to do\n",
      "the retrieval of memories.\n",
      "Question: Should the retrieval logic prioritize recent interactions (chat history based) or a\n",
      "specific type of user information (user information retrieval from data uploaded)? Or\n",
      "should it focus on profile-relevant information extraction?\n",
      "Answer: Great question. The candidate is expected to take a call on this based on their\n",
      "understanding of how humans think about memory.\n",
      "Mem0 Interview Project\n",
      "Goal\n",
      "Develop a personalized AI Assistant that can understand and remember user details and\n",
      "preferences from unstructured data sources, providing a tailored conversational experience.\n",
      "Requirements\n",
      "●\n",
      "Data Ingestion: The AI Assistant should allow users to input various unstructured data\n",
      "sources, such as blog post content, personal bio etc.\n",
      "●\n",
      "User Profiling: From the ingested data, the AI Assistant must deduce relevant user details\n",
      "and preferences, creating a personalized user profile.\n",
      "●\n",
      "Contextual Response Generation: During conversations, the AI Assistant should recall and\n",
      "leverage the user's profile information to provide contextualized and personalized\n",
      "responses.\n",
      "●\n",
      "End-to-End Functionality: Implement a chat CLI where the user inputs natural language\n",
      "queries, which can perform the following tasks:\n",
      "○\n",
      "Save something in memory\n",
      "○\n",
      "Deduce memory from unstructured text\n",
      "○\n",
      "Update memory\n",
      "○\n",
      "Delete memory\n",
      "Process the user input with your algorithm and return relevant responses.\n",
      "Bonus\n",
      "●\n",
      "Custom Retrieval Logic: Develop a unique logic for fetching the most relevant contextual\n",
      "information in response to user queries.\n",
      "●\n",
      "UI Design: Showcase your design prowess by creating a custom UI to show off your full\n",
      "stack skills.\n",
      "●\n",
      "Innovation Encouraged: We urge you to think outside the box. Impress us with your\n",
      "creativity and technical skills. The sky is merely the starting point.\n",
      "●\n",
      "Unit tests: Add unit tests if you can. We would love to see unit tests for your functionality.\n",
      "Deliverables\n",
      "●\n",
      "CLI interface: Develop a fully functional CLI where a user can interact with the AI assistant,\n",
      "ask questions, and store relevant information in memory for personalization.\n",
      "●\n",
      "Architecture Diagram: Provide a screenshot of an architecture diagram, created with tools\n",
      "like Excalidraw or using pen paper, detailing your strategy.\n",
      "●\n",
      "Code Quality: Submit well-documented code as a GitHub repository, accompanied by a\n",
      "comprehensive README. Ensure your code is of high quality.\n",
      "●\n",
      "GitHub Repository: Send us a link to your GitHub repository so that we can checkout the\n",
      "code.\n",
      "●\n",
      "Areas of improvements: While we understand that this is just an assignment, tell us what\n",
      "approaches you might have taken if you had one month to work on this problem\n",
      "statement to improve the memory system.\n",
      "Getting started\n",
      "●\n",
      "Feel free to use any open source library of your choice.\n",
      "Timeline\n",
      "●\n",
      "We have tested and found that the project can be fully finished in less than 72 hours.\n",
      "●\n",
      "Nevertheless, what matters is the final quality of the deliverable and showing a well\n",
      "thought methodology, proper working and task completion accuracy.\n",
      "●\n",
      "You have 4 days to submit the project from when you start (please send an email to\n",
      "founders@mem0.ai when you start the project to start tracking).\n",
      "ID\n",
      "Name\n",
      "Age\n",
      "Department\n",
      "Salary\n",
      "101\n",
      "Alice\n",
      "25\n",
      "HR\n",
      "50000\n",
      "102\n",
      "Bob\n",
      "30\n",
      "Engineering\n",
      "70000\n",
      "103\n",
      "Charlie\n",
      "35\n",
      "Marketing\n",
      "60000\n",
      "104\n",
      "David\n",
      "40\n",
      "Sales\n",
      "65000\n",
      "105\n",
      "Eve\n",
      "28\n",
      "Finance\n",
      "72000\n",
      "ID\n",
      "Name Age Departmen\n",
      "t\n",
      "Salar\n",
      "y\n",
      "101 Alice\n",
      "25\n",
      "HR\n",
      "50000\n",
      "102 Bob\n",
      "30\n",
      "Engineerin\n",
      "g\n",
      "70000\n",
      "103 Charli\n",
      "e\n",
      "35\n",
      "Marketing\n",
      "60000\n",
      "104 David 40\n",
      "Sales\n",
      "65000\n",
      "105 Eve\n",
      "28\n",
      "Finance\n",
      "72000\n",
      "\n",
      "Images extracted to directory: extracted_images\n"
     ]
    },
    {
     "ename": "DeprecationError",
     "evalue": "PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeprecationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages extracted to directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Extract tables\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mextract_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTables extracted to directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mextract_tables\u001b[0;34m(pdf_path, table_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(table_dir):\n\u001b[1;32m     29\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(table_dir)\n\u001b[0;32m---> 31\u001b[0m tables \u001b[38;5;241m=\u001b[39m \u001b[43mcamelot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tables):\n\u001b[1;32m     33\u001b[0m     table_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(table_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/camelot/io.py:111\u001b[0m, in \u001b[0;36mread_pdf\u001b[0;34m(filepath, pages, password, flavor, suppress_stdout, layout_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m validate_input(kwargs, flavor\u001b[38;5;241m=\u001b[39mflavor)\n\u001b[0;32m--> 111\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mPDFHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m remove_extra(kwargs, flavor\u001b[38;5;241m=\u001b[39mflavor)\n\u001b[1;32m    113\u001b[0m tables \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m    114\u001b[0m     flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[1;32m    115\u001b[0m     suppress_stdout\u001b[38;5;241m=\u001b[39msuppress_stdout,\n\u001b[1;32m    116\u001b[0m     layout_kwargs\u001b[38;5;241m=\u001b[39mlayout_kwargs,\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    118\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/camelot/handlers.py:50\u001b[0m, in \u001b[0;36mPDFHandler.__init__\u001b[0;34m(self, filepath, pages, password)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/camelot/handlers.py:74\u001b[0m, in \u001b[0;36mPDFHandler._get_pages\u001b[0;34m(self, filepath, pages)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     instream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     infile \u001b[38;5;241m=\u001b[39m \u001b[43mPdfFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m infile\u001b[38;5;241m.\u001b[39misEncrypted:\n\u001b[1;32m     76\u001b[0m         infile\u001b[38;5;241m.\u001b[39mdecrypt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword)\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/PyPDF2/_reader.py:1974\u001b[0m, in \u001b[0;36mPdfFileReader.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1974\u001b[0m     \u001b[43mdeprecation_with_replacement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPdfFileReader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPdfReader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1976\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# maintain the default\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/PyPDF2/_utils.py:369\u001b[0m, in \u001b[0;36mdeprecation_with_replacement\u001b[0;34m(old_name, new_name, removed_in)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecation_with_replacement\u001b[39m(\n\u001b[1;32m    364\u001b[0m     old_name: \u001b[38;5;28mstr\u001b[39m, new_name: \u001b[38;5;28mstr\u001b[39m, removed_in: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    Raise an exception that a feature was already removed, but has a replacement.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mdeprecation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEPR_MSG_HAPPENED\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremoved_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/PyPDF2/_utils.py:351\u001b[0m, in \u001b[0;36mdeprecation\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecation\u001b[39m(msg: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DeprecationError(msg)\n",
      "\u001b[0;31mDeprecationError\u001b[0m: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead."
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import camelot\n",
    "import os\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_images(pdf_path, image_dir):\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "        \n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_number in range(len(doc)):\n",
    "        for img_index, img in enumerate(doc.get_page_images(page_number)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_path = f\"{image_dir}/page{page_number+1}_img{img_index+1}.{image_ext}\"\n",
    "            with open(image_path, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "\n",
    "def extract_tables(pdf_path, table_dir):\n",
    "    if not os.path.exists(table_dir):\n",
    "        os.makedirs(table_dir)\n",
    "        \n",
    "    tables = camelot.read_pdf(pdf_path, pages='all')\n",
    "    for i, table in enumerate(tables):\n",
    "        table_path = os.path.join(table_dir, f\"table_{i}.csv\")\n",
    "        table.to_csv(table_path)\n",
    "\n",
    "pdf_path = \"text_image_table.pdf\"  # Path to the uploaded PDF\n",
    "image_dir = \"extracted_images\"\n",
    "table_dir = \"extracted_tables\"\n",
    "\n",
    "# Extract text\n",
    "text = extract_text(pdf_path)\n",
    "print(\"Extracted Text:\\n\", text)\n",
    "\n",
    "# Extract images\n",
    "extract_images(pdf_path, image_dir)\n",
    "print(f\"Images extracted to directory: {image_dir}\")\n",
    "\n",
    "# Extract tables\n",
    "extract_tables(pdf_path, table_dir)\n",
    "print(f\"Tables extracted to directory: {table_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      " 1.\n",
      "First step is to set up the assistant which is up and running & then needs to be\n",
      "personalized with the next content that is added.\n",
      "2.\n",
      "User details and preferences are open ended ? or do we get proper inputs from the user\n",
      "on what he likes and what he doesn’t? Or do we have to extract user profile from the\n",
      "unstructured input that that they share and generate a user profile based on the inputs\n",
      "Ans: As I understand this needs to be done by AI when we pass the unstructured data to\n",
      "the user where it can parse the info and create a personalized user profile\n",
      "3.\n",
      "Then this profile which is stored should be used for custom response generation. -> This\n",
      "profile should be saved in memory and should be constantly updated.\n",
      "https://github.com/Shubhamsaboo/awesome-llm-apps/blob/main/local_chatgpt_with_memory/lo\n",
      "cal_chatgpt_memory.py\n",
      "What does it mean by custom retrieval logic? Contextual information here means whatever is\n",
      "the memory of LLM from that fetch the user preferences for the query asked and then generate\n",
      "the response to user queries.\n",
      "Deliverables:\n",
      "Once a new model is launched you should be able to upload unstructured data -> create custom\n",
      "user profile -> For every query retrieve most relevant preferences based on the user profile and\n",
      "prepare the response? The retrieval here is from memory right??\n",
      "What do you mean by any open source library? Likes of embed chain as well where building a\n",
      "RAG system is one highly effective.\n",
      "Streamlit/Shiny for UI Design.\n",
      "API functionality, Load, data types for unstructured data - unittest\n",
      "Vector DB, FAISS - can store.\n",
      "Memory Store - MongoDB\n",
      "For pdf unstructured library\n",
      "Questions and Answers\n",
      "Question: Can I use the likes of OpenAI Models or something local?\n",
      "Answer: Yes, you are free to use any model.\n",
      "Question: Currently, can we restrict the scope to PDFs and blogs? If not, what are the\n",
      "different unstructured data sources the Assistant should handle?\n",
      "Answer: Sure, restricting to PDFs and blogs is fine for the task.\n",
      "Question: What specific user details and preferences should the AI Assistant extract and\n",
      "store from the unstructured data?\n",
      "Answer: User details and preferences are kind of open-ended, and this is where you can get a\n",
      "lot creative.\n",
      "Question: Are there any restrictions or guidelines on which open-source libraries can be\n",
      "used for this project?\n",
      "Answer: There is no restriction on using open-source libraries. Feel free to use any library of\n",
      "your choice.\n",
      "Bonus:\n",
      "Question: Do you have any specific requirements or expectations for the custom retrieval\n",
      "logic?\n",
      "Answer: No specific requirements. Again, we want you to be smart and creative about how to do\n",
      "the retrieval of memories.\n",
      "Question: Should the retrieval logic prioritize recent interactions (chat history based) or a\n",
      "specific type of user information (user information retrieval from data uploaded)? Or\n",
      "should it focus on profile-relevant information extraction?\n",
      "Answer: Great question. The candidate is expected to take a call on this based on their\n",
      "understanding of how humans think about memory.\n",
      "Mem0 Interview Project\n",
      "Goal\n",
      "Develop a personalized AI Assistant that can understand and remember user details and\n",
      "preferences from unstructured data sources, providing a tailored conversational experience.\n",
      "Requirements\n",
      "●\n",
      "Data Ingestion: The AI Assistant should allow users to input various unstructured data\n",
      "sources, such as blog post content, personal bio etc.\n",
      "●\n",
      "User Profiling: From the ingested data, the AI Assistant must deduce relevant user details\n",
      "and preferences, creating a personalized user profile.\n",
      "●\n",
      "Contextual Response Generation: During conversations, the AI Assistant should recall and\n",
      "leverage the user's profile information to provide contextualized and personalized\n",
      "responses.\n",
      "●\n",
      "End-to-End Functionality: Implement a chat CLI where the user inputs natural language\n",
      "queries, which can perform the following tasks:\n",
      "○\n",
      "Save something in memory\n",
      "○\n",
      "Deduce memory from unstructured text\n",
      "○\n",
      "Update memory\n",
      "○\n",
      "Delete memory\n",
      "Process the user input with your algorithm and return relevant responses.\n",
      "Bonus\n",
      "●\n",
      "Custom Retrieval Logic: Develop a unique logic for fetching the most relevant contextual\n",
      "information in response to user queries.\n",
      "●\n",
      "UI Design: Showcase your design prowess by creating a custom UI to show off your full\n",
      "stack skills.\n",
      "●\n",
      "Innovation Encouraged: We urge you to think outside the box. Impress us with your\n",
      "creativity and technical skills. The sky is merely the starting point.\n",
      "●\n",
      "Unit tests: Add unit tests if you can. We would love to see unit tests for your functionality.\n",
      "Deliverables\n",
      "●\n",
      "CLI interface: Develop a fully functional CLI where a user can interact with the AI assistant,\n",
      "ask questions, and store relevant information in memory for personalization.\n",
      "●\n",
      "Architecture Diagram: Provide a screenshot of an architecture diagram, created with tools\n",
      "like Excalidraw or using pen paper, detailing your strategy.\n",
      "●\n",
      "Code Quality: Submit well-documented code as a GitHub repository, accompanied by a\n",
      "comprehensive README. Ensure your code is of high quality.\n",
      "●\n",
      "GitHub Repository: Send us a link to your GitHub repository so that we can checkout the\n",
      "code.\n",
      "●\n",
      "Areas of improvements: While we understand that this is just an assignment, tell us what\n",
      "approaches you might have taken if you had one month to work on this problem\n",
      "statement to improve the memory system.\n",
      "Getting started\n",
      "●\n",
      "Feel free to use any open source library of your choice.\n",
      "Timeline\n",
      "●\n",
      "We have tested and found that the project can be fully finished in less than 72 hours.\n",
      "●\n",
      "Nevertheless, what matters is the final quality of the deliverable and showing a well\n",
      "thought methodology, proper working and task completion accuracy.\n",
      "●\n",
      "You have 4 days to submit the project from when you start (please send an email to\n",
      "founders@mem0.ai when you start the project to start tracking).\n",
      "ID\n",
      "Name\n",
      "Age\n",
      "Department\n",
      "Salary\n",
      "101\n",
      "Alice\n",
      "25\n",
      "HR\n",
      "50000\n",
      "102\n",
      "Bob\n",
      "30\n",
      "Engineering\n",
      "70000\n",
      "103\n",
      "Charlie\n",
      "35\n",
      "Marketing\n",
      "60000\n",
      "104\n",
      "David\n",
      "40\n",
      "Sales\n",
      "65000\n",
      "105\n",
      "Eve\n",
      "28\n",
      "Finance\n",
      "72000\n",
      "ID\n",
      "Name Age Departmen\n",
      "t\n",
      "Salar\n",
      "y\n",
      "101 Alice\n",
      "25\n",
      "HR\n",
      "50000\n",
      "102 Bob\n",
      "30\n",
      "Engineerin\n",
      "g\n",
      "70000\n",
      "103 Charli\n",
      "e\n",
      "35\n",
      "Marketing\n",
      "60000\n",
      "104 David 40\n",
      "Sales\n",
      "65000\n",
      "105 Eve\n",
      "28\n",
      "Finance\n",
      "72000\n",
      "\n",
      "Images extracted to directory: extracted_images\n",
      "Tables extracted to directory: extracted_tables\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_images(pdf_path, image_dir):\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "        \n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_number in range(len(doc)):\n",
    "        for img_index, img in enumerate(doc.get_page_images(page_number)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_path = f\"{image_dir}/page{page_number+1}_img{img_index+1}.{image_ext}\"\n",
    "            with open(image_path, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "\n",
    "def extract_tables(pdf_path, table_dir):\n",
    "    if not os.path.exists(table_dir):\n",
    "        os.makedirs(table_dir)\n",
    "        \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "            for j, table in enumerate(tables):\n",
    "                if table:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                    table_path = os.path.join(table_dir, f\"table_page{i+1}_{j+1}.csv\")\n",
    "                    df.to_csv(table_path, index=False)\n",
    "\n",
    "pdf_path = \"text_image_table.pdf\"  # Path to the uploaded PDF\n",
    "image_dir = \"extracted_images\"\n",
    "table_dir = \"extracted_tables\"\n",
    "\n",
    "# Extract text\n",
    "text = extract_text(pdf_path)\n",
    "print(\"Extracted Text:\\n\", text)\n",
    "\n",
    "# Extract images\n",
    "extract_images(pdf_path, image_dir)\n",
    "print(f\"Images extracted to directory: {image_dir}\")\n",
    "\n",
    "# Extract tables\n",
    "extract_tables(pdf_path, table_dir)\n",
    "print(f\"Tables extracted to directory: {table_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static content fetched successfully.\n",
      "Text saved to file: selenium/content.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import os\n",
    "\n",
    "def fetch_static_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch the page: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_dynamic_content(url):\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Adjust this based on the complexity of the page\n",
    "        page_content = driver.page_source\n",
    "        driver.quit()\n",
    "        return page_content\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the page with Selenium: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = \"\\n\".join([para.get_text() for para in paragraphs])\n",
    "    return text\n",
    "\n",
    "def save_text(text, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(f\"Text saved to file: {filename}\")\n",
    "\n",
    "def extract_and_save_blog_content(url, base_dir):\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        \n",
    "    # Try fetching static content first\n",
    "    content = fetch_static_content(url)\n",
    "    if content:\n",
    "        print(\"Static content fetched successfully.\")\n",
    "    else:\n",
    "        print(\"Switching to dynamic content extraction.\")\n",
    "        content = fetch_dynamic_content(url)\n",
    "\n",
    "    if content:\n",
    "        text = extract_text(content)\n",
    "        text_filename = os.path.join(base_dir, \"content.txt\")\n",
    "        save_text(text, text_filename)\n",
    "    else:\n",
    "        print(\"Failed to fetch content from the URL.\")\n",
    "\n",
    "# Example usage\n",
    "url = 'https://deshraj.xyz/'\n",
    "url = 'https://taranjeet.co/about/'\n",
    "\n",
    "base_dir = \"selenium\"  # Folder name to save the content\n",
    "extract_and_save_blog_content(url, base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static content fetched successfully.\n",
      "Text saved to file: playwright/content.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.sync_api import sync_playwright\n",
    "import os\n",
    "\n",
    "def fetch_static_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch the page: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_dynamic_content_with_playwright(url):\n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            page = browser.new_page()\n",
    "            page.goto(url)\n",
    "            page.wait_for_timeout(3000)  # Adjust this based on the complexity of the page\n",
    "            content = page.content()\n",
    "            browser.close()\n",
    "            return content\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the page with Playwright: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = \"\\n\".join([para.get_text() for para in paragraphs])\n",
    "    return text\n",
    "\n",
    "def save_text(text, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(f\"Text saved to file: {filename}\")\n",
    "\n",
    "def extract_and_save_blog_content(url, base_dir):\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        \n",
    "    # Try fetching static content first\n",
    "    content = fetch_static_content(url)\n",
    "    if content:\n",
    "        print(\"Static content fetched successfully.\")\n",
    "    else:\n",
    "        print(\"Switching to dynamic content extraction.\")\n",
    "        content = fetch_dynamic_content_with_playwright(url)\n",
    "\n",
    "    if content:\n",
    "        text = extract_text(content)\n",
    "        text_filename = os.path.join(base_dir, \"content.txt\")\n",
    "        save_text(text, text_filename)\n",
    "    else:\n",
    "        print(\"Failed to fetch content from the URL.\")\n",
    "\n",
    "# Example usage\n",
    "url = 'https://deshraj.xyz/'\n",
    "url = 'https://taranjeet.co/about/'\n",
    "\n",
    "base_dir = \"playwright\"  # Folder name to save the content\n",
    "extract_and_save_blog_content(url, base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static content fetched successfully.\n",
      "Fetched HTML content:\n",
      " <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=X-UA-Compatible content=\"IE=edge,chrome=1\"> <meta name=viewport content=\"width=device-width, initial-scale=1\"> <meta name=description content=\"Senior Product Manager adroit in Software Engineering\"> <meta name=author content=\"Taranjeet Singh\"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>About | Taranjeet Singh</title> <meta name=\"generator\" content=\"Jekyll v3.9.5\" /> <meta property=\"og:title\" content=\"About\" /> <meta property=\"og:locale\" content=\"en_US\" /> <link rel=\"canonical\" href=\"https://taranjeet.co/about/\" /> <meta property=\"og:url\" content=\"https://taranjeet.co/about/\" /> <meta property=\"og:site_name\" content=\"Taranjeet Singh\" /> <meta property=\"og:type\" content=\"website\" /> <meta name=\"twitter:card\" content=\"summary\" /> <meta property=\"twitter:title\" content=\"About\" /> <meta name=\"twitter:site\" content=\"@\" /> <script type=\"application/ld+json\"> {\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"hea\n",
      "Text saved to file: selenium/content.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import os\n",
    "\n",
    "def fetch_static_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Static content fetched successfully.\")\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch static content: Status code {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch the page: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_dynamic_content(url):\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Increase wait time if needed\n",
    "        page_content = driver.page_source\n",
    "        driver.quit()\n",
    "        print(\"Dynamic content fetched successfully.\")\n",
    "        return page_content\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the page with Selenium: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(html_content):\n",
    "    # Print the HTML content for inspection\n",
    "    print(\"Fetched HTML content:\\n\", html_content[:1000])  # Print the first 1000 characters for inspection\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    if paragraphs:\n",
    "        text = \"\\n\".join([para.get_text() for para in paragraphs])\n",
    "        return text\n",
    "    else:\n",
    "        print(\"No paragraphs found in the HTML content.\")\n",
    "        return None\n",
    "\n",
    "def save_text(text, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(f\"Text saved to file: {filename}\")\n",
    "\n",
    "def extract_and_save_blog_content(url, base_dir):\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        \n",
    "    # Try fetching static content first\n",
    "    content = fetch_static_content(url)\n",
    "    if not content:\n",
    "        print(\"Switching to dynamic content extraction.\")\n",
    "        content = fetch_dynamic_content(url)\n",
    "\n",
    "    if content:\n",
    "        text = extract_text(content)\n",
    "        if text:\n",
    "            text_filename = os.path.join(base_dir, \"content.txt\")\n",
    "            save_text(text, text_filename)\n",
    "        else:\n",
    "            print(\"Failed to extract text from the content.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch content from the URL.\")\n",
    "\n",
    "# Example usage\n",
    "url = 'https://deshraj.xyz/'\n",
    "url = 'https://taranjeet.co/about/'\n",
    "base_dir = \"selenium\"  # Folder name to save the content\n",
    "extract_and_save_blog_content(url, base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static content fetched successfully.\n",
      "[https://github.com/UMass-Rescue/IntelligentInformationExtractor/blob/main/model.py] Cleaned page size: 7035 characters, down from 10329 (shrunk: 3294 chars, 31.89%)\n",
      "Text saved to file: extracted_content/deshrajcontent.txt\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ensure you have the required libraries installed:\n",
    "# !pip install requests beautifulsoup4\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define a shared requests session\n",
    "session = requests.Session()\n",
    "\n",
    "def fetch_static_content(url):\n",
    "    \"\"\"Fetch static content from the URL using requests.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\",\n",
    "    }\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        print(\"Static content fetched successfully.\")\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch static content: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_dynamic_content(url):\n",
    "    \"\"\"Fetch dynamic content from the URL using Selenium.\"\"\"\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    import time\n",
    "\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Adjust this based on the complexity of the page\n",
    "        page_content = driver.page_source\n",
    "        driver.quit()\n",
    "        print(\"Dynamic content fetched successfully.\")\n",
    "        return page_content.encode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the page with Selenium: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_content(html, url):\n",
    "    \"\"\"Clean the HTML content by removing unwanted tags.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    original_size = len(str(soup.get_text()))\n",
    "\n",
    "    tags_to_exclude = [\n",
    "        \"nav\", \"aside\", \"form\", \"header\", \"noscript\", \"svg\", \"canvas\", \"footer\", \"script\", \"style\",\n",
    "    ]\n",
    "    for tag in soup(tags_to_exclude):\n",
    "        tag.decompose()\n",
    "\n",
    "    ids_to_exclude = [\"sidebar\", \"main-navigation\", \"menu-main-menu\"]\n",
    "    for id_ in ids_to_exclude:\n",
    "        tags = soup.find_all(id=id_)\n",
    "        for tag in tags:\n",
    "            tag.decompose()\n",
    "\n",
    "    classes_to_exclude = [\n",
    "        \"elementor-location-header\", \"navbar-header\", \"nav\", \"header-sidebar-wrapper\",\n",
    "        \"blog-sidebar-wrapper\", \"related-posts\",\n",
    "    ]\n",
    "    for class_name in classes_to_exclude:\n",
    "        tags = soup.find_all(class_=class_name)\n",
    "        for tag in tags:\n",
    "            tag.decompose()\n",
    "\n",
    "    content = soup.get_text()\n",
    "    cleaned_size = len(content)\n",
    "    if original_size != 0:\n",
    "        print(\n",
    "            f\"[{url}] Cleaned page size: {cleaned_size} characters, down from {original_size} (shrunk: {original_size-cleaned_size} chars, {round((1-(cleaned_size/original_size)) * 100, 2)}%)\"\n",
    "        )\n",
    "    \n",
    "    return content\n",
    "\n",
    "def save_text(content, filename):\n",
    "    \"\"\"Save the extracted text to a file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    print(f\"Text saved to file: {filename}\")\n",
    "\n",
    "def extract_and_save_blog_content(url, base_dir):\n",
    "    \"\"\"Extract and save the blog content from the URL.\"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        \n",
    "    # Try fetching static content first\n",
    "    content = fetch_static_content(url)\n",
    "    if not content:\n",
    "        print(\"Switching to dynamic content extraction.\")\n",
    "        content = fetch_dynamic_content(url)\n",
    "\n",
    "    if content:\n",
    "        cleaned_content = clean_content(content, url)\n",
    "        text_filename = os.path.join(base_dir, \"deshrajcontent.txt\")\n",
    "        save_text(cleaned_content, text_filename)\n",
    "    else:\n",
    "        print(\"Failed to fetch content from the URL.\")\n",
    "\n",
    "# Example usage\n",
    "url = 'https://deshraj.xyz/'  # Replace with the URL you want to test\n",
    "# url = \"\"\"https://github.com/UMass-Rescue/IntelligentInformationExtractor/blob/main/model.py\"\"\"\n",
    "# url = 'https://taranjeet.co/about/'\n",
    "\n",
    "base_dir = \"extracted_content\"  # Folder name to save the content\n",
    "extract_and_save_blog_content(url, base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUvnewriunvceiuwn\n",
      "<!DOCTYPE html><html><head>\n",
      "    <title>Deshraj Yadav</title>\n",
      "    <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\">\n",
      "    <link href=\"https://fonts.googleapis.com/css?family=Roboto:400,100,‌​100italic,300,300ita‌​lic,400italic,500,50‌​0italic,700,700itali‌​c,900italic,900\" rel=\"stylesheet\" type=\"text/css\">\n",
      "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css\" integrity=\"sha512-MV7K8+y+gLIBoVD59lQIYicR65iaqukzvf/nwasF0nqhPay5w/9lJmVM2hMDcnK1OnMGCdVK+iQrJ7lzPJQd1w==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\">\n",
      "\n",
      "    <link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\">\n",
      "    <link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"/favicon-32x32.png\">\n",
      "    <link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"/favicon-16x16.png\">\n",
      "    <link rel=\"manifest\" href=\"/site.webmanifest\">\n",
      "    <link rel=\"mask-icon\" href=\"/safari-pinned-tab.svg\" color=\"#5bbad5\">\n",
      "    <meta na\n",
      "Paragraphs\n",
      "👋 I'm Deshraj Yadav, Co-founder and CTO at Mem0 (f.k.a Embedchain). I am broadly interested in the field of Artificial Intelligence and Machine Learning Infrastructure.\n",
      "Paragraphs\n",
      "👷‍♂️ Previously, I was Senior Autopilot Engineer at Tesla Autopilot where I led the Autopilot's AI Platform which helped the Tesla Autopilot team to track large scale training and model evaluation experiments, provide monitoring and observability into jobs and training cluster issues.\n",
      "Paragraphs\n",
      "🚀 I had built EvalAI as my masters thesis at Georgia Tech (advised by Dhruv Batra and Devi Parikh), which is an open-source platform for evaluating and comparing machine learning and artificial intelligence algorithms 🤖 at scale.\n",
      "Paragraphs\n",
      "🏏 Outside of work, I am very much into cricket and play in two leagues (Cricbay and NACL) in San Francisco Bay Area.\n",
      "Paragraphs\n",
      "📬 I’m best reached via email. deshraj at mem0 dot ai. Social media links:\n",
      "Paragraphs\n",
      "(September 2023 - Present)\n",
      "Paragraphs\n",
      "(Jul 2019 - September 2023)\n",
      "Paragraphs\n",
      "Lead Autopilot's AI Platform that enables the team to:\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Previously, I worked in AI-Tooling team where I was responsible for designing and implementing a diverse set of tools like 3D-labelling platform, model versioning tool, and active learning based label boosting software.\n",
      "Paragraphs\n",
      "(Mar 2019 - Mar 2020)\n",
      "Paragraphs\n",
      "Caliper is the only platform that enables recruiters to evaluate practical AI skills. At scale. Through automated challenges on real-world problems.\n",
      "Paragraphs\n",
      "(May 2018 - Aug 2018)\n",
      "Paragraphs\n",
      "Building scalable end-to-end platform to train, test, and visualize machine learning algorithms on massive amounts of visual data.\n",
      "Paragraphs\n",
      "(Jun 2016 - May 2017)\n",
      "Paragraphs\n",
      "Worked on the problem of Visual Dialog which later got accepted in CVPR 2017 as a Spotlight Paper. During my visit, I also worked as Teaching Assistant for Intro. to Machine Learning Course for Fall-2016 instructed by Dr. Stefan Lee.\n",
      "Paragraphs\n",
      "(Mar 2016 - Present)\n",
      "Paragraphs\n",
      "Served as Organization Mentor for Google Summer of Code (GSoC) 2016. Later, I have been serving as an Organization Administrator in GSoC 2017, 2018, 2019, and 2020.\n",
      "Paragraphs\n",
      "(Apr 2015 - Aug 2015)\n",
      "Paragraphs\n",
      "Selected for GSOC 2015 under CloudCV Organization. My responsibilities included integrating NVIDIA's Deep Learning Framework DIGITS to provide workspaces for the researchers and other computer vision developers.\n",
      "Paragraphs\n",
      "(Dec 2015 - Feb 2016)\n",
      "Paragraphs\n",
      "Responsible for setting an automated test suite for Continuous Integration in the backend codebase which led to a very lean software engineering team as the team was able to spend more time on building new features rather than maintaining and fixing bugs and issues.\n",
      "Paragraphs\n",
      "(Sep 2015 - Dec 2015)\n",
      "Paragraphs\n",
      "Worked on building an IoT based product for indoor navigation in big offices and malls using Beacons (Low Energy Bluetooth devices). I was responsible for building REST APIs and creating the Android Application from scratch which communicates with the beacons to get real time position of a person using beacons signal strength.\n",
      "Paragraphs\n",
      "(Aug 2014 - Dec 2014)\n",
      "Paragraphs\n",
      "Responsible for the development of the whole product from scratch, managing the server, database management and other backend scripting.\n",
      "Paragraphs\n",
      "(May 2014 - Jul 2014)\n",
      "Paragraphs\n",
      "Responsible for development of company's web application, Facebook App development and setting up NoSQL database. Implemented scripts to do image enhancement, and cropping borders etc.\n",
      "Paragraphs\n",
      "Specialization in Machine Learning\n",
      "Paragraphs\n",
      "Expected Graduation: December 2018\n",
      "Paragraphs\n",
      "GPA: 3.7/4.0\n",
      "Paragraphs\n",
      "Passed with an aggregate of 78% (with Hons.)\n",
      "Paragraphs\n",
      "Class Rank: 9th rank (or top-6%)\n",
      "Paragraphs\n",
      "Open source platform to create, collaborate and participate in the AI Challenges organized around the globe.\n",
      "Paragraphs\n",
      "Proof of concept of a central, standardized open source platform for human-in-the-loop evaluation of AI agents. Developed the infrastructure to pair Amazon Mechanical Turk (AMT) users in real-time with artificial visual dialog agents.\n",
      "Paragraphs\n",
      "Fabrik is an online collaborative platform to build, visualize and train deep learning models via a simple drag-and-drop interface.\n",
      "Paragraphs\n",
      "Built a visual chatbot which can hold a meaningful dialog with humans natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the chatbot has to ground the question in image, infer context from history, and answer the question accurately.\n",
      "Paragraphs\n",
      "In Visual Question Answering, given an image and a free-form natural language question about the image (e.g., \"What kindof store is this?\", \"How many people are waiting in the queue?\", \"Is it safe to cross the street?\") the model's task is to automatically produce a concise, accurate, free-form, natural language answer (\"bakery\", \"5\", \"Yes\"). This demo is implemented using Hierarchical Question-Image Co-Attention model.\n",
      "Paragraphs\n",
      "Gradient-weighted Class Activation Mapping (Grad-CAM) is a novel class-discriminative localization technique, that can be used to make CNN based models interpretable. Grad-CAM highlights regions of the image the VQA model looks at while making predictions. Given an image and a caption or question about that image, the model shows where it looked while doing prediction.\n",
      "Paragraphs\n",
      "Origami is an AI-as-a-service that allows researchers to easily convert their deep learning models into an online service that is widely accessible to everyone without the need to setup the infrastructure, resolve the dependencies, and build a web service around the deep learning model.\n",
      "Paragraphs\n",
      "Beam search, the standard work-horse for decoding outputs from neural sequence models like RNNs produces generic and uninteresting sequences. This is inadequate for AI tasks with inherent ambiguity — for example, there can be multiple correct ways of describing the contents of an image. The demo overcomes this by proposing a diversity-promoting replacement, Diverse Beam Search that produces sequences that are significantly different — with runtime and memory requirements comparable to beam search.\n",
      "Paragraphs\n",
      "Web based application that acts as a medium for interaction between all students, faculties and management of JSSATE Noida college. Officially recognized by college. Serves 10,000+ requests per day, 20,000+ users, 20,000+ notices uploaded.\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Deshraj Yadav, Rishabh Jain, Harsh Agrawal, Prithvijit Chattopadhyay, Taranjeet Singh, Akash Jain, Shiv Baran Singh, Stefan Lee, Dhruv Batra\n",
      "arXiv preprint\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Project Webpage Github repository\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Utsav Garg, Viraj Prabhu, Deshraj Yadav, Ram Ramrakhya, Harsh Agrawal, Dhruv Batra\n",
      "arXiv preprint\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Github repository\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Arjun Chandrasekaran*, Viraj Prabhu*, Deshraj Yadav*, Prithvijit Chattopadhyay*, Devi Parikh\n",
      "EMNLP 2018\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Prithvijit Chattopadhyay*, Deshraj Yadav*, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh\n",
      "HCOMP 2017\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Code\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Arjun Chandrasekaran*, Deshraj Yadav*, Prithvijit Chattopadhyay*, Viraj Prabhu*, Devi Parikh\n",
      "Chalearn Looking at People Workshop, CVPR 2017\n",
      "Paragraphs\n",
      "\n",
      "Paragraphs\n",
      "Demo Code\n",
      "Paragraphs\n",
      "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M.F. Moura, Devi Parikh, Dhruv Batra\n",
      "CVPR 2017 - Spotlight\n",
      "Paragraphs\n",
      "Demo Project Webpage Visual Chatbot Code Data Collection Code\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio if running in a Jupyter notebook or interactive environment\n",
    "if asyncio.get_event_loop().is_running():\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "# Create the asynchronous HTML session\n",
    "session = AsyncHTMLSession()\n",
    " \n",
    "# Define the URL\n",
    "# url = 'https://www.geeksforgeeks.org/how-to-create-and-use-env-files-in-python/'\n",
    "\n",
    "url = 'https://deshraj.xyz/'  # Replace with the URL you want to test\n",
    "# url = \"\"\"https://github.com/UMass-Rescue/IntelligentInformationExtractor/blob/main/model.py\"\"\"\n",
    "# url = 'https://taranjeet.co/about/'\n",
    "\n",
    "async def fetch_and_render(url):\n",
    "    # Fetch the page\n",
    "    r = await session.get(url)\n",
    "\n",
    "    # Render the page\n",
    "    await r.html.arender(sleep=1, keep_page=True, scrolldown=1)\n",
    "\n",
    "    # Print the rendered HTML (for debugging purposes)\n",
    "    print(\"NUvnewriunvceiuwn\")\n",
    "    print(r.html.html[:1000])  # Print the first 1000 characters of the HTML content\n",
    "\n",
    "    # Extract and print text content\n",
    "    paragraphs = r.html.find('p')\n",
    "    for p in paragraphs:\n",
    "        print(\"Paragraphs\")\n",
    "        print(p.text)\n",
    "\n",
    "# Run the asynchronous function\n",
    "asyncio.run(fetch_and_render(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Apply nest_asyncio if running in a Jupyter notebook or interactive environment\n",
    "if asyncio.get_event_loop().is_running():\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "# Create the asynchronous HTML session\n",
    "session = AsyncHTMLSession()\n",
    "\n",
    "# Define the URL\n",
    "\n",
    "\n",
    "\n",
    "async def fetch_and_save_html(url):\n",
    "    # Fetch the page\n",
    "    r = await session.get(url)\n",
    "\n",
    "    # Render the page\n",
    "    await r.html.arender(sleep=1, keep_page=True, scrolldown=1)\n",
    "\n",
    "    # Get the entire HTML content\n",
    "    html_content = r.html.html\n",
    "\n",
    "    # Save the HTML content to a file\n",
    "    with open('rendered_page.html', 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "    print(\"HTML content saved to 'rendered_page.html'.\")\n",
    "\n",
    "    return html_content\n",
    "\n",
    "async def clean_html():\n",
    "    # Load the saved HTML content using BeautifulSoup\n",
    "    with open('rendered_page.html', 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Example of cleaning: remove all script and style tags\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # Save the cleaned content\n",
    "    with open('cleaned_page.html', 'w', encoding='utf-8') as file:\n",
    "        file.write(str(soup.prettify()))\n",
    "    print(\"Cleaned HTML content saved to 'cleaned_page.html'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved to 'extracted_html_files/saivineethkumar.github.io/rendered_page.html'.\n",
      "Cleaned HTML content saved to 'extracted_html_files/saivineethkumar.github.io/cleaned_page.html'.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "\n",
    "# Apply nest_asyncio if running in a Jupyter notebook or interactive environment\n",
    "if asyncio.get_event_loop().is_running():\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "# Create the asynchronous HTML session\n",
    "session = AsyncHTMLSession()\n",
    "\n",
    "# Function to extract a safe folder name from a URL\n",
    "def url_to_folder_name(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    folder_name = parsed_url.netloc + parsed_url.path.replace(\"/\", \"_\")\n",
    "    return folder_name.strip('_')\n",
    "\n",
    "# Define the main function to fetch and save HTML content\n",
    "async def fetch_and_save_html(url):\n",
    "    # Fetch the page\n",
    "    r = await session.get(url)\n",
    "\n",
    "    # Render the page\n",
    "    await r.html.arender(sleep=1, keep_page=True, scrolldown=1)\n",
    "\n",
    "    # Get the entire HTML content\n",
    "    html_content = r.html.html\n",
    "\n",
    "    # Create directory for saving the files\n",
    "    folder_name = url_to_folder_name(url)\n",
    "    base_dir = Path(f\"extracted_html_files/{folder_name}\")\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the HTML content to a file\n",
    "    rendered_file_path = base_dir / 'rendered_page.html'\n",
    "    with open(rendered_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "    print(f\"HTML content saved to '{rendered_file_path}'.\")\n",
    "\n",
    "    return rendered_file_path\n",
    "\n",
    "async def clean_html(rendered_file_path):\n",
    "    # Load the saved HTML content using BeautifulSoup\n",
    "    with open(rendered_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Example of cleaning: remove all script and style tags\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # Save the cleaned content\n",
    "    cleaned_file_path = rendered_file_path.parent / 'cleaned_page.html'\n",
    "    with open(cleaned_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(soup.prettify()))\n",
    "    print(f\"Cleaned HTML content saved to '{cleaned_file_path}'.\")\n",
    "\n",
    "# Define a function to run the entire process\n",
    "async def process_url(url):\n",
    "    rendered_file_path = await fetch_and_save_html(url)\n",
    "    await clean_html(rendered_file_path)\n",
    "\n",
    "# Example usage\n",
    "url = 'https://deshraj.xyz/'  # Replace with the URL you want to test\n",
    "url = 'https://saivineethkumar.github.io/'\n",
    "# url = \"\"\"https://github.com/UMass-Rescue/IntelligentInformationExtractor/blob/main/model.py\"\"\"\n",
    "# url = 'https://taranjeet.co/about/'asyncio.run(process_url(url))\n",
    "\n",
    "asyncio.run(process_url(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Connection is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://saivineethkumar.github.io/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# url = 'https://taranjeet.co/about/'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run the asynchronous function to fetch and save HTML\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# html_content = asyncio.run(fetch_and_save_html(url))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Clean the saved HTML content\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[46], line 64\u001b[0m, in \u001b[0;36mprocess_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_url\u001b[39m(url):\n\u001b[0;32m---> 64\u001b[0m     rendered_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fetch_and_save_html(url)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m clean_html(rendered_file_path)\n",
      "Cell \u001b[0;32mIn[46], line 27\u001b[0m, in \u001b[0;36mfetch_and_save_html\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     24\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Render the page\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39marender(sleep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keep_page\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scrolldown\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Get the entire HTML content\u001b[39;00m\n\u001b[1;32m     30\u001b[0m html_content \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39mhtml\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/requests_html.py:626\u001b[0m, in \u001b[0;36mHTML.arender\u001b[0;34m(self, retries, script, wait, scrolldown, sleep, reload, timeout, keep_page)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content:\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m         content, result, page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_render(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, script\u001b[38;5;241m=\u001b[39mscript, sleep\u001b[38;5;241m=\u001b[39msleep, wait\u001b[38;5;241m=\u001b[39mwait, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml, reload\u001b[38;5;241m=\u001b[39mreload, scrolldown\u001b[38;5;241m=\u001b[39mscrolldown, timeout\u001b[38;5;241m=\u001b[39mtimeout, keep_page\u001b[38;5;241m=\u001b[39mkeep_page)\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/requests_html.py:505\u001b[0m, in \u001b[0;36mHTML._async_render\u001b[0;34m(self, url, script, scrolldown, sleep, wait, reload, content, timeout, keep_page)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Handle page creation and js rendering. Internal use for render/arender methods. \"\"\"\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser\u001b[38;5;241m.\u001b[39mnewPage()\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# Wait before rendering the page, to prevent timeouts.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(wait)\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/pyppeteer/browser.py:202\u001b[0m, in \u001b[0;36mBrowser.newPage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnewPage\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Page:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make new page on this browser and return its object.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_defaultContext\u001b[38;5;241m.\u001b[39mnewPage()\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/pyppeteer/browser.py:358\u001b[0m, in \u001b[0;36mBrowserContext.newPage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnewPage\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Page:\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new page in the browser context.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_browser\u001b[38;5;241m.\u001b[39m_createPageInContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/pyppeteer/browser.py:209\u001b[0m, in \u001b[0;36mBrowser._createPageInContext\u001b[0;34m(self, contextId)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m contextId:\n\u001b[1;32m    207\u001b[0m     options[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserContextId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m contextId\n\u001b[0;32m--> 209\u001b[0m targetId \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTarget.createTarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargetId\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    211\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_targets\u001b[38;5;241m.\u001b[39mget(targetId)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/TakeHomeAssignments/Mem0/mem0_env/lib/python3.12/site-packages/pyppeteer/connection.py:85\u001b[0m, in \u001b[0;36mConnection.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Detect connection availability from the second transmission\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastId \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection is closed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "\u001b[0;31mConnectionError\u001b[0m: Connection is closed"
     ]
    }
   ],
   "source": [
    "# Define the URL\n",
    "# url = 'https://www.geeksforgeeks.org/how-to-create-and-use-env-files-in-python/'\n",
    "\n",
    "# url = 'https://deshraj.xyz/'  # Replace with the URL you want to test\n",
    "# url = \"\"\"https://github.com/UMass-Rescue/IntelligentInformationExtractor/blob/main/model.py\"\"\"\n",
    "url = 'https://saivineethkumar.github.io/'\n",
    "# url = 'https://taranjeet.co/about/'\n",
    "\n",
    "# Run the asynchronous function to fetch and save HTML\n",
    "# html_content = asyncio.run(fetch_and_save_html(url))\n",
    "\n",
    "# Clean the saved HTML content\n",
    "asyncio.run(process_url(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to 'extracted_html_files/saivineethkumar.github.io/extracted_text.txt'.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_html(filename):\n",
    "    \"\"\"Load HTML content from a file.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    return html_content\n",
    "\n",
    "def parse_text_from_html(html_content):\n",
    "    \"\"\"Parse text information from HTML content.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract text from paragraphs\n",
    "    paragraphs = soup.find_all('p')\n",
    "    paragraph_texts = [p.get_text() for p in paragraphs]\n",
    "\n",
    "    # Extract text from headings (h1, h2, h3, etc.)\n",
    "    headings = []\n",
    "    for i in range(1, 7):\n",
    "        for h in soup.find_all(f'h{i}'):\n",
    "            headings.append(h.get_text())\n",
    "\n",
    "    # Extract text from lists\n",
    "    lists = []\n",
    "    for ul in soup.find_all('ul'):\n",
    "        lists.append([li.get_text() for li in ul.find_all('li')])\n",
    "    \n",
    "    # Combine extracted text\n",
    "    extracted_text = {\n",
    "        \"paragraphs\": paragraph_texts,\n",
    "        \"headings\": headings,\n",
    "        \"lists\": lists\n",
    "    }\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "def save_extracted_text(extracted_text, filename):\n",
    "    \"\"\"Save the extracted text to a file.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for key, texts in extracted_text.items():\n",
    "            file.write(f\"### {key.upper()} ###\\n\\n\")\n",
    "            if isinstance(texts, list):\n",
    "                for text in texts:\n",
    "                    if isinstance(text, list):\n",
    "                        for item in text:\n",
    "                            file.write(f\"- {item}\\n\")\n",
    "                    else:\n",
    "                        file.write(f\"{text}\\n\")\n",
    "            file.write(\"\\n\\n\")\n",
    "    print(f\"Extracted text saved to '{filename}'.\")\n",
    "\n",
    "# Example usage\n",
    "html_filename = 'extracted_html_files/saivineethkumar.github.io/cleaned_page.html'  # Replace with your HTML file name\n",
    "extracted_text_filename = 'extracted_html_files/saivineethkumar.github.io/extracted_text.txt'  # Output file for the extracted text\n",
    "\n",
    "# Load HTML content\n",
    "html_content = load_html(html_filename)\n",
    "\n",
    "# Parse text information from the HTML content\n",
    "extracted_text = parse_text_from_html(html_content)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "save_extracted_text(extracted_text, extracted_text_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
