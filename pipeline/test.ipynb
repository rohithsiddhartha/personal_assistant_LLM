{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all text files to create chunks and embeddings...\n",
      "Processing /Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/extracted_contents/pranay/text/pranay.txt...\n",
      "Processing /Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/extracted_contents/aman.ai_primers_ai_ml-comp/extracted_text.txt...\n",
      "Processing /Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/extracted_contents/deshraj.xyz/extracted_text.txt...\n",
      "Processing /Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/extracted_contents/image_text/text/image_text.txt...\n",
      "Chunks and embeddings saved to /Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/final.csv\n",
      "All chunks and embeddings saved to /Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/final.csv\n",
      "All text chunks and embeddings are saved to /Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/final.csv\n"
     ]
    }
   ],
   "source": [
    "from TextProcessor import TextProcessor\n",
    "import os\n",
    "user_dir = '/Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha'\n",
    "text_processor = TextProcessor()\n",
    "output_csv = os.path.join(user_dir, \"final.csv\")\n",
    "print(\"Processing all text files to create chunks and embeddings...\")\n",
    "k = text_processor.process_directory(user_dir, output_csv)\n",
    "print(f\"All text chunks and embeddings are saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Hey there, this is Pranay!\n",
      "I'm a Master's student in Computer Science at the University of Massachusetts Amherst, focusing on Deep Learning and Computer\n",
      "Vision. My academic path has led me to explore the compelling crossroads of these fields, giving me the opportunity to delve deep and\n",
      "gain hands-on experience.\n",
      "Previously, I was a Graduate Student Researcher with Reality Labs team at Meta, collaborating with Dr. Shane Moon on the IMU2CLIP\n",
      "project. I also had the opportunity to be a research intern at AirLab, associated with the Robotics Institute at Carnegie Mellon University.\n",
      "There, I worked alongside Dr. Chen Wang and Dr. Sebastian Scherer on Few Shot Object Detection, and contributed towards PyPose.\n",
      "My initial steps into research were guided by Dr. Sumeet Kumar at the Indian School of Business Hyderabad, where I worked on\n",
      "identifying surrogate product placements in YouTube Kids videos using Few Shot Learning.\n",
      "Contact : pranayr@umass.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "News\n",
      "• Mar 2023 - PyPose was accepted at CVPR 2023. Stay tuned for more details!\n",
      "• Feb 2023 - Started working with Meta Reality Labs on the IMU2CLIP project.\n",
      "• Sep 2022 - Joined UMass Amherst for my masters in Computer Science\n",
      "• July 2022 - One paper accepted at ECCV 2022. Stay tuned for more details!\n",
      "• Feb 2022 - Acting as a reviewer for IEEE RA-L.\n",
      "• Sep 2021 - Started as an intern at AirLab, CMU.\n",
      "• May 2021 - Started as an intern at SRITNE, ISB Hyderabad.\n",
      "Experience\n",
      "Carnegie Mellon University\n",
      "Research Intern, AirLab\n",
      "Since Sep '21\n",
      "Indian School of Business\n",
      "Research Intern, SRITNE\n",
      "May '21 - Aug '22\n",
      "IIITDM Jabalpur\n",
      "B.Tech in Electronics and Communication\n",
      "Aug '18 - May '22\n",
      "Publications\n",
      "AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration\n",
      "Bowen Li, Chen Wang, Pranay Reddy, Seungchan Kim, Sebastian Scherer. [ECCV2022, Accepted.]\n",
      "Leverage class-agnostic relation to implement few-shot detection w/o fine-tuning, which enables fast and efficient robotic\n",
      "exploration.\n",
      "Paper\n",
      "Code\n",
      "Webpage\n",
      "PyPose: A Library for Robot Learning with Physics-based Optimization\n",
      "Chen Wang, Dasong Gao, Kuan Xu, Junyi Geng, Yaoyu Hu, Yuheng Qiu, Bowen Li, Fan Yang, Brady Moon, Abhinav Pandey,\n",
      "Aryan, Jiahe Xu, Tianhao Wu, Haonan He, D aning Huang, Zhongqiang Ren, Shibo Zhao, Taimeng Fu, Pranay Reddy, Xiao\n",
      "Lin, Wenshan Wang, Jingnan Shi, Rajat Talak, Han Wang, Huai Yu, Shanzhao Wang, Ananth Kashyap, Rohan Bandaru,\n",
      "Karthik Dantu, Jiajun Wu, Luca Carlone, Marco Hutter, Sebastian Scherer. [CVPR 2023, Accepted.]\n",
      "An open source library that connects classic robotics with modern learning methods seamlessly.\n",
      "Paper\n",
      "Code\n",
      "Webpage\n",
      " \n",
      "\n",
      " Aman's AI Journal • ML Algorithms Comparative Analysis\n",
      " \n",
      " Back to Top\n",
      " \n",
      " Distilled AI\n",
      " \n",
      " Back to aman.ai\n",
      " \n",
      " ML Algorithms Comparative Analysis\n",
      " \n",
      " Overview\n",
      " \n",
      " Classification Algorithms\n",
      " \n",
      " Logistic Regression\n",
      " \n",
      " Naive Bayes Classifier\n",
      " \n",
      " Regression Algorithms\n",
      " \n",
      " Linear Regression\n",
      " \n",
      " Classification and Regression Algorithms\n",
      " \n",
      " K-Nearest Neighbors\n",
      " \n",
      " Support Vector Machines\n",
      " \n",
      " Explain the Kernel Trick in SVM and Why We Use It and How to Choose What Kernel to Use?\n",
      " \n",
      " Decision Trees\n",
      " \n",
      " Model Ensembles\n",
      " \n",
      " Bagging and Boosting\n",
      " \n",
      " Bootstrapping\n",
      " \n",
      " Bagging\n",
      " \n",
      " Boosting\n",
      " \n",
      " Bagging vs. Boosting\n",
      " \n",
      " Random Forests\n",
      " \n",
      " Gradient Boosting\n",
      " \n",
      " XGBoost\n",
      " \n",
      " Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means\n",
      " \n",
      " What is K-Means Clustering?\n",
      " \n",
      " What is Latent Dirichlet Allocation (LDA)?\n",
      " \n",
      " Machine Learning or Deep Learning?\n",
      " \n",
      " Hidden Markov Model (HMM) in Machine Learning\n",
      " \n",
      " What is a Hidden Markov Model?\n",
      " \n",
      " Key Components of an HMM\n",
      " \n",
      " Applications of HMMs in NLP\n",
      " \n",
      " Pros and Cons of HMMs\n",
      " \n",
      " Summary\n",
      " \n",
      " References\n",
      " \n",
      " Citation\n",
      " \n",
      " Overview\n",
      " \n",
      " The figure below\n",
      " \n",
      " (source)\n",
      " \n",
      " offers a deep dive on the pros and cons of the most commonly used regression and classification algorithms and the use-cases for each.\n",
      " \n",
      " Classification Algorithms\n",
      " \n",
      " Logistic Regression\n",
      " \n",
      " To start off here, Logistic Regression is a misnomer as it does not pertain to a regression problem at all.\n",
      " \n",
      " Logistic regression estimates the probability of an event occurring based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1.\n",
      " \n",
      " You can use your returned value in one of two ways:\n",
      " \n",
      " You may just need an output of 0 or 1 and use it “as is”.\n",
      " \n",
      " Say your model predicts the probability that your baby will cry at night as:\n",
      " \n",
      " p\n",
      " \n",
      " (\n",
      " \n",
      " c\n",
      " \n",
      " r\n",
      " \n",
      " y\n",
      " \n",
      " |\n",
      " \n",
      " n\n",
      " \n",
      " i\n",
      " \n",
      " g\n",
      " \n",
      " h\n",
      " \n",
      " t\n",
      " \n",
      " )\n",
      " \n",
      " =\n",
      " \n",
      " 0.05\n",
      " \n",
      " p\n",
      " \n",
      " (\n",
      " \n",
      " c\n",
      " \n",
      " r\n",
      " \n",
      " y\n",
      " \n",
      " |\n",
      " \n",
      " n\n",
      " \n",
      " i\n",
      " \n",
      " g\n",
      " \n",
      " h\n",
      " \n",
      " t\n",
      " \n",
      " )\n",
      " \n",
      " =\n",
      " \n",
      " 0.05\n",
      " \n",
      " Let’s leverage this information to see how many times a year you’ll have to wake up to soothe your baby:\n",
      " \n",
      " w\n",
      " \n",
      " a\n",
      " \n",
      " k\n",
      " \n",
      " e\n",
      " \n",
      " U\n",
      " \n",
      " p\n",
      " \n",
      " =\n",
      " \n",
      " p\n",
      " \n",
      " (\n",
      " \n",
      " c\n",
      " \n",
      " r\n",
      " \n",
      " y\n",
      " \n",
      " |\n",
      " \n",
      " n\n",
      " \n",
      " i\n",
      " \n",
      " g\n",
      " \n",
      " h\n",
      " \n",
      " t\n",
      " \n",
      " s\n",
      " \n",
      " )\n",
      " \n",
      " ∗\n",
      " \n",
      " n\n",
      " \n",
      " i\n",
      " \n",
      " g\n",
      " \n",
      " h\n",
      " \n",
      " t\n",
      " \n",
      " s\n",
      " \n",
      " =\n",
      " \n",
      " 0.05\n",
      " \n",
      " ∗\n",
      " \n",
      " 365\n",
      " \n",
      " =\n",
      " \n",
      " 18\n",
      " \n",
      " days\n",
      " \n",
      " w\n",
      " \n",
      " a\n",
      " \n",
      " k\n",
      " \n",
      " e\n",
      " \n",
      " U\n",
      " \n",
      " p\n",
      " \n",
      " =\n",
      " \n",
      " p\n",
      " \n",
      " (\n",
      " \n",
      " c\n",
      " \n",
      " r\n",
      " \n",
      " y\n",
      " \n",
      " |\n",
      " \n",
      " n\n",
      " \n",
      " i\n",
      " \n",
      " g\n",
      " \n",
      " h\n",
      " \n",
      " t\n",
      " \n",
      " s\n",
      " \n",
      " )\n",
      " \n",
      " ∗\n",
      " \n",
      " n\n",
      " \n",
      " i\n",
      " \n",
      " g\n",
      " \n",
      " h\n",
      " \n",
      " t\n",
      " \n",
      " s\n",
      " \n",
      " =\n",
      " \n",
      " 0.05\n",
      " \n",
      " ∗\n",
      " \n",
      " 365\n",
      " \n",
      " =\n",
      " \n",
      " 18\n",
      " \n",
      " days\n",
      " \n",
      " Or you may want to convert it into a binary category such as: spam or not spam and convert it to a binary classification problem.\n",
      " \n",
      " In this scenario, you’d want an accurate prediction of 0 or 1 and no probabilities in between.\n",
      " \n",
      " The best way to obtain this is by leveraging the sigmoid function as it guarantees a value between 0 and 1.\n",
      " \n",
      " Sigmoid function is represented as:\n",
      " \n",
      " y\n",
      " \n",
      " ′\n",
      " \n",
      " =\n",
      " \n",
      " 1\n",
      " \n",
      " 1\n",
      " \n",
      " +\n",
      " \n",
      " e\n",
      " \n",
      " −\n",
      " \n",
      " z\n",
      " \n",
      " y\n",
      " \n",
      " ′\n",
      " \n",
      " =\n",
      " \n",
      " 1\n",
      " \n",
      " 1\n",
      " \n",
      " +\n",
      " \n",
      " e\n",
      " \n",
      " −\n",
      " \n",
      " z\n",
      " \n",
      " where,\n",
      " \n",
      " y\n",
      " \n",
      " ′\n",
      " \n",
      " y\n",
      " \n",
      " ′\n",
      " \n",
      " is the output of the logistic regression model for a particular example.\n",
      " \n",
      " z\n",
      " \n",
      " =\n",
      " \n",
      " b\n",
      " \n",
      " +\n",
      " \n",
      " w\n",
      " \n",
      " 1\n",
      " \n",
      " x\n",
      " \n",
      " 1\n",
      " \n",
      " +\n",
      " \n",
      " w\n",
      " \n",
      " 2\n",
      " \n",
      " x\n",
      " \n",
      " 2\n",
      " \n",
      " +\n",
      " \n",
      " …\n",
      " \n",
      " +\n",
      " \n",
      " w\n",
      " \n",
      " N\n",
      " \n",
      " x\n",
      " \n",
      " N\n",
      " \n",
      " z\n",
      " \n",
      " =\n",
      " \n",
      " b\n",
      " \n",
      " +\n",
      " \n",
      " w\n",
      " \n",
      " 1\n",
      " \n",
      " x\n",
      " \n",
      " 1\n",
      " \n",
      " +\n",
      " \n",
      " w\n",
      " \n",
      " 2\n",
      " \n",
      " x\n",
      " \n",
      " 2\n",
      " \n",
      " +\n",
      " \n",
      " …\n",
      " \n",
      " +\n",
      " \n",
      " w\n",
      " \n",
      " N\n",
      " \n",
      " x\n",
      " \n",
      " N\n",
      " \n",
      " The\n",
      " \n",
      " w\n",
      " \n",
      " w\n",
      " \n",
      " values are the model’s learned weights, and\n",
      " \n",
      " b\n",
      " \n",
      " b\n",
      " \n",
      " is the bias.\n",
      " \n",
      " The\n",
      " \n",
      " x\n",
      " \n",
      " x\n",
      " \n",
      " values are the feature values for a particular example.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Algorithm is quite simple and efficient.\n",
      " \n",
      " Provides concrete probability scores as output.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Bad at handling a large number of categorical features.\n",
      " \n",
      " It assumes that the data is free of missing values and predictors are independent of each other.\n",
      " \n",
      " Use case:\n",
      " \n",
      " Logistic regression is used when the dependent variable (target) is categorical as in binary classification problems.\n",
      " \n",
      " For example, To predict whether an email is spam (1) or (0) or whether the tumor is malignant (1) or not (0).\n",
      " \n",
      " Naive Bayes Classifier\n",
      " \n",
      " Naive Bayes is a supervised learning algorithms based on Bayes’ theorem which can serve as either a binary or multi-class classifier.\n",
      " \n",
      " It is termed “naive” because it makes the naive assumption of conditional independence between every pair of features given the value of the class variable.\n",
      " \n",
      " The figure below below\n",
      " \n",
      " (source)\n",
      " \n",
      " shows the equation for Bayes Theorem and its individual components:\n",
      " \n",
      " The thought behind naive Bayes classification is to try to classify the data by maximizing:\n",
      " \n",
      " P\n",
      " \n",
      " (\n",
      " \n",
      " O\n",
      " \n",
      " ∣\n",
      " \n",
      " C\n",
      " \n",
      " i\n",
      " \n",
      " )\n",
      " \n",
      " P\n",
      " \n",
      " (\n",
      " \n",
      " C\n",
      " \n",
      " i\n",
      " \n",
      " )\n",
      " \n",
      " P\n",
      " \n",
      " (\n",
      " \n",
      " O\n",
      " \n",
      " ∣\n",
      " \n",
      " C\n",
      " \n",
      " i\n",
      " \n",
      " )\n",
      " \n",
      " P\n",
      " \n",
      " (\n",
      " \n",
      " C\n",
      " \n",
      " i\n",
      " \n",
      " )\n",
      " \n",
      " where,\n",
      " \n",
      " O\n",
      " \n",
      " O\n",
      " \n",
      " is the object or tuple in a dataset.\n",
      " \n",
      " i\n",
      " \n",
      " i\n",
      " \n",
      " is an index of the class.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Under-the-hood, Naive Bayes involves a multiplication (once the probability is known) which makes the algorithm simplistic and fast.\n",
      " \n",
      " It can also be used to solve multi-class prediction problems.\n",
      " \n",
      " This classifier performs better than other models with less training data if the assumption of independence of features holds.\n",
      " \n",
      " Cons:\n",
      " \n",
      " It assumes that all the features are independent. This is actually a big con because features in reality are frequently not fully independent.\n",
      " \n",
      " Use case:\n",
      " \n",
      " When the assumption of independence holds between features, Naive Bayes classifier typically performs better than logistic regression and requires less training data.\n",
      " \n",
      " It performs well in case of categorical input variables compared to continuous/numerical variable(s).\n",
      " \n",
      " Regression Algorithms\n",
      " \n",
      " Linear Regression\n",
      " \n",
      " Linear regression analysis is a supervised machine learning algorithm that is used to predict the value of an output variable based on the value of an input variable.\n",
      " \n",
      " The output variable we’re looking to predict is called the dependent variable.\n",
      " \n",
      " The input variable we’re using to predict the output variable’s value is called the independent variable.\n",
      " \n",
      " Assumes a linear relationship between the output and input variable(s) and fits a linear equation on the data.\n",
      " \n",
      " The goal of Linear Regression is to predict output values for inputs that are not present in the data set, with the belief that those outputs would fall on the fitted line.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Performs very well for linearly separated data.\n",
      " \n",
      " Easy to implement and is interpretable.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Prone to noise and overfitting.\n",
      " \n",
      " Very sensitive to outliers.\n",
      " \n",
      " Use case:\n",
      " \n",
      " Linear regression is commonly used for predictive analysis and modeling.\n",
      " \n",
      " Classification and Regression Algorithms\n",
      " \n",
      " K-Nearest Neighbors\n",
      " \n",
      " Based on the age-old adage “birds of a feather flock together”.\n",
      " \n",
      " The\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " -nearest neighbors algorithm, also known as\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " -NN, is a non-parametric, supervised machine learning algorithm, which uses proximity to make classifications or predictions about the grouping of an individual data point.\n",
      " \n",
      " While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.\n",
      " \n",
      " The value of\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " is a hyperparameter which represents the number of neighbors you’d like the algorithm to refer as it generates its output.\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " -NN answers the question that given the current data, what are the\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " most similar data points to the query.\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " -NN calculates distance typically using either Euclidean or Manhattan distance:\n",
      " \n",
      " Euclidean distance:\n",
      " \n",
      " d\n",
      " \n",
      " (\n",
      " \n",
      " x\n",
      " \n",
      " ,\n",
      " \n",
      " y\n",
      " \n",
      " )\n",
      " \n",
      " =\n",
      " \n",
      " ∑\n",
      " \n",
      " i\n",
      " \n",
      " =\n",
      " \n",
      " 1\n",
      " \n",
      " n\n",
      " \n",
      " (\n",
      " \n",
      " y\n",
      " \n",
      " i\n",
      " \n",
      " −\n",
      " \n",
      " x\n",
      " \n",
      " i\n",
      " \n",
      " )\n",
      " \n",
      " 2\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " ‾\n",
      " \n",
      " \n",
      " \n",
      " ⎷\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " d\n",
      " \n",
      " (\n",
      " \n",
      " x\n",
      " \n",
      " ,\n",
      " \n",
      " y\n",
      " \n",
      " )\n",
      " \n",
      " =\n",
      " \n",
      " ∑\n",
      " \n",
      " i\n",
      " \n",
      " =\n",
      " \n",
      " 1\n",
      " \n",
      " n\n",
      " \n",
      " (\n",
      " \n",
      " y\n",
      " \n",
      " i\n",
      " \n",
      " −\n",
      " \n",
      " x\n",
      " \n",
      " i\n",
      " \n",
      " )\n",
      " \n",
      " 2\n",
      " \n",
      " Manhattan distance:\n",
      " \n",
      " d\n",
      " \n",
      " (\n",
      " \n",
      " x\n",
      " \n",
      " ,\n",
      " \n",
      " y\n",
      " \n",
      " )\n",
      " \n",
      " =\n",
      " \n",
      " ∑\n",
      " \n",
      " i\n",
      " \n",
      " =\n",
      " \n",
      " 1\n",
      " \n",
      " m\n",
      " \n",
      " ∣\n",
      " \n",
      " ∣\n",
      " \n",
      " x\n",
      " \n",
      " i\n",
      " \n",
      " −\n",
      " \n",
      " y\n",
      " \n",
      " i\n",
      " \n",
      " ∣\n",
      " \n",
      " ∣\n",
      " \n",
      " d\n",
      " \n",
      " (\n",
      " \n",
      " x\n",
      " \n",
      " ,\n",
      " \n",
      " y\n",
      " \n",
      " )\n",
      " \n",
      " =\n",
      " \n",
      " ∑\n",
      " \n",
      " i\n",
      " \n",
      " =\n",
      " \n",
      " 1\n",
      " \n",
      " m\n",
      " \n",
      " |\n",
      " \n",
      " x\n",
      " \n",
      " i\n",
      " \n",
      " −\n",
      " \n",
      " y\n",
      " \n",
      " i\n",
      " \n",
      " |\n",
      " \n",
      " This is the high-level view of how the algorithm works:\n",
      " \n",
      " For each example in the data:\n",
      " \n",
      " Calculate distance between query example and current example from the data.\n",
      " \n",
      " Add the distance and index to an ordered collection.\n",
      " \n",
      " Sort in ascending order by distance.\n",
      " \n",
      " Pick first\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " from sorted order.\n",
      " \n",
      " Get labels of selected\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " entries.\n",
      " \n",
      " If regression, return the mean of\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " labels.\n",
      " \n",
      " If classification, return the majority vote (some implementations incorporate weights for each vote) of the labels.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Easy to implement.\n",
      " \n",
      " Needs only a few hyperparameters which are:\n",
      " \n",
      " The value of\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " .\n",
      " \n",
      " Distance metric used.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Does not scale well as it takes too much memory and data storage compared with other classifiers.\n",
      " \n",
      " Prone to overfitting if the value of\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " is too low and will underfit if the value of\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " is too high.\n",
      " \n",
      " Use case:\n",
      " \n",
      " While\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " -NNs can be used for regression problems, they are typically used for classification.\n",
      " \n",
      " When labelled data is too expensive or impossible to obtain.\n",
      " \n",
      " When the dataset is relatively smaller and is noise free.\n",
      " \n",
      " Support Vector Machines\n",
      " \n",
      " The objective of Support Vector Machines (SVMs) is to find a hyperplane in an\n",
      " \n",
      " N\n",
      " \n",
      " N\n",
      " \n",
      " -dimensional space (where\n",
      " \n",
      " N\n",
      " \n",
      " N\n",
      " \n",
      " is the number of features) that distinctly classifies the data points.\n",
      " \n",
      " Note that a hyperplane is a decision boundary that helps classify the data points. If the number of input features is 2, hyperplane is just a line, if input features is 3, it becomes a 2D plane.\n",
      " \n",
      " The subset of training data points utilized in the decision function are called “support vectors”, hence the name Support Vector Machines.\n",
      " \n",
      " In the instance that the data is not linearly separable, we need to use the kernel trick (also called the polynomial trick). The SVM kernel is a function that takes low dimensional input space and transforms it into higher-dimensional space, i.e., it enables learning a non-linear decision boundary to separate datapoints. The gist of the kernel trick is that learning a linear model in the higher-dimensional space is equivalent to learning a non-linear model in the original lower-dimensional input space. More on this in the article on\n",
      " \n",
      " SVM Kernel/Polynomial Trick\n",
      " \n",
      " .\n",
      " \n",
      " The image below\n",
      " \n",
      " (source)\n",
      " \n",
      " displays the linear hyperplane separating the two classes such that the distance from the hyperplane to the nearest data point on each side is maximized. This hyperplane is known as the maximum-margin hyperplane/hard margin.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Effective in high dimensional spaces.\n",
      " \n",
      " Effective in cases where the number of dimensions is greater than the number of samples.\n",
      " \n",
      " Works well when there is a clear margin of separation between classes.\n",
      " \n",
      " Memory efficient as it uses a subset of training points in the decision function (“support vectors”).\n",
      " \n",
      " Versatile since different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Doesn’t perform well when we have large datasets because the required training time is higher.\n",
      " \n",
      " If the number of features is much greater than the number of samples, avoiding over-fitting in choosing kernel functions and regularization term is crucial.\n",
      " \n",
      " SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n",
      " \n",
      " Doesn’t perform very well when the dataset has more noise, i.e., when target classes are overlapping.\n",
      " \n",
      " Use case:\n",
      " \n",
      " While SVMs can be used for regression problems, they are typically used for classification (similar to\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " -NN) and outliers detection.\n",
      " \n",
      " Works great if the number of features is high and they occur in high dimensional spaces.\n",
      " \n",
      " The following figure\n",
      " \n",
      " (source)\n",
      " \n",
      " shows SVM flavors.\n",
      " \n",
      " Explain the Kernel Trick in SVM and Why We Use It and How to Choose What Kernel to Use?\n",
      " \n",
      " Kernels are used in SVM to map the original input data into a particular higher dimensional space where it will be easier to find patterns in the data and train the model with better performance.\n",
      " \n",
      " For e.g.: If we have binary class data which form a ring-like pattern (inner and outer rings representing two different class instances) when plotted in 2D space, a linear SVM kernel will not be able to differentiate the two classes well when compared to a RBF (radial basis function) kernel, mapping the data into a particular higher dimensional space where the two classes are clearly separable.\n",
      " \n",
      " Typically without the kernel trick, in order to calculate support vectors and support vector classifiers, we need first to transform data points one by one to the higher dimensional space, and do the calculations based on SVM equations in the higher dimensional space, then return the results. The ‘trick’ in the kernel trick is that we design the kernels based on some conditions as mathematical functions that are equivalent to a dot product in the higher dimensional space without even having to transform data points to the higher dimensional space. i.e we can calculate support vectors and support vector classifiers in the same space where the data is provided which saves a lot of time and calculations.\n",
      " \n",
      " Having domain knowledge can be very helpful in choosing the optimal kernel for your problem, however in the absence of such knowledge following this default rule can be helpful:\n",
      "For linear problems, we can try linear or logistic kernels and for nonlinear problems, we can use RBF or Gaussian kernels.\n",
      " \n",
      " Decision Trees\n",
      " \n",
      " A Decision Tree is a tree with a flowchart-like structure consisting of 3 elements as shown in the following image\n",
      " \n",
      " (source)\n",
      " \n",
      " :\n",
      " \n",
      " The internal node denotes a test on an attribute.\n",
      " \n",
      " Each branch represents an outcome of the test.\n",
      " \n",
      " Each leaf node (terminal node) holds a class label.\n",
      " \n",
      " Here is a figure (source)](https://python.plainenglish.io/decision-trees-easy-intuitive-way-with-python-23131eaad311) that illustrates an example decision tree with the thought process behind deciding to play tennis:\n",
      " \n",
      " The objective of a Decision Tree is to create a training model that can to predict the class of the target variable by learning simple decision rules inferred from prior data (training data).\n",
      " \n",
      " Pros:\n",
      " \n",
      " Interpretability is high due to the intuitive nature of a tree.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Decision trees are susceptible to overfitting (\n",
      " \n",
      " random forests\n",
      " \n",
      " are a great way to fix this issue).\n",
      " \n",
      " Small changes in data can lead to large structural changes on the tree.\n",
      " \n",
      " Use case:\n",
      " \n",
      " When you want to be able to lay out all the possible outcomes of a problem and work on challenging each option.\n",
      " \n",
      " Model Ensembles\n",
      " \n",
      " Bagging and Boosting\n",
      " \n",
      " Bagging and boosting are two popular ensemble learning techniques used in machine learning to improve the performance of predictive models by combining multiple weaker models. In other words, they combine multiple models to produce a more stable and accurate final model compared to a single classifier. They aim to decrease the bias and variance of a model (with the end goal of having low bias and modest variance to model the nuances of the training data, while not underfitting/overfitting it). While they have similar goals, they differ in their approach and how they create the ensemble.\n",
      " \n",
      " Ensemble learning is a powerful approach that combines multiple models to improve the predictive performance of machine learning algorithms. By leveraging the diversity of these models, ensemble learning helps mitigate the issues of bias, variance, and noise commonly encountered in individual models. It achieves this by training a set of classifiers or experts and allowing them to vote or contribute to the final prediction or classification.\n",
      " \n",
      " Bootstrapping\n",
      " \n",
      " Before diving into the specifics of bagging and boosting, let’s first understand bootstrapping. Bootstrapping is a sampling technique that involves creating subsets of observations from the original dataset with replacement.\n",
      " \n",
      " Each subset has the same size as the original dataset, and the random sampling allows us to better understand the bias and variance within the dataset. It helps estimate the mean and standard deviation by resampling from the dataset.\n",
      " \n",
      " Bagging\n",
      " \n",
      " Bagging, short for Bootstrap Aggregation, is a straightforward yet powerful ensemble method. It applies the bootstrap procedure to high-variance machine learning algorithms, typically decision trees. The idea behind bagging is to combine the results of multiple models, such as decision trees, to obtain a more generalized and robust prediction. It creates subsets (bags) from the original dataset using random sampling with replacement, and each subset is used to train a base model or weak model independently. These models run in parallel and are independent of each other.\n",
      " \n",
      " The final prediction is determined by combining the predictions from all the models, often through averaging or majority voting.\n",
      " \n",
      " Boosting\n",
      " \n",
      " Boosting is a sequential process where each subsequent model attempts to correct the errors made by the previous model. Unlike bagging, boosting involves training learners sequentially, with early learners fitting simple models to the data and subsequent learners analyzing the data for errors. The goal is to solve for net error from the prior model by adjusting the weights assigned to each data point. Boosting assigns higher weights to misclassified data points, so subsequent learners focus more on these difficult cases.\n",
      " \n",
      " Through this iterative process, boosting aims to convert a collection of weak learners into a stronger and more accurate model. The final model, often referred to as a strong learner, is a weighted combination of all the models.\n",
      " \n",
      " Bagging vs. Boosting\n",
      " \n",
      " Bagging and Boosting are both ensemble learning techniques used to improve the performance of machine learning models. However, they differ in their approach and objectives. Here are the key differences between Bagging and Boosting:\n",
      " \n",
      " Data Sampling:\n",
      " \n",
      " Bagging: In Bagging (short for Bootstrap Aggregating), multiple training datasets are created by randomly sampling from the original dataset with replacement. Each dataset is of the same size as the original dataset.\n",
      " \n",
      " Boosting: In Boosting, the training datasets are also created by random sampling with replacement. However, each new dataset gives more weight to the instances that were misclassified by previous models. This allows subsequent models to focus more on difficult cases.\n",
      " \n",
      " Model Independence:\n",
      " \n",
      " Bagging: In Bagging, each model is built independently of the others. They are trained on different subsets of the data and can be constructed in parallel.\n",
      " \n",
      " Boosting: In Boosting, models are built sequentially. Each new model is influenced by the performance of previously built models. Misclassified instances are given higher weights, and subsequent models try to correct those errors.\n",
      " \n",
      " Weighting of Models:\n",
      " \n",
      " Bagging: In Bagging, all models have equal weight when making predictions. The final prediction is often obtained by averaging the predictions of all models or using majority voting.\n",
      " \n",
      " Boosting: In Boosting, models are weighted based on their performance. Models with better classification results are given higher weights. The final prediction is obtained by combining the weighted predictions of all models.\n",
      " \n",
      " Objective:\n",
      " \n",
      " Bagging: Bagging aims to reduce the variance of a single model. It helps to improve stability and reduce overfitting by combining multiple models trained on different subsets of the data.\n",
      " \n",
      " Boosting: Boosting aims to reduce the bias of a single model. It focuses on difficult instances and tries to correct the model’s mistakes by giving more weight to misclassified instances. Boosting can improve the overall accuracy of the model but may be more prone to overfitting.\n",
      " \n",
      " Examples:\n",
      " \n",
      " Bagging: Random Forest is an extension of Bagging that uses decision trees as base models and combines their predictions to make final predictions.\n",
      " \n",
      " Boosting: Gradient Boosting is a popular Boosting algorithm that sequentially adds decision trees to the model, with each new tree correcting the mistakes of the previous ones.\n",
      " \n",
      " The image below\n",
      " \n",
      " (source)\n",
      " \n",
      " is an illustrated example of bagging and boosting.\n",
      " \n",
      " Random Forests\n",
      " \n",
      " A random forest is a robust ML algorithm that relies on having an ensemble of different models and making them vote for the prediction.\n",
      " \n",
      " An essential feature of random forest is to encourage diversity in the models; that way, we ensure the models have different predictions that will improve their model performance.\n",
      " \n",
      " Random forest encourages diversity by using random sampling with a replacement but also changing the root node, which is the first feature in which we split our data.\n",
      " \n",
      " The results are a set of decision trees with different root nodes taken from similar (not equal) datasets, and each has a vote in the prediction of the model.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Less prone to overfitting compared to\n",
      " \n",
      " decision trees\n",
      " \n",
      " .\n",
      " \n",
      " Cons:\n",
      " \n",
      " Interpretability is low compared to\n",
      " \n",
      " decision trees\n",
      " \n",
      " .\n",
      " \n",
      " Use case:\n",
      " \n",
      " When the model is overfitting and you want better generalization.\n",
      " \n",
      " Gradient Boosting\n",
      " \n",
      " Gradient Boosting is an ensemble machine learning algorithm that combines multiple weak models to create a strong model.\n",
      " \n",
      " It is an iterative process where each iteration, a new model is fit to the residual errors made by the previous model, with the goal of decreasing the overall prediction error.\n",
      " \n",
      " The algorithm works as follows:\n",
      " \n",
      " Initialize the model with a weak learner, typically a decision tree with a single split.\n",
      " \n",
      " Compute the negative gradient of the loss function with respect to the current prediction.\n",
      " \n",
      " Fit a new model to the negative gradient.\n",
      " \n",
      " Update the prediction by adding the prediction from the new model.\n",
      " \n",
      " Repeat steps 2-4 for a specified number of iterations, or until a stopping criterion is met.\n",
      " \n",
      " Combine the predictions from all models to get the final prediction.\n",
      " \n",
      " XGBoost\n",
      " \n",
      " XGBoost algorithm is a gradient boosting algorithm that is highly efficient and scalable.\n",
      " \n",
      " Here’s a high-level overview of the XGBoost algorithm:\n",
      " \n",
      " Initialize the model with a weak learner, usually a decision tree stump (a decision tree with a single split)\n",
      " \n",
      " Compute the negative gradient of the loss function with respect to the current prediction\n",
      " \n",
      " Fit a decision tree to the negative gradient to make a new prediction\n",
      " \n",
      " Add the prediction from this tree to the current prediction\n",
      " \n",
      " Repeat steps 2-4 for a specified number of trees, or until a stopping criterion is met\n",
      " \n",
      " Combine the predictions from all trees to get the final prediction\n",
      " \n",
      " The following figure summarizes parallelizing XGBoost\n",
      " \n",
      " (source)\n",
      " \n",
      " .\n",
      " \n",
      " The content below is taken from\n",
      " \n",
      " Damien Benveniste’s LinkedIn post linked here\n",
      " \n",
      " But why XGBoost became so popular? There are 2 aspects that made the success of XGBoost back in 2014.\n",
      " \n",
      " The first one is the regularized learning objective that allows for better pruning of the trees.\n",
      " \n",
      " The second one, is the ability to distribute the Gradient Boosting learning process across multiple threads or machines, allowing it to handle larger scales of data. Boosting algorithms have been known to perform very well on most large data sets, but the iterative process of boosting makes those painfully slow!\n",
      " \n",
      " How do you parallelize a boosting algorithm then? In the case of Random Forest, it is easy, you just distribute the data across threads, build independent trees there, and average the resulting tree predictions. In the case of an iterative process like boosting, you need to parallelize the tree building itself. It all comes down to how you find an optimal split in a tree: for each feature, sort the data and linearly scan the feature to find the best split. If you have N samples and M features, it is O(NM log(N)) time complexity at each node. In pseudo-code:\n",
      " \n",
      "best_split\n",
      " \n",
      "=\n",
      " \n",
      "None\n",
      "for\n",
      " \n",
      "feature\n",
      " \n",
      "in\n",
      " \n",
      "features\n",
      ":\n",
      " \n",
      "for\n",
      " \n",
      "sample\n",
      " \n",
      "in\n",
      " \n",
      "sorted\n",
      " \n",
      "samples\n",
      ":\n",
      " \n",
      "if\n",
      " \n",
      "split\n",
      " \n",
      "is\n",
      " \n",
      "better\n",
      " \n",
      "than\n",
      " \n",
      "best_split\n",
      ":\n",
      " \n",
      "best_split\n",
      " \n",
      "=\n",
      " \n",
      "f\n",
      "(\n",
      "feature\n",
      ",\n",
      " \n",
      "sample\n",
      ")\n",
      " So you can parallelize split search by scanning each feature independently and reduce the resulting splits to the optimal one.\n",
      " \n",
      " XGBoost is not the first attempt to parallelize GBM, but they used a series of tricks that made it very efficient:\n",
      " \n",
      " First, all the columns are pre-sorted while keeping a pointer to the original index of the entry. This removes the need to sort the feature at every search.\n",
      " \n",
      " They used a Compressed Column Format for a more efficient distribution of the data.\n",
      " \n",
      " They used a cache-aware prefetching algorithm to minimize the non-contiguous memory \n",
      "access that results from the pre-sorting step.\n",
      " \n",
      " Not directly about parallelization, but they came out with an approximated split search algorithm that speeds the tree building further.\n",
      " \n",
      " As of today, you can train XGBoost across cores on the same machine, but also on AWS YARN, Kubernetes, Spark, and GPU and you can use Dask or Ray to do it (https://lnkd.in/dtQTfu62).\n",
      " \n",
      " One thing to look out for is that there is a limit to how much XGBoost can be parallelized. With too many threads, the data communication between threads becomes a bottleneck and the training speed plateaus. Here is a example explaining that effect: https://lnkd.in/d9SEcQuV.\n",
      " \n",
      " Clustering in Machine Learning: Latent Dirichlet Allocation and K-Means\n",
      " \n",
      " Clustering is a type of unsupervised learning method in machine learning, as it involves grouping unlabeled data based on their underlying patterns. This article will delve into two popular clustering algorithms: Latent Dirichlet Allocation (LDA) and K-Means.\n",
      " \n",
      " What is K-Means Clustering?\n",
      " \n",
      " K-Means is a centroid-based clustering method, meaning it clusters the data into k different groups by trying to minimize the distance between data points in the same group, often measured by Euclidean distance. The center of each group (the centroid) is calculated as the mean of all the data points in the cluster.\n",
      " \n",
      " Here’s how the algorithm works:\n",
      " \n",
      " Select k initial centroids, where k is a user-defined number of clusters.\n",
      " \n",
      " Assign each data point to the nearest centroid. These clusters will form the initial clusters.\n",
      " \n",
      " Recalculate the centroid (mean) of each cluster.\n",
      " \n",
      " Repeat steps 2 and 3 until the centroids don’t change significantly, or a maximum number of iterations is reached.\n",
      " \n",
      " K-Means is a simple yet powerful algorithm, but it has its drawbacks. The algorithm’s performance can be greatly affected by the choice of initial centroids and the value of k. Additionally, it works best on datasets where clusters are spherical and roughly the same size.\n",
      " \n",
      " What is Latent Dirichlet Allocation (LDA)?\n",
      " \n",
      " Latent Dirichlet Allocation (LDA) is a generative statistical model widely used for topic modeling in natural language processing. Rather than clustering based on distances, LDA assumes that each document in a corpus is a mixture of a certain number of topics, and each word in the document is attributable to one of the document’s topics.\n",
      " \n",
      " In LDA:\n",
      " \n",
      " You specify the number of topics (k) you believe exist in your corpus.\n",
      " \n",
      " The algorithm assigns every word in every document to a temporary topic (initially, this assignment is random).\n",
      " \n",
      " For each document, the algorithm goes through each word, and for each topic, calculates:\n",
      " \n",
      " How often the topic occurs in the document, and\n",
      " \n",
      " How often the word occurs with the topic throughout the corpus.\n",
      " \n",
      " Based on these calculations, the algorithm reassigns the word to a new topic.\n",
      " \n",
      " Steps 3 and 4 are repeated a large number of times, and the algorithm ultimately provides a steady state where the document topic and word topic assignments are considered optimal.\n",
      " \n",
      " LDA’s major advantage is its ability to uncover hidden thematic structure in the corpus. However, its interpretability relies heavily on the quality of the text preprocessing and the choice of the number of topics, which often requires domain knowledge.\n",
      " \n",
      " Machine Learning or Deep Learning?\n",
      " \n",
      " Both K-means and LDA are traditional machine learning algorithms. They rely on explicit programming and feature engineering, whereas deep learning algorithms automatically discover the features to be used for learning through the learning process by building high-level features from data.\n",
      " \n",
      " Nevertheless, they remain essential tools in the data scientist’s arsenal. The choice between machine learning or deep learning methods will depend on the problem at hand, the nature of the data available, and the computational resources at your disposal.\n",
      " \n",
      " Hidden Markov Model (HMM) in Machine Learning\n",
      " \n",
      " Hidden Markov Models (HMMs) are statistical models that have been widely applied in various fields of study, including finance, genomics, and most notably in natural language processing and speech recognition systems.\n",
      " \n",
      " What is a Hidden Markov Model?\n",
      " \n",
      " A Hidden Markov Model is a statistical model where the system being modeled is assumed to be a Markov process — i.e., a random process where the future states depend only on the current state and not on the sequence of events that preceded it — with hidden states.\n",
      " \n",
      " In an HMM, we deal with two types of sequences:\n",
      " \n",
      " An observable sequence (also known as emission sequence)\n",
      " \n",
      " A hidden sequence, which corresponds to the hidden states that generate the observable sequence\n",
      " \n",
      " The term “hidden” in HMM refers to the fact that while the output states (observable sequence) are visible to an observer, the sequence of states that led to those outputs (hidden sequence) is unknown or hidden.\n",
      " \n",
      " Key Components of an HMM\n",
      " \n",
      " A Hidden Markov Model is characterized by the following components:\n",
      " \n",
      " States:\n",
      " \n",
      " These are the hidden states in the model. They cannot be directly observed, but they can be inferred from the observable states.\n",
      " \n",
      " Observations:\n",
      " \n",
      " These are the output states which are directly visible.\n",
      " \n",
      " Transition Probabilities:\n",
      " \n",
      " These are the probabilities of transitioning from one hidden state to another.\n",
      " \n",
      " Emission Probabilities:\n",
      " \n",
      " These are the probabilities of an observable state being generated from a hidden state.\n",
      " \n",
      " Initial State Probabilities:\n",
      " \n",
      " These are the probabilities of starting in each hidden state.\n",
      " \n",
      " Applications of HMMs in NLP\n",
      " \n",
      " In the field of Natural Language Processing, HMMs have been extensively used, especially for tasks like Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and even in speech recognition and machine translation systems. For instance, in POS tagging, the hidden states could represent the parts of speech, while the visible states could represent the words in sentences.\n",
      " \n",
      " Pros and Cons of HMMs\n",
      " \n",
      " HMMs are beneficial due to their simplicity, interpretability, and their effectiveness in dealing with temporal data. However, they make some strong assumptions, like the Markov assumption and the assumption of independence among observable states given the hidden states, which may not hold true in all scenarios. Additionally, HMMs may suffer from issues with scalability and can struggle to model complex, long-distance dependencies in the data.\n",
      " \n",
      " Despite these limitations, HMMs remain a valuable tool in the machine learning toolbox and serve as a foundation for more complex models in sequence prediction tasks.\n",
      " \n",
      " Summary\n",
      " \n",
      " Linear Regression\n",
      " \n",
      " Definition:\n",
      " \n",
      " Linear regression is a supervised learning algorithm used for predicting a continuous target variable based on one or more input features.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Simple and interpretable, fast training and prediction, works well with linear relationships.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Assumes linear relationships, sensitive to outliers, may not capture complex patterns.\n",
      " \n",
      " Decision Trees\n",
      " \n",
      " Definition:\n",
      " \n",
      " Decision trees are hierarchical structures that make decisions based on a sequence of rules and conditions, leading to a final prediction.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Easy to understand and interpret, handles both numerical and categorical data, can capture non-linear relationships.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Prone to overfitting, can be unstable and sensitive to small changes in data.\n",
      " \n",
      " Random Forest\n",
      " \n",
      " Definition:\n",
      " \n",
      " Random forest is an ensemble learning method that combines multiple decision trees to make predictions.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Robust and handles high-dimensional data, reduces overfitting through ensemble learning, provides feature importance.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Requires more computational resources, lack of interpretability for individual trees.\n",
      " \n",
      " Support Vector Machines (SVM)\n",
      " \n",
      " Definition:\n",
      " \n",
      " Support Vector Machines is a binary classification algorithm that finds an optimal hyperplane to separate data into different classes.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Effective in high-dimensional spaces, works well with both linear and non-linear data, handles outliers well.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Can be sensitive to the choice of kernel function and parameters, computationally expensive for large datasets.\n",
      " \n",
      " Naive Bayes\n",
      " \n",
      " Definition:\n",
      " \n",
      " Naive Bayes is a probabilistic classifier that applies Bayes’ theorem with the assumption of independence between features.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Simple and fast, works well with high-dimensional data, performs well with categorical features.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Assumes independence between features, may not capture complex relationships.\n",
      " \n",
      " Neural Networks\n",
      " \n",
      " Definition:\n",
      " \n",
      " Neural networks are a set of interconnected nodes or “neurons” organized in layers, capable of learning complex patterns from data.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Powerful and flexible, can learn complex patterns, works well with large datasets, can handle various data types.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Requires large amounts of data for training, computationally intensive, prone to overfitting if not properly regularized.\n",
      " \n",
      " K-Nearest Neighbors (k-NN)\n",
      " \n",
      " Definition:\n",
      " \n",
      " k-Nearest Neighbors is a non-parametric algorithm that makes predictions based on the majority vote of its k nearest neighbors.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Simple and easy to understand, no training phase, works well with small datasets and non-linear data.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Computationally expensive during prediction, sensitive to irrelevant features, doesn’t provide explicit model representation.\n",
      " \n",
      " Gradient Boosting\n",
      " \n",
      " Definition:\n",
      " \n",
      " Gradient Boosting is an ensemble method that combines weak learners (typically decision trees) to create a strong predictive model.\n",
      " \n",
      " Pros:\n",
      " \n",
      " Produces highly accurate models, handles different types of data, handles missing values well, provides feature importance.\n",
      " \n",
      " Cons:\n",
      " \n",
      " Can be prone to overfitting if not properly tuned, computationally expensive.\n",
      " \n",
      " References\n",
      " \n",
      " IBM tech blog: What is logistic regression?\n",
      " \n",
      " Google Crash Course\n",
      " \n",
      " Towards Data Science\n",
      " \n",
      " Analytics Vidhya\n",
      " \n",
      " Science Direct\n",
      " \n",
      " SciKit Learn\n",
      " \n",
      " KD Nuggets\n",
      " \n",
      " Geeks for Geeks\n",
      " \n",
      " KD Nuggets Trees\n",
      " \n",
      " Edureka\n",
      " \n",
      " Citation\n",
      " \n",
      " If you found our work useful, please cite it as:\n",
      " \n",
      "@article{Chadha2020DistilledMLAlgorithmsCompAnalysis,\n",
      " title = {ML Algorithms Comparative Analysis},\n",
      " author = {Chadha, Aman},\n",
      " journal = {Distilled AI},\n",
      " year = {2020},\n",
      " note = {\\url{https://aman.ai}}\n",
      "}\n",
      " |\n",
      " \n",
      " |\n",
      " \n",
      " |\n",
      " \n",
      " |\n",
      " \n",
      " www.amanchadha.com \n",
      "\n",
      " Deshraj Yadav\n",
      " \n",
      " Toggle navigation\n",
      " \n",
      " Deshraj Yadav\n",
      " \n",
      " Achievements\n",
      " \n",
      " Experience\n",
      " \n",
      " Education\n",
      " \n",
      " Publications\n",
      " \n",
      " Projects\n",
      " \n",
      " CV\n",
      " \n",
      " About me\n",
      " \n",
      " 👋 I'm Deshraj Yadav, Co-founder and CTO at\n",
      " \n",
      " Mem0\n",
      " \n",
      " (f.k.a Embedchain). I am broadly interested in the field of Artificial Intelligence and Machine Learning Infrastructure.\n",
      " \n",
      " 👷‍♂️ Previously, I was Senior Autopilot Engineer at Tesla Autopilot where I led the Autopilot's AI Platform which helped the Tesla Autopilot team to track large scale training and model evaluation experiments, provide monitoring and observability into jobs and training cluster issues.\n",
      " \n",
      " 🚀 I had built\n",
      " \n",
      " EvalAI\n",
      " \n",
      " as my\n",
      " \n",
      " masters thesis\n",
      " \n",
      " at Georgia Tech (advised by\n",
      " \n",
      " Dhruv Batra\n",
      " \n",
      " and\n",
      " \n",
      " Devi Parikh\n",
      " \n",
      " ), which is\n",
      " an open-source platform for evaluating and comparing machine learning and artificial intelligence algorithms 🤖 at\n",
      " scale.\n",
      " \n",
      " 🏏 Outside of work, I am very much into cricket and play in two leagues (\n",
      " \n",
      " Cricbay\n",
      " \n",
      " and\n",
      " \n",
      " NACL\n",
      " \n",
      " ) in San Francisco Bay Area.\n",
      " \n",
      " 📬 I’m best reached via email. deshraj at mem0 dot ai. Social media links:\n",
      " \n",
      " News\n",
      " \n",
      " September 2023: Started working as a Co-founder and CTO at\n",
      " \n",
      " Embedchain\n",
      " \n",
      " . Embedchain is an Open Source RAG Framework that makes it easy to create and deploy AI apps.\n",
      " \n",
      " July 2019: Started working as a Machine Learning\n",
      " Engineer at\n",
      " \n",
      " Tesla Autopilot\n",
      " \n",
      " March 2019: Co-founded\n",
      " \n",
      " Caliper\n",
      " \n",
      " . Caliper is the only\n",
      " platform that enables recruiters to evaluate\n",
      " practical AI skills\n",
      " \n",
      " December 2018: I successfully defended my\n",
      " Masters thesis 'EvalAI - Evaluating AI Systems at Scale'.\n",
      " \n",
      " Thesis\n",
      " \n",
      " ,\n",
      " \n",
      " Slides\n",
      " \n",
      " ,\n",
      " \n",
      " Photos\n",
      " \n",
      " August 2018: Our paper 'Do explanations make VQA models more predictable to a human?'\n",
      " is accepted in\n",
      " \n",
      " EMNLP, 2018\n",
      " \n",
      " June 2018: Organizing the 1st\n",
      " \n",
      " Visual\n",
      " Dialog Challenge\n",
      " \n",
      " !\n",
      " \n",
      " June 2018: Presenting Human-in-the-loop evaluation for Computer Vision Challenges Demo in\n",
      " \n",
      " CVPR 2018\n",
      " \n",
      " May 2018: Starting as Research Intern in\n",
      " \n",
      " Snap Inc.,\n",
      " Los Angeles\n",
      " \n",
      " April 2018: Serving as reviewer for\n",
      " \n",
      " ECCV 2018\n",
      " \n",
      " February 2018: Serving as Organization Administrator for CloudCV in\n",
      " \n",
      " Google Summer of Code 2018\n",
      " \n",
      " December 2017: Won\n",
      " \n",
      " Snap Inc. Research\n",
      " Scholarship\n",
      " \n",
      " , 2017 (\n",
      " \n",
      " Georgia Tech's\n",
      " published story\n",
      " \n",
      " )\n",
      " \n",
      " October 2017: Serving as Organization Administrator for CloudCV in\n",
      " \n",
      " Google Code-In 2017\n",
      " \n",
      " October 2017: Representing\n",
      " \n",
      " CloudCV\n",
      " \n",
      " at Google Summer of\n",
      " Code Mentor Summit 2017, Google Sunnyvale\n",
      " \n",
      " June 2017: Our paper\n",
      " \n",
      " 'Evaluating Visual\n",
      " Conversational Agents via Cooperative Human-AI Games'\n",
      " \n",
      " is accepted in\n",
      " \n",
      " HCOMP 2017\n",
      " \n",
      " March 2017: Joining\n",
      " \n",
      " Georgia Tech\n",
      " \n",
      " as a Masters of\n",
      " Computer Science Student starting Fall 2017\n",
      " \n",
      " Experience\n",
      " \n",
      " Co-founder and CTO,\n",
      " \n",
      " Mem0 (f.k.a Embedchain)\n",
      " \n",
      " (September 2023 - Present)\n",
      " \n",
      " Machine Learning Engineer,\n",
      " \n",
      " Tesla\n",
      " Autopilot\n",
      " \n",
      " (Jul 2019 - September 2023)\n",
      " \n",
      " Lead Autopilot's AI Platform that enables the team to:\n",
      " \n",
      " Track large scale training and model evaluation experiments\n",
      " \n",
      " Provide monitoring and observability into jobs and training cluster issues\n",
      " \n",
      " Automate tasks in the machine learning workflows so the team can focus on model training\n",
      " \n",
      " Make it convenient for engineers and researchers on the team to look at the metrics, logs, and data usage all in one place rather than looking at different dashboards\n",
      " \n",
      " Previously, I worked in AI-Tooling team where I was responsible for designing and implementing a diverse set of tools like 3D-labelling platform, model versioning tool, and active learning based label boosting software.\n",
      " \n",
      " Co-founder,\n",
      " \n",
      " Caliper\n",
      " \n",
      " (Mar 2019 - Mar 2020)\n",
      " \n",
      " Caliper is the only platform that enables recruiters to evaluate practical AI skills. At scale.\n",
      " Through automated challenges on real-world problems.\n",
      " \n",
      " Research Intern,\n",
      " \n",
      " Snap Inc.\n",
      " \n",
      " (May 2018 - Aug 2018)\n",
      " \n",
      " Building scalable end-to-end platform to train, test, and visualize machine learning algorithms\n",
      " on massive amounts of visual data.\n",
      " \n",
      " Research Assistant,\n",
      " \n",
      " Machine Learning\n",
      " and Perception Lab, Virginia Tech\n",
      " \n",
      " (Jun 2016 - May 2017)\n",
      " \n",
      " Worked on the problem of\n",
      " \n",
      " Visual Dialog\n",
      " \n",
      " which later got\n",
      " accepted in CVPR 2017 as a Spotlight Paper. During my visit, I also worked as Teaching Assistant for\n",
      " \n",
      " Intro. to Machine Learning Course\n",
      " \n",
      " for\n",
      " Fall-2016 instructed by\n",
      " \n",
      " Dr. Stefan Lee\n",
      " \n",
      " .\n",
      " \n",
      " Organization Administrator,\n",
      " \n",
      " CloudCV\n",
      " \n",
      " ,\n",
      " \n",
      " Google Summer of Code\n",
      " \n",
      " (Mar 2016 - Present)\n",
      " \n",
      " Served as Organization Mentor for Google Summer of Code (GSoC) 2016. Later, I have been serving\n",
      " as an Organization Administrator in GSoC 2017, 2018, 2019, and 2020.\n",
      " \n",
      " Student Developer, CloudCV,\n",
      " \n",
      " Google Summer of Code 2015\n",
      " \n",
      " (Apr 2015 - Aug 2015)\n",
      " \n",
      " Selected for GSOC 2015 under CloudCV Organization. My responsibilities included integrating\n",
      " NVIDIA's Deep Learning Framework DIGITS to provide workspaces for the researchers and other computer vision\n",
      " developers.\n",
      " \n",
      " Software Development Internship,\n",
      " \n",
      " Cityflo\n",
      " \n",
      " (Dec 2015 - Feb 2016)\n",
      " \n",
      " Responsible for setting an automated test suite for Continuous Integration in the backend\n",
      " codebase which led to a very lean software engineering team as the team was able to spend more time on building\n",
      " new features rather than maintaining and fixing bugs and issues.\n",
      " \n",
      " Software Development Internship,\n",
      " \n",
      " Siftr Labs\n",
      " \n",
      " (Sep 2015 - Dec 2015)\n",
      " \n",
      " Worked on building an IoT based product for indoor navigation in big offices and malls using\n",
      " Beacons (Low Energy Bluetooth devices). I was responsible for building REST APIs and creating the Android\n",
      " Application from scratch which communicates with the beacons to get real time position of a person using beacons\n",
      " signal strength.\n",
      " \n",
      " Backend Developer,\n",
      " \n",
      " Fratmart\n",
      " \n",
      " (Aug 2014 - Dec 2014)\n",
      " \n",
      " Responsible for the development of the whole product from scratch, managing the server,\n",
      " database management and other backend scripting.\n",
      " \n",
      " Backend Developer Internship,\n",
      " \n",
      " TakeZero\n",
      " \n",
      " (May 2014 - Jul 2014)\n",
      " \n",
      " Responsible for development of\n",
      " \n",
      " company's web application\n",
      " \n",
      " ,\n",
      " Facebook App development and setting up NoSQL database. Implemented scripts to do image enhancement, and\n",
      " cropping borders etc.\n",
      " \n",
      " Educational Qualifications\n",
      " \n",
      " Masters in Computer Science (Aug 2017 - Dec 2018)\n",
      " \n",
      " Georgia Institute of Technology, Atlanta\n",
      " \n",
      " Specialization in Machine Learning\n",
      " \n",
      " Expected Graduation: December 2018\n",
      " \n",
      " GPA: 3.7/4.0\n",
      " \n",
      " B.Tech in Computer Science and Engineering (Aug 2012 - Jun 2016)\n",
      " \n",
      " JSS Academy of Technical Education, Noida\n",
      " \n",
      " Passed with an aggregate of 78% (with Hons.)\n",
      " \n",
      " Class Rank: 9th rank (or top-6%)\n",
      " \n",
      " Projects\n",
      " \n",
      " EvalAI\n",
      " \n",
      " : Evaluating state of\n",
      " the art in AI\n",
      " \n",
      " Open source platform to create, collaborate and participate in the AI Challenges organized around the globe.\n",
      " \n",
      " Code\n",
      " \n",
      " Star\n",
      " \n",
      " Fork\n",
      " \n",
      " Invited talk on EvalAI in VQA 2017 Workshop\n",
      " \n",
      " HIEME: Human-Interactive Evaluation Made Easy\n",
      " \n",
      " Proof of concept of a central, standardized open source platform for human-in-the-loop evaluation of AI\n",
      " agents.\n",
      " Developed the infrastructure to pair Amazon Mechanical Turk (AMT) users in real-time with artificial visual\n",
      " dialog agents.\n",
      " \n",
      " HIEME Architecture\n",
      " \n",
      " Short Video on evaluating Visual Dialog Agents using HIEME\n",
      " \n",
      " Fabrik\n",
      " \n",
      " : Collaboratively\n",
      " build, visualize, and design neural nets in browser\n",
      " \n",
      " Fabrik is an online collaborative platform to build, visualize and train deep learning models via a simple\n",
      " drag-and-drop interface.\n",
      " \n",
      " Code\n",
      " \n",
      " Star\n",
      " \n",
      " Fork\n",
      " \n",
      " Visual Chatbot\n",
      " \n",
      " : A\n",
      " chatbot that can see!\n",
      " \n",
      " Built a visual chatbot which can hold a meaningful dialog with humans natural, conversational language about\n",
      " visual content. Specifically, given an image, a dialog history, and a question about the image, the chatbot\n",
      " has to ground the question in image, infer context from history, and answer the question accurately.\n",
      " \n",
      " Code\n",
      " \n",
      " Star\n",
      " \n",
      " Fork\n",
      " \n",
      " Demo Video\n",
      " \n",
      " Visual Question Answering (VQA)\n",
      " \n",
      " Demo\n",
      " \n",
      " In Visual Question Answering, given an image and a free-form natural language question about the image (e.g.,\n",
      " \"What kindof store is this?\", \"How many people are waiting in the queue?\", \"Is it safe to cross the street?\")\n",
      " the model's task is to automatically produce a concise, accurate, free-form, natural language answer\n",
      " (\"bakery\", \"5\", \"Yes\"). This demo is implemented using\n",
      " \n",
      " Hierarchical Question-Image Co-Attention\n",
      " \n",
      " model.\n",
      " \n",
      " Code\n",
      " \n",
      " Star\n",
      " \n",
      " Fork\n",
      " \n",
      " Live Demo\n",
      " \n",
      " Gradient-weighted Class\n",
      " Activation Mapping (Grad-CAM)\n",
      " \n",
      " Demo\n",
      " \n",
      " Gradient-weighted Class Activation Mapping (Grad-CAM) is a novel class-discriminative localization technique,\n",
      " that can be used to make CNN based models interpretable. Grad-CAM highlights regions of the image the VQA\n",
      " model looks at while making predictions. Given an image and a caption or question about that image, the model\n",
      " shows where it looked while doing prediction.\n",
      " \n",
      " Code\n",
      " \n",
      " Star\n",
      " \n",
      " Fork\n",
      " \n",
      " Grad-CAM VQA Demo\n",
      " \n",
      " Grad-CAM Classification Demo\n",
      " \n",
      " Grad-CAM Captioning Demo\n",
      " \n",
      " Grad-CAM Demo Video\n",
      " \n",
      " Origami\n",
      " \n",
      " : Artificial\n",
      " Intelligence as a Service\n",
      " \n",
      " Origami is an AI-as-a-service that allows researchers to easily convert their deep learning models into an\n",
      " online service that is widely accessible to everyone without the need to setup the infrastructure, resolve the\n",
      " dependencies, and build a web service around the deep learning model.\n",
      " \n",
      " Code\n",
      " \n",
      " Star\n",
      " \n",
      " Fork\n",
      " \n",
      " Diverse Beam Search\n",
      " \n",
      " Demo\n",
      " \n",
      " Beam search, the standard work-horse for decoding outputs from neural sequence models like RNNs produces\n",
      " generic and uninteresting sequences. This is inadequate for AI tasks with inherent ambiguity — for example,\n",
      " there can be multiple correct ways of describing the contents of an image. The demo overcomes this by\n",
      " proposing a diversity-promoting replacement,\n",
      " \n",
      " Diverse Beam Search\n",
      " \n",
      " that produces sequences that are\n",
      " significantly different — with runtime and memory requirements comparable to beam search.\n",
      " \n",
      " Code\n",
      " \n",
      " Star\n",
      " \n",
      " Fork\n",
      " \n",
      " Live Demo\n",
      " \n",
      " JSS InfoConnect\n",
      " \n",
      " Web based application that acts as a medium for interaction between all students, faculties and management of\n",
      " JSSATE Noida college. Officially recognized by college. Serves 10,000+ requests per day, 20,000+ users,\n",
      " 20,000+ notices uploaded.\n",
      " \n",
      " Code\n",
      " \n",
      " Star\n",
      " \n",
      " Fork\n",
      " \n",
      " Publications\n",
      " \n",
      " EvalAI: Towards Better Evaluation Systems for AI Agents\n",
      " \n",
      " Deshraj Yadav\n",
      " \n",
      " , Rishabh Jain, Harsh Agrawal, Prithvijit Chattopadhyay, Taranjeet\n",
      " Singh, Akash Jain, Shiv Baran Singh, Stefan Lee, Dhruv Batra\n",
      " \n",
      " arXiv preprint\n",
      " \n",
      " Project Webpage\n",
      " \n",
      " Github repository\n",
      " \n",
      " Fabrik: An Online Collaborative Neural Network Editor\n",
      " \n",
      " Utsav Garg, Viraj Prabhu,\n",
      " \n",
      " Deshraj Yadav\n",
      " \n",
      " , Ram Ramrakhya, Harsh Agrawal, Dhruv\n",
      " Batra\n",
      " \n",
      " arXiv preprint\n",
      " \n",
      " Github repository\n",
      " \n",
      " Do explanation modalities make VQA models more predictable to a human?\n",
      " \n",
      " Arjun Chandrasekaran\n",
      " \n",
      " *\n",
      " \n",
      " , Viraj Prabhu\n",
      " \n",
      " *\n",
      " \n",
      " ,\n",
      " \n",
      " Deshraj\n",
      " Yadav\n",
      " \n",
      " *\n",
      " \n",
      " , Prithvijit Chattopadhyay\n",
      " \n",
      " *\n",
      " \n",
      " , Devi Parikh\n",
      " \n",
      " EMNLP 2018\n",
      " \n",
      " Evaluating Visual Conversational Agents via Cooperative Human-AI Games\n",
      " \n",
      " Prithvijit Chattopadhyay\n",
      " \n",
      " *\n",
      " \n",
      " ,\n",
      " \n",
      " Deshraj Yadav\n",
      " \n",
      " *\n",
      " \n",
      " , Viraj Prabhu,\n",
      " Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh\n",
      " \n",
      " HCOMP 2017\n",
      " \n",
      " Code\n",
      " \n",
      " It Takes Two to Tango: Towards Theory of AI's Mind\n",
      " \n",
      " Arjun Chandrasekaran\n",
      " \n",
      " *\n",
      " \n",
      " ,\n",
      " \n",
      " Deshraj Yadav\n",
      " \n",
      " *\n",
      " \n",
      " , Prithvijit\n",
      " Chattopadhyay\n",
      " \n",
      " *\n",
      " \n",
      " , Viraj Prabhu\n",
      " \n",
      " *\n",
      " \n",
      " , Devi Parikh\n",
      " \n",
      " Chalearn Looking at People Workshop, CVPR 2017\n",
      " \n",
      " Demo\n",
      " \n",
      " Code\n",
      " \n",
      " Visual Dialog\n",
      " \n",
      " Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\n",
      " \n",
      " Deshraj Yadav\n",
      " \n",
      " , José M.F.\n",
      " Moura, Devi Parikh, Dhruv Batra\n",
      " \n",
      " CVPR 2017 - Spotlight\n",
      " \n",
      " Demo\n",
      " \n",
      " Project Webpage\n",
      " \n",
      " Visual Chatbot Code\n",
      " \n",
      " Data Collection Code\n",
      " \n",
      " Honors\n",
      " \n",
      " Snap Research Scholarship\n",
      " \n",
      " recipient, 2017: One of 8 awardees, selected from a pool of 60+ applicants from across the world\n",
      " \n",
      " Travel Scholarship recipient for Google Summer of Code Mentor Summit, 2016 and 2017\n",
      " \n",
      " National Means Cum-Merit Scholarship\n",
      " \n",
      " recipient with\n",
      " State Rank 13 (offered by Ministry of Human Resource and Development, Government of India)\n",
      " \n",
      " Prime Minister Scholarship Scheme\n",
      " \n",
      " recipient\n",
      " offered by Ministry of Defence, India\n",
      " \n",
      " Head Organizer of\n",
      " \n",
      " Hackathon\n",
      " 2015\n",
      " \n",
      " : More than 350 students participated in this hackathon\n",
      " \n",
      " Winner of Cassiopeian Wars 2014 organized by Microsoft Mobile Innovation Labs\n",
      " \n",
      " First Runner Up at HACK-A-THON SUMMER 2014 organized by Google Developers Group Lingaya's\n",
      " University. Won prize of worth Rs. 25,000\n",
      " \n",
      " Second Runner Up in Linux Challenge organized at Netaji Subhash Institute Of Technology\n",
      " (NSIT Delhi)\n",
      " \n",
      " First Runner Up at Junkyard Wars organized at Deen Dayal Upadhyay College, Delhi\n",
      " University\n",
      " \n",
      " Ⓒ Deshraj Yadav, 2022 \n",
      "\n",
      " 1.\n",
      "First step is to set up the assistant which is up and running & then needs to be\n",
      "personalized with the next content that is added.\n",
      "2.\n",
      "User details and preferences are open ended ? or do we get proper inputs from the user\n",
      "on what he likes and what he doesn’t? Or do we have to extract user profile from the\n",
      "unstructured input that that they share and generate a user profile based on the inputs\n",
      "Ans: As I understand this needs to be done by AI when we pass the unstructured data to\n",
      "the user where it can parse the info and create a personalized user profile\n",
      "3.\n",
      "Then this profile which is stored should be used for custom response generation. -> This\n",
      "profile should be saved in memory and should be constantly updated.\n",
      "https://github.com/Shubhamsaboo/awesome-llm-apps/blob/main/local_chatgpt_with_memory/lo\n",
      "cal_chatgpt_memory.py\n",
      "What does it mean by custom retrieval logic? Contextual information here means whatever is\n",
      "the memory of LLM from that fetch the user preferences for the query asked and then generate\n",
      "the response to user queries.\n",
      "Deliverables:\n",
      "Once a new model is launched you should be able to upload unstructured data -> create custom\n",
      "user profile -> For every query retrieve most relevant preferences based on the user profile and\n",
      "prepare the response? The retrieval here is from memory right??\n",
      "What do you mean by any open source library? Likes of embed chain as well where building a\n",
      "RAG system is one highly effective.\n",
      "Streamlit/Shiny for UI Design.\n",
      "API functionality, Load, data types for unstructured data - unittest\n",
      "Vector DB, FAISS - can store.\n",
      "Memory Store - MongoDB\n",
      "For pdf unstructured library\n",
      "Questions and Answers\n",
      "Question: Can I use the likes of OpenAI Models or something local?\n",
      "Answer: Yes, you are free to use any model.\n",
      "Question: Currently, can we restrict the scope to PDFs and blogs? If not, what are the\n",
      "different unstructured data sources the Assistant should handle?\n",
      "Answer: Sure, restricting to PDFs and blogs is fine for the task.\n",
      "Question: What specific user details and preferences should the AI Assistant extract and\n",
      "store from the unstructured data?\n",
      "Answer: User details and preferences are kind of open-ended, and this is where you can get a\n",
      "lot creative.\n",
      "Question: Are there any restrictions or guidelines on which open-source libraries can be\n",
      "used for this project?\n",
      "Answer: There is no restriction on using open-source libraries. Feel free to use any library of\n",
      "your choice.\n",
      "Bonus:\n",
      "Question: Do you have any specific requirements or expectations for the custom retrieval\n",
      "logic?\n",
      "Answer: No specific requirements. Again, we want you to be smart and creative about how to do\n",
      "the retrieval of memories.\n",
      "Question: Should the retrieval logic prioritize recent interactions (chat history based) or a\n",
      "specific type of user information (user information retrieval from data uploaded)? Or\n",
      "should it focus on profile-relevant information extraction?\n",
      "Answer: Great question. The candidate is expected to take a call on this based on their\n",
      "understanding of how humans think about memory.\n",
      "Mem0 Interview Project\n",
      "Goal\n",
      "Develop a personalized AI Assistant that can understand and remember user details and\n",
      "preferences from unstructured data sources, providing a tailored conversational experience.\n",
      "Requirements\n",
      "●\n",
      "Data Ingestion: The AI Assistant should allow users to input various unstructured data\n",
      "sources, such as blog post content, personal bio etc.\n",
      "●\n",
      "User Profiling: From the ingested data, the AI Assistant must deduce relevant user details\n",
      "and preferences, creating a personalized user profile.\n",
      "●\n",
      "Contextual Response Generation: During conversations, the AI Assistant should recall and\n",
      "leverage the user's profile information to provide contextualized and personalized\n",
      "responses.\n",
      "●\n",
      "End-to-End Functionality: Implement a chat CLI where the user inputs natural language\n",
      "queries, which can perform the following tasks:\n",
      "○\n",
      "Save something in memory\n",
      "○\n",
      "Deduce memory from unstructured text\n",
      "○\n",
      "Update memory\n",
      "○\n",
      "Delete memory\n",
      "Process the user input with your algorithm and return relevant responses.\n",
      "Bonus\n",
      "●\n",
      "Custom Retrieval Logic: Develop a unique logic for fetching the most relevant contextual\n",
      "information in response to user queries.\n",
      "●\n",
      "UI Design: Showcase your design prowess by creating a custom UI to show off your full\n",
      "stack skills.\n",
      "●\n",
      "Innovation Encouraged: We urge you to think outside the box. Impress us with your\n",
      "creativity and technical skills. The sky is merely the starting point.\n",
      "●\n",
      "Unit tests: Add unit tests if you can. We would love to see unit tests for your functionality.\n",
      "Deliverables\n",
      "●\n",
      "CLI interface: Develop a fully functional CLI where a user can interact with the AI assistant,\n",
      "ask questions, and store relevant information in memory for personalization.\n",
      "●\n",
      "Architecture Diagram: Provide a screenshot of an architecture diagram, created with tools\n",
      "like Excalidraw or using pen paper, detailing your strategy.\n",
      "●\n",
      "Code Quality: Submit well-documented code as a GitHub repository, accompanied by a\n",
      "comprehensive README. Ensure your code is of high quality.\n",
      "●\n",
      "GitHub Repository: Send us a link to your GitHub repository so that we can checkout the\n",
      "code.\n",
      "●\n",
      "Areas of improvements: While we understand that this is just an assignment, tell us what\n",
      "approaches you might have taken if you had one month to work on this problem\n",
      "statement to improve the memory system.\n",
      "Getting started\n",
      "●\n",
      "Feel free to use any open source library of your choice.\n",
      "Timeline\n",
      "●\n",
      "We have tested and found that the project can be fully finished in less than 72 hours.\n",
      "●\n",
      "Nevertheless, what matters is the final quality of the deliverable and showing a well\n",
      "thought methodology, proper working and task completion accuracy.\n",
      "●\n",
      "You have 4 days to submit the project from when you start (please send an email to\n",
      "founders@mem0.ai when you start the project to start tracking).\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 371 entries, 0 to 370\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Index       371 non-null    int64 \n",
      " 1   Text        371 non-null    object\n",
      " 2   Embeddings  371 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 8.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(output_csv)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[-0.06132100895047188, -0.09634503722190857, 0.01176245603710413, -0.055531393736600876, -0.017015518620610237, -0.019503764808177948, -0.01038281712681055, -0.008632141165435314, -0.033889081329107285, -0.02844257839024067, -0.09639979898929596, 0.003028060309588909, -0.031204696744680405, 0.01544266939163208, -0.0370916910469532, 0.05753031000494957, -0.03540007770061493, 0.048031534999608994, -0.08253747224807739, 0.012661705724895, -0.0010644815629348159, -0.04163643717765808, -0.04869190976023674, -0.07566624879837036, 0.016232024878263474, 0.09520312398672104, 0.0858212262392044, -0.06254158169031143, -0.05208657309412956, -0.07405931502580643, -0.034553881734609604, -0.03210700303316116, 0.014480569399893284, 0.022715356200933456, 0.01802811399102211, 0.06785898655653, 0.024928530678153038, 0.028537563979625702, -0.017395231872797012, -0.017841463908553123, -0.05682046338915825, 0.02140711061656475, -0.0018520014127716422, 0.023868205025792122, 0.0913764014840126, 0.024438846856355667, 0.05223768949508667, -0.03903620317578316, 0.015012158080935478, -0.03361320123076439, -0.10138815641403198, -0.06889690458774567, -0.012730740010738373, 0.016317663714289665, -0.07145599275827408, 0.07282856106758118, 0.06292679160833359, -0.004846874624490738, -0.003545805811882019, 0.02528364211320877, 0.03450021147727966, -0.030443638563156128, -0.00797845609486103, 0.03812142089009285, 0.017498549073934555, -0.02311786822974682, -0.007017870899289846, 0.020769082009792328, 0.08073792606592178, -0.0737646147608757, 0.04191108047962189, 0.1071496307849884, -0.01782139576971531, -0.01733831688761711, 0.0310515109449625, -0.016883743926882744, 0.08588937669992447, -0.04611785709857941, 0.08798790723085403, -0.06703987717628479, 0.12190399318933487, 0.01411288883537054, -0.00815446674823761, 0.048451606184244156, 0.09858124703168869, -0.07904526591300964, -0.08111231029033661, 0.013413871638476849, -0.03257521986961365, -0.05940653011202812, -0.01866120658814907, -0.05727424845099449, -0.03236769512295723, -0.011953339911997318, 0.06541185826063156, -0.009613986127078533, -0.038517169654369354, -0.13527262210845947, -0.05977066606283188, 0.021681074053049088, -0.055542781949043274, -0.04075077921152115, 0.012193865142762661, -0.027896126732230186, 0.03484988585114479, 0.056018564850091934, 0.03961518779397011, 0.040911175310611725, 0.08571156859397888, -0.041573818773031235, 0.0822707787156105, 0.02760974131524563, -0.03712117671966553, 0.005551782436668873, 0.008555954322218895, -0.0024343356490135193, -0.008197469636797905, 0.04505643621087074, 0.08273409307003021, 0.0065505364909768105, -0.11375453323125839, 0.012928643263876438, -0.062283970415592194, -0.03106580302119255, -0.0066464738920331, -0.07956596463918686, -0.06019308790564537, 1.6170168018589803e-34, -0.05294281244277954, 0.01101708970963955, 0.030139926820993423, 0.03098706342279911, 0.08296038955450058, -0.04697674140334129, -0.023730216547846794, 0.03386547788977623, -0.06445632129907608, -0.004579183179885149, -0.007823588326573372, 0.07583428919315338, -0.06909483671188354, 0.14517059922218323, 0.023926658555865288, -0.12173175811767578, -0.02322360873222351, 0.011087995953857899, -0.014115413650870323, 0.016040464863181114, -1.1993584848823957e-05, -0.0024286846164613962, -0.0007236138335429132, 0.01950831338763237, -0.0076047820039093494, -0.014972075819969177, 0.024927960708737373, 0.057628925889730453, 0.06392676383256912, -0.005998624954372644, -0.03827052190899849, 0.06198694929480553, -0.047260310500860214, -0.018421197310090065, 0.027870692312717438, 2.9595192245324142e-05, -0.016842585057020187, -0.01969834230840206, 0.052269674837589264, -0.039324406534433365, -0.014742708764970303, 0.08200886100530624, 0.012021023780107498, -0.09493854641914368, -0.02241268754005432, 0.026782220229506493, 0.07716438919305801, 0.008581244386732578, -0.04519376903772354, -0.05712568387389183, -0.06795763224363327, -0.016305727884173393, -0.05809495225548744, -0.08392830938100815, -0.014933538623154163, -0.03717122972011566, 0.029286816716194153, 0.02673213556408882, 0.06980215013027191, -0.05545990541577339, 0.03287370130419731, 0.07203445583581924, -0.04020499065518379, -0.02165483869612217, -0.04180086776614189, 0.033248309046030045, 0.06282579898834229, 0.041148457676172256, 0.0700688511133194, 0.02191946469247341, -0.015310361050069332, 0.0510459803044796, -0.011947983875870705, -0.09914163500070572, -0.0033746047411113977, 0.03780793026089668, 0.005764157045632601, 0.0019764855969697237, -0.026160985231399536, 0.11785117536783218, -0.048111844807863235, 0.0149121955037117, 0.010277638211846352, 0.006678020115941763, -0.03963708132505417, 0.08494013547897339, 0.010007316246628761, -0.07183932512998581, 0.007738959509879351, 0.025577399879693985, 0.00403577508404851, -0.009086526930332184, -0.020363783463835716, -0.0007549950969405472, -0.033926162868738174, -2.7363927079899013e-33, -0.005193980876356363, 0.06003756448626518, -0.0007526144618168473, 0.0173933207988739, 0.049456752836704254, -0.02207338996231556, -0.011579124256968498, 0.010557210072875023, -0.05343867465853691, -0.05239538475871086, -0.022205209359526634, 0.0007999904337339103, -0.04279332980513573, -0.002207712735980749, -0.027583513408899307, -0.011819861829280853, -0.02097955532371998, -0.020529842004179955, -0.09566793590784073, -0.09025388956069946, 0.014651592820882797, 0.11337260901927948, -0.09222979098558426, -0.03258577734231949, -0.05920562520623207, -0.030020438134670258, -0.03322305530309677, -0.008847348392009735, -0.0786275714635849, 0.04353410378098488, -1.4467274922935758e-05, -0.014444504864513874, -0.027380269020795822, 0.011054250411689281, -0.0040696533396840096, 0.06828762590885162, 0.014060967601835728, -0.010352878831326962, 0.007666484452784061, -0.020451245829463005, 0.059821709990501404, -0.016047872602939606, 0.015376169234514236, -0.03708086535334587, -0.007624512538313866, 0.005222912412136793, 0.00690885167568922, 0.041595783084630966, -0.03631291538476944, -0.019677966833114624, -0.0006506814970634878, 0.044020283967256546, 0.005516161676496267, -0.11221620440483093, 0.030017325654625893, -0.026110710576176643, 0.04129950329661369, -0.009912853129208088, 0.0392255000770092, 0.08139143884181976, -0.067711740732193, -0.059072140604257584, -0.06535092741250992, 0.014363831840455532, -0.029474547132849693, 0.05808534100651741, -0.09452559053897858, 0.057846538722515106, -0.013534829020500183, -0.035681404173374176, 0.0447574257850647, 0.030630195513367653, -0.023544082418084145, 0.058635368943214417, -0.09652203321456909, 0.016296925023198128, 0.06225627660751343, -0.020384378731250763, 0.010497167706489563, 0.033387474715709686, 0.07639353722333908, -0.11530221253633499, -0.006093124393373728, 0.12164177000522614, 0.02222265675663948, 0.1692737489938736, -0.010106861591339111, -0.107184037566185, 0.0401252806186676, -0.025852715596556664, -0.11041052639484406, 0.05203859135508537, -0.0010782367317005992, -0.04359737038612366, 0.0027628943789750338, -3.5126383579608955e-08, -0.05710253864526749, 0.05239817500114441, 0.048624537885189056, -0.05425135791301727, -4.345341585576534e-05, -0.02005545049905777, -0.02016637846827507, 0.11815379559993744, -0.03168250992894173, 0.03211180493235588, 0.009159158915281296, -0.03702982887625694, -0.05535430461168289, -0.011781807988882065, 0.07892866432666779, 0.10382583737373352, 0.09987084567546844, -0.04444245249032974, 0.05023793503642082, 0.013522528111934662, 0.06618176400661469, 0.010517734102904797, -0.018950585275888443, 0.0280145276337862, -0.045019928365945816, -0.030930306762456894, 0.00026580996927805245, 0.018800387158989906, -0.04692323878407478, -0.01966703124344349, -0.013117851689457893, 0.08247509598731995, 0.013197978027164936, -0.020638369023799896, 0.08307179808616638, 0.002106060739606619, -0.006016467697918415, -0.02041691727936268, -0.025102602317929268, -0.05693778395652771, -0.053276896476745605, -0.00870474148541689, 0.06403608620166779, 0.018236687406897545, -0.024244006723165512, 0.030995965003967285, 0.062463514506816864, -0.03250289708375931, -0.021578272804617882, 0.03899332508444786, -0.026262450963258743, 0.015853624790906906, 0.06698515266180038, 0.06577911227941513, 0.021292684599757195, 0.029409555718302727, 0.043043483048677444, -0.02508562244474888, -0.07020927220582962, 0.20137342810630798, 0.04441576078534126, 0.056686073541641235, -0.07862310111522675, 0.04889542609453201]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Embeddings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hey there, this is Pranay!\n",
    "I'm a Master's student in Computer Science at the University of Massachusetts Amherst, focusing on Deep Learning and Computer\n",
    "Vision. My academic path has led me to explore the compelling crossroads of these fields, giving me the opportunity to delve deep and\n",
    "gain hands-on experience.\n",
    "Previously, I was a Graduate Student Researcher with Reality Labs team at Meta, collaborating with Dr. Shane Moon on the IMU2CLIP\n",
    "project. I also had the opportunity to be a research intern at AirLab, associated with the Robotics Institute at Carnegie Mellon University.\n",
    "There, I worked alongside Dr. Chen Wang and Dr. Sebastian Scherer on Few Shot Object Detection, and contributed towards PyPose.\n",
    "My initial steps into research were guided by Dr. Sumeet Kumar at the Indian School of Business Hyderabad, where I worked on\n",
    "identifying surrogate product placements in YouTube Kids videos using Few Shot Learning.\n",
    "Contact : pranayr@umass.edu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "News\n",
    "• Mar 2023 - PyPose was accepted at CVPR 2023. Stay tuned for more details!\n",
    "• Feb 2023 - Started working with Meta Reality Labs on the IMU2CLIP project.\n",
    "• Sep 2022 - Joined UMass Amherst for my masters in Computer Science\n",
    "• July 2022 - One paper accepted at ECCV 2022. Stay tuned for more details!\n",
    "• Feb 2022 - Acting as a reviewer for IEEE RA-L.\n",
    "• Sep 2021 - Started as an intern at AirLab, CMU.\n",
    "• May 2021 - Started as an intern at SRITNE, ISB Hyderabad.\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import numpy as np\n",
    "\n",
    "def extract_text(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def encode_texts(texts, model_name='all-MiniLM-L6-v2'):\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "def save_to_csv(file_path, texts, embeddings):\n",
    "\n",
    "    data = {\n",
    "        \"Index\": list(range(len(texts))),\n",
    "        \"Text\": texts,\n",
    "        \"Embeddings\": [emb.tolist() for emb in embeddings]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def process_and_save_text(file_path, output_csv):\n",
    "    text = extract_text(file_path)\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    embeddings = encode_texts(chunks)\n",
    "    \n",
    "    save_to_csv(output_csv, chunks, embeddings)\n",
    "    print(f\"Chunks and embeddings saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=2)\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "embeddings = encode_texts(chunks)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV...\n",
      "before literal eval - type= <class 'str'> [-0.06132100895047188, -0.09634503722190857, 0.01176245603710413, -0.055531393736600876, -0.017015518620610237, -0.019503764808177948, -0.01038281712681055, -0.008632141165435314, -0.033889081329107285, -0.02844257839024067, -0.09639979898929596, 0.003028060309588909, -0.031204696744680405, 0.01544266939163208, -0.0370916910469532, 0.05753031000494957, -0.03540007770061493, 0.048031534999608994, -0.08253747224807739, 0.012661705724895, -0.0010644815629348159, -0.04163643717765808, -0.04869190976023674, -0.07566624879837036, 0.016232024878263474, 0.09520312398672104, 0.0858212262392044, -0.06254158169031143, -0.05208657309412956, -0.07405931502580643, -0.034553881734609604, -0.03210700303316116, 0.014480569399893284, 0.022715356200933456, 0.01802811399102211, 0.06785898655653, 0.024928530678153038, 0.028537563979625702, -0.017395231872797012, -0.017841463908553123, -0.05682046338915825, 0.02140711061656475, -0.0018520014127716422, 0.023868205025792122, 0.0913764014840126, 0.024438846856355667, 0.05223768949508667, -0.03903620317578316, 0.015012158080935478, -0.03361320123076439, -0.10138815641403198, -0.06889690458774567, -0.012730740010738373, 0.016317663714289665, -0.07145599275827408, 0.07282856106758118, 0.06292679160833359, -0.004846874624490738, -0.003545805811882019, 0.02528364211320877, 0.03450021147727966, -0.030443638563156128, -0.00797845609486103, 0.03812142089009285, 0.017498549073934555, -0.02311786822974682, -0.007017870899289846, 0.020769082009792328, 0.08073792606592178, -0.0737646147608757, 0.04191108047962189, 0.1071496307849884, -0.01782139576971531, -0.01733831688761711, 0.0310515109449625, -0.016883743926882744, 0.08588937669992447, -0.04611785709857941, 0.08798790723085403, -0.06703987717628479, 0.12190399318933487, 0.01411288883537054, -0.00815446674823761, 0.048451606184244156, 0.09858124703168869, -0.07904526591300964, -0.08111231029033661, 0.013413871638476849, -0.03257521986961365, -0.05940653011202812, -0.01866120658814907, -0.05727424845099449, -0.03236769512295723, -0.011953339911997318, 0.06541185826063156, -0.009613986127078533, -0.038517169654369354, -0.13527262210845947, -0.05977066606283188, 0.021681074053049088, -0.055542781949043274, -0.04075077921152115, 0.012193865142762661, -0.027896126732230186, 0.03484988585114479, 0.056018564850091934, 0.03961518779397011, 0.040911175310611725, 0.08571156859397888, -0.041573818773031235, 0.0822707787156105, 0.02760974131524563, -0.03712117671966553, 0.005551782436668873, 0.008555954322218895, -0.0024343356490135193, -0.008197469636797905, 0.04505643621087074, 0.08273409307003021, 0.0065505364909768105, -0.11375453323125839, 0.012928643263876438, -0.062283970415592194, -0.03106580302119255, -0.0066464738920331, -0.07956596463918686, -0.06019308790564537, 1.6170168018589803e-34, -0.05294281244277954, 0.01101708970963955, 0.030139926820993423, 0.03098706342279911, 0.08296038955450058, -0.04697674140334129, -0.023730216547846794, 0.03386547788977623, -0.06445632129907608, -0.004579183179885149, -0.007823588326573372, 0.07583428919315338, -0.06909483671188354, 0.14517059922218323, 0.023926658555865288, -0.12173175811767578, -0.02322360873222351, 0.011087995953857899, -0.014115413650870323, 0.016040464863181114, -1.1993584848823957e-05, -0.0024286846164613962, -0.0007236138335429132, 0.01950831338763237, -0.0076047820039093494, -0.014972075819969177, 0.024927960708737373, 0.057628925889730453, 0.06392676383256912, -0.005998624954372644, -0.03827052190899849, 0.06198694929480553, -0.047260310500860214, -0.018421197310090065, 0.027870692312717438, 2.9595192245324142e-05, -0.016842585057020187, -0.01969834230840206, 0.052269674837589264, -0.039324406534433365, -0.014742708764970303, 0.08200886100530624, 0.012021023780107498, -0.09493854641914368, -0.02241268754005432, 0.026782220229506493, 0.07716438919305801, 0.008581244386732578, -0.04519376903772354, -0.05712568387389183, -0.06795763224363327, -0.016305727884173393, -0.05809495225548744, -0.08392830938100815, -0.014933538623154163, -0.03717122972011566, 0.029286816716194153, 0.02673213556408882, 0.06980215013027191, -0.05545990541577339, 0.03287370130419731, 0.07203445583581924, -0.04020499065518379, -0.02165483869612217, -0.04180086776614189, 0.033248309046030045, 0.06282579898834229, 0.041148457676172256, 0.0700688511133194, 0.02191946469247341, -0.015310361050069332, 0.0510459803044796, -0.011947983875870705, -0.09914163500070572, -0.0033746047411113977, 0.03780793026089668, 0.005764157045632601, 0.0019764855969697237, -0.026160985231399536, 0.11785117536783218, -0.048111844807863235, 0.0149121955037117, 0.010277638211846352, 0.006678020115941763, -0.03963708132505417, 0.08494013547897339, 0.010007316246628761, -0.07183932512998581, 0.007738959509879351, 0.025577399879693985, 0.00403577508404851, -0.009086526930332184, -0.020363783463835716, -0.0007549950969405472, -0.033926162868738174, -2.7363927079899013e-33, -0.005193980876356363, 0.06003756448626518, -0.0007526144618168473, 0.0173933207988739, 0.049456752836704254, -0.02207338996231556, -0.011579124256968498, 0.010557210072875023, -0.05343867465853691, -0.05239538475871086, -0.022205209359526634, 0.0007999904337339103, -0.04279332980513573, -0.002207712735980749, -0.027583513408899307, -0.011819861829280853, -0.02097955532371998, -0.020529842004179955, -0.09566793590784073, -0.09025388956069946, 0.014651592820882797, 0.11337260901927948, -0.09222979098558426, -0.03258577734231949, -0.05920562520623207, -0.030020438134670258, -0.03322305530309677, -0.008847348392009735, -0.0786275714635849, 0.04353410378098488, -1.4467274922935758e-05, -0.014444504864513874, -0.027380269020795822, 0.011054250411689281, -0.0040696533396840096, 0.06828762590885162, 0.014060967601835728, -0.010352878831326962, 0.007666484452784061, -0.020451245829463005, 0.059821709990501404, -0.016047872602939606, 0.015376169234514236, -0.03708086535334587, -0.007624512538313866, 0.005222912412136793, 0.00690885167568922, 0.041595783084630966, -0.03631291538476944, -0.019677966833114624, -0.0006506814970634878, 0.044020283967256546, 0.005516161676496267, -0.11221620440483093, 0.030017325654625893, -0.026110710576176643, 0.04129950329661369, -0.009912853129208088, 0.0392255000770092, 0.08139143884181976, -0.067711740732193, -0.059072140604257584, -0.06535092741250992, 0.014363831840455532, -0.029474547132849693, 0.05808534100651741, -0.09452559053897858, 0.057846538722515106, -0.013534829020500183, -0.035681404173374176, 0.0447574257850647, 0.030630195513367653, -0.023544082418084145, 0.058635368943214417, -0.09652203321456909, 0.016296925023198128, 0.06225627660751343, -0.020384378731250763, 0.010497167706489563, 0.033387474715709686, 0.07639353722333908, -0.11530221253633499, -0.006093124393373728, 0.12164177000522614, 0.02222265675663948, 0.1692737489938736, -0.010106861591339111, -0.107184037566185, 0.0401252806186676, -0.025852715596556664, -0.11041052639484406, 0.05203859135508537, -0.0010782367317005992, -0.04359737038612366, 0.0027628943789750338, -3.5126383579608955e-08, -0.05710253864526749, 0.05239817500114441, 0.048624537885189056, -0.05425135791301727, -4.345341585576534e-05, -0.02005545049905777, -0.02016637846827507, 0.11815379559993744, -0.03168250992894173, 0.03211180493235588, 0.009159158915281296, -0.03702982887625694, -0.05535430461168289, -0.011781807988882065, 0.07892866432666779, 0.10382583737373352, 0.09987084567546844, -0.04444245249032974, 0.05023793503642082, 0.013522528111934662, 0.06618176400661469, 0.010517734102904797, -0.018950585275888443, 0.0280145276337862, -0.045019928365945816, -0.030930306762456894, 0.00026580996927805245, 0.018800387158989906, -0.04692323878407478, -0.01966703124344349, -0.013117851689457893, 0.08247509598731995, 0.013197978027164936, -0.020638369023799896, 0.08307179808616638, 0.002106060739606619, -0.006016467697918415, -0.02041691727936268, -0.025102602317929268, -0.05693778395652771, -0.053276896476745605, -0.00870474148541689, 0.06403608620166779, 0.018236687406897545, -0.024244006723165512, 0.030995965003967285, 0.062463514506816864, -0.03250289708375931, -0.021578272804617882, 0.03899332508444786, -0.026262450963258743, 0.015853624790906906, 0.06698515266180038, 0.06577911227941513, 0.021292684599757195, 0.029409555718302727, 0.043043483048677444, -0.02508562244474888, -0.07020927220582962, 0.20137342810630798, 0.04441576078534126, 0.056686073541641235, -0.07862310111522675, 0.04889542609453201]\n",
      "after literal eval - type= <class 'numpy.ndarray'> [-6.13210090e-02 -9.63450372e-02  1.17624560e-02 -5.55313937e-02\n",
      " -1.70155186e-02 -1.95037648e-02 -1.03828171e-02 -8.63214117e-03\n",
      " -3.38890813e-02 -2.84425784e-02 -9.63997990e-02  3.02806031e-03\n",
      " -3.12046967e-02  1.54426694e-02 -3.70916910e-02  5.75303100e-02\n",
      " -3.54000777e-02  4.80315350e-02 -8.25374722e-02  1.26617057e-02\n",
      " -1.06448156e-03 -4.16364372e-02 -4.86919098e-02 -7.56662488e-02\n",
      "  1.62320249e-02  9.52031240e-02  8.58212262e-02 -6.25415817e-02\n",
      " -5.20865731e-02 -7.40593150e-02 -3.45538817e-02 -3.21070030e-02\n",
      "  1.44805694e-02  2.27153562e-02  1.80281140e-02  6.78589866e-02\n",
      "  2.49285307e-02  2.85375640e-02 -1.73952319e-02 -1.78414639e-02\n",
      " -5.68204634e-02  2.14071106e-02 -1.85200141e-03  2.38682050e-02\n",
      "  9.13764015e-02  2.44388469e-02  5.22376895e-02 -3.90362032e-02\n",
      "  1.50121581e-02 -3.36132012e-02 -1.01388156e-01 -6.88969046e-02\n",
      " -1.27307400e-02  1.63176637e-02 -7.14559928e-02  7.28285611e-02\n",
      "  6.29267916e-02 -4.84687462e-03 -3.54580581e-03  2.52836421e-02\n",
      "  3.45002115e-02 -3.04436386e-02 -7.97845609e-03  3.81214209e-02\n",
      "  1.74985491e-02 -2.31178682e-02 -7.01787090e-03  2.07690820e-02\n",
      "  8.07379261e-02 -7.37646148e-02  4.19110805e-02  1.07149631e-01\n",
      " -1.78213958e-02 -1.73383169e-02  3.10515109e-02 -1.68837439e-02\n",
      "  8.58893767e-02 -4.61178571e-02  8.79879072e-02 -6.70398772e-02\n",
      "  1.21903993e-01  1.41128888e-02 -8.15446675e-03  4.84516062e-02\n",
      "  9.85812470e-02 -7.90452659e-02 -8.11123103e-02  1.34138716e-02\n",
      " -3.25752199e-02 -5.94065301e-02 -1.86612066e-02 -5.72742485e-02\n",
      " -3.23676951e-02 -1.19533399e-02  6.54118583e-02 -9.61398613e-03\n",
      " -3.85171697e-02 -1.35272622e-01 -5.97706661e-02  2.16810741e-02\n",
      " -5.55427819e-02 -4.07507792e-02  1.21938651e-02 -2.78961267e-02\n",
      "  3.48498859e-02  5.60185649e-02  3.96151878e-02  4.09111753e-02\n",
      "  8.57115686e-02 -4.15738188e-02  8.22707787e-02  2.76097413e-02\n",
      " -3.71211767e-02  5.55178244e-03  8.55595432e-03 -2.43433565e-03\n",
      " -8.19746964e-03  4.50564362e-02  8.27340931e-02  6.55053649e-03\n",
      " -1.13754533e-01  1.29286433e-02 -6.22839704e-02 -3.10658030e-02\n",
      " -6.64647389e-03 -7.95659646e-02 -6.01930879e-02  1.61701680e-34\n",
      " -5.29428124e-02  1.10170897e-02  3.01399268e-02  3.09870634e-02\n",
      "  8.29603896e-02 -4.69767414e-02 -2.37302165e-02  3.38654779e-02\n",
      " -6.44563213e-02 -4.57918318e-03 -7.82358833e-03  7.58342892e-02\n",
      " -6.90948367e-02  1.45170599e-01  2.39266586e-02 -1.21731758e-01\n",
      " -2.32236087e-02  1.10879960e-02 -1.41154137e-02  1.60404649e-02\n",
      " -1.19935848e-05 -2.42868462e-03 -7.23613834e-04  1.95083134e-02\n",
      " -7.60478200e-03 -1.49720758e-02  2.49279607e-02  5.76289259e-02\n",
      "  6.39267638e-02 -5.99862495e-03 -3.82705219e-02  6.19869493e-02\n",
      " -4.72603105e-02 -1.84211973e-02  2.78706923e-02  2.95951922e-05\n",
      " -1.68425851e-02 -1.96983423e-02  5.22696748e-02 -3.93244065e-02\n",
      " -1.47427088e-02  8.20088610e-02  1.20210238e-02 -9.49385464e-02\n",
      " -2.24126875e-02  2.67822202e-02  7.71643892e-02  8.58124439e-03\n",
      " -4.51937690e-02 -5.71256839e-02 -6.79576322e-02 -1.63057279e-02\n",
      " -5.80949523e-02 -8.39283094e-02 -1.49335386e-02 -3.71712297e-02\n",
      "  2.92868167e-02  2.67321356e-02  6.98021501e-02 -5.54599054e-02\n",
      "  3.28737013e-02  7.20344558e-02 -4.02049907e-02 -2.16548387e-02\n",
      " -4.18008678e-02  3.32483090e-02  6.28257990e-02  4.11484577e-02\n",
      "  7.00688511e-02  2.19194647e-02 -1.53103611e-02  5.10459803e-02\n",
      " -1.19479839e-02 -9.91416350e-02 -3.37460474e-03  3.78079303e-02\n",
      "  5.76415705e-03  1.97648560e-03 -2.61609852e-02  1.17851175e-01\n",
      " -4.81118448e-02  1.49121955e-02  1.02776382e-02  6.67802012e-03\n",
      " -3.96370813e-02  8.49401355e-02  1.00073162e-02 -7.18393251e-02\n",
      "  7.73895951e-03  2.55773999e-02  4.03577508e-03 -9.08652693e-03\n",
      " -2.03637835e-02 -7.54995097e-04 -3.39261629e-02 -2.73639271e-33\n",
      " -5.19398088e-03  6.00375645e-02 -7.52614462e-04  1.73933208e-02\n",
      "  4.94567528e-02 -2.20733900e-02 -1.15791243e-02  1.05572101e-02\n",
      " -5.34386747e-02 -5.23953848e-02 -2.22052094e-02  7.99990434e-04\n",
      " -4.27933298e-02 -2.20771274e-03 -2.75835134e-02 -1.18198618e-02\n",
      " -2.09795553e-02 -2.05298420e-02 -9.56679359e-02 -9.02538896e-02\n",
      "  1.46515928e-02  1.13372609e-01 -9.22297910e-02 -3.25857773e-02\n",
      " -5.92056252e-02 -3.00204381e-02 -3.32230553e-02 -8.84734839e-03\n",
      " -7.86275715e-02  4.35341038e-02 -1.44672749e-05 -1.44445049e-02\n",
      " -2.73802690e-02  1.10542504e-02 -4.06965334e-03  6.82876259e-02\n",
      "  1.40609676e-02 -1.03528788e-02  7.66648445e-03 -2.04512458e-02\n",
      "  5.98217100e-02 -1.60478726e-02  1.53761692e-02 -3.70808654e-02\n",
      " -7.62451254e-03  5.22291241e-03  6.90885168e-03  4.15957831e-02\n",
      " -3.63129154e-02 -1.96779668e-02 -6.50681497e-04  4.40202840e-02\n",
      "  5.51616168e-03 -1.12216204e-01  3.00173257e-02 -2.61107106e-02\n",
      "  4.12995033e-02 -9.91285313e-03  3.92255001e-02  8.13914388e-02\n",
      " -6.77117407e-02 -5.90721406e-02 -6.53509274e-02  1.43638318e-02\n",
      " -2.94745471e-02  5.80853410e-02 -9.45255905e-02  5.78465387e-02\n",
      " -1.35348290e-02 -3.56814042e-02  4.47574258e-02  3.06301955e-02\n",
      " -2.35440824e-02  5.86353689e-02 -9.65220332e-02  1.62969250e-02\n",
      "  6.22562766e-02 -2.03843787e-02  1.04971677e-02  3.33874747e-02\n",
      "  7.63935372e-02 -1.15302213e-01 -6.09312439e-03  1.21641770e-01\n",
      "  2.22226568e-02  1.69273749e-01 -1.01068616e-02 -1.07184038e-01\n",
      "  4.01252806e-02 -2.58527156e-02 -1.10410526e-01  5.20385914e-02\n",
      " -1.07823673e-03 -4.35973704e-02  2.76289438e-03 -3.51263836e-08\n",
      " -5.71025386e-02  5.23981750e-02  4.86245379e-02 -5.42513579e-02\n",
      " -4.34534159e-05 -2.00554505e-02 -2.01663785e-02  1.18153796e-01\n",
      " -3.16825099e-02  3.21118049e-02  9.15915892e-03 -3.70298289e-02\n",
      " -5.53543046e-02 -1.17818080e-02  7.89286643e-02  1.03825837e-01\n",
      "  9.98708457e-02 -4.44424525e-02  5.02379350e-02  1.35225281e-02\n",
      "  6.61817640e-02  1.05177341e-02 -1.89505853e-02  2.80145276e-02\n",
      " -4.50199284e-02 -3.09303068e-02  2.65809969e-04  1.88003872e-02\n",
      " -4.69232388e-02 -1.96670312e-02 -1.31178517e-02  8.24750960e-02\n",
      "  1.31979780e-02 -2.06383690e-02  8.30717981e-02  2.10606074e-03\n",
      " -6.01646770e-03 -2.04169173e-02 -2.51026023e-02 -5.69377840e-02\n",
      " -5.32768965e-02 -8.70474149e-03  6.40360862e-02  1.82366874e-02\n",
      " -2.42440067e-02  3.09959650e-02  6.24635145e-02 -3.25028971e-02\n",
      " -2.15782728e-02  3.89933251e-02 -2.62624510e-02  1.58536248e-02\n",
      "  6.69851527e-02  6.57791123e-02  2.12926846e-02  2.94095557e-02\n",
      "  4.30434830e-02 -2.50856224e-02 -7.02092722e-02  2.01373428e-01\n",
      "  4.44157608e-02  5.66860735e-02 -7.86231011e-02  4.88954261e-02]\n",
      "CSV Loaded.\n",
      "Saving new entry: Hello buddies I'm Rohith siddh...\n",
      "Encoding text: Hello buddies I'm Rohith siddh...\n",
      "Text encoded. Embedding shape: (384,)\n",
      "Saving CSV...\n",
      "CSV Saved.\n",
      "Entry saved to CSV.\n",
      "Retrieving entries using mmr metric for query: rohithsiddhartha...\n",
      "Encoding text: rohithsiddhartha...\n",
      "Text encoded. Embedding shape: (384,)\n",
      "Running MMR...\n",
      "MMR completed.\n",
      "Retrieved entries:\n",
      "Hello buddies I'm Rohith siddhartha a graduate from iit khragarpu\n",
      "a binary or multi-class classifier.\n",
      " \n",
      " It is termed “naive” because it makes the naive assumption of conditional independence between every pair of features given the value of the class variable.\n",
      " \n",
      " The figure below below\n",
      " \n",
      " (source)\n",
      " \n",
      " shows the\n",
      "(May 2014 - Jul 2014)\n",
      " \n",
      " Responsible for development of\n",
      " \n",
      " company's web application\n",
      " \n",
      " ,\n",
      " Facebook App development and setting up NoSQL database. Implemented scripts to do image enhancement, and\n",
      " cropping borders etc.\n",
      " \n",
      " Educational\n",
      "gradient of the loss function with respect to the current prediction.\n",
      " \n",
      " Fit a new model to the negative gradient.\n",
      " \n",
      " Update the prediction by adding the prediction from the new model.\n",
      " \n",
      " Repeat steps 2-4 for a specified number of iterations, or\n",
      "in the data:\n",
      " \n",
      " Calculate distance between query example and current example from the data.\n",
      " \n",
      " Add the distance and index to an ordered collection.\n",
      " \n",
      " Sort in ascending order by distance.\n",
      " \n",
      " Pick first\n",
      " \n",
      " k\n",
      " \n",
      " k\n",
      " \n",
      " from sorted order.\n",
      " \n",
      " Get labels of\n",
      "Das, Stefan Lee, Dhruv Batra, Devi Parikh\n",
      " \n",
      " HCOMP 2017\n",
      " \n",
      " Code\n",
      " \n",
      " It Takes Two to Tango: Towards Theory of AI's Mind\n",
      " \n",
      " Arjun Chandrasekaran\n",
      " \n",
      " *\n",
      " \n",
      " ,\n",
      " \n",
      " Deshraj Yadav\n",
      " \n",
      " *\n",
      " \n",
      " , Prithvijit\n",
      " Chattopadhyay\n",
      " \n",
      " *\n",
      " \n",
      " , Viraj Prabhu\n",
      " \n",
      " *\n",
      " \n",
      " , Devi Parikh\n",
      "365\n",
      " \n",
      " =\n",
      " \n",
      " 18\n",
      " \n",
      " days\n",
      " \n",
      " w\n",
      " \n",
      " a\n",
      " \n",
      " k\n",
      " \n",
      " e\n",
      " \n",
      " U\n",
      " \n",
      " p\n",
      " \n",
      " =\n",
      " \n",
      " p\n",
      " \n",
      " (\n",
      " \n",
      " c\n",
      " \n",
      " r\n",
      " \n",
      " y\n",
      " \n",
      " |\n",
      " \n",
      " n\n",
      " \n",
      " i\n",
      " \n",
      " g\n",
      " \n",
      " h\n",
      " \n",
      " t\n",
      " \n",
      " s\n",
      " \n",
      " )\n",
      " \n",
      " ∗\n",
      " \n",
      " n\n",
      " \n",
      " i\n",
      " \n",
      " g\n",
      " \n",
      " h\n",
      " \n",
      " t\n",
      " \n",
      " s\n",
      " \n",
      " =\n",
      " \n",
      " 0.05\n",
      " \n",
      " ∗\n",
      " \n",
      " 365\n",
      " \n",
      " =\n",
      " \n",
      " 18\n",
      " \n",
      " days\n",
      " \n",
      " Or you may want to convert it into a binary\n",
      "of an HMM\n",
      " \n",
      " A Hidden Markov Model is characterized by the following components:\n",
      " \n",
      " States:\n",
      " \n",
      " These are the hidden states in the model. They cannot be directly observed, but they can be inferred from the observable states.\n",
      " \n",
      " Observations:\n",
      " \n",
      " These\n",
      "create a personalized user profile\n",
      "3.\n",
      "Then this profile which is stored should be used for custom response generation. -> This\n",
      "profile should be saved in memory and should be constantly\n",
      "it works best on datasets where clusters are spherical and roughly the same size.\n",
      " \n",
      " What is Latent Dirichlet Allocation (LDA)?\n",
      " \n",
      " Latent Dirichlet Allocation (LDA) is a generative statistical model widely used for topic modeling in natural language\n",
      "Updating entry for query: rohithsiddhartha graduated fro with new text: Hello buddies I'm Rohith siddh...\n",
      "Retrieving entries using euclidean metric for query: rohithsiddhartha graduated fro...\n",
      "Encoding text: rohithsiddhartha graduated fro...\n",
      "Text encoded. Embedding shape: (384,)\n",
      "Retrieved 1 entries.\n",
      "Top 1 similar entry:\n",
      "Hello buddies I'm Rohith siddhartha a graduate from iit khragarpu\n",
      "Encoding text: rohithsiddhartha graduated fro...\n",
      "Text encoded. Embedding shape: (384,)\n",
      "Saving CSV...\n",
      "CSV Saved.\n",
      "Entry updated in CSV.\n",
      "Deleting entry for query: iit kharagpur...\n",
      "Retrieving entries using euclidean metric for query: iit kharagpur...\n",
      "Encoding text: iit kharagpur...\n",
      "Text encoded. Embedding shape: (384,)\n",
      "Retrieved 1 entries.\n",
      "Top 1 similar entry:\n",
      "rohithsiddhartha graduated from iit kharagpur\n",
      "Saving CSV...\n",
      "CSV Saved.\n",
      "Entry deleted from CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from ast import literal_eval\n",
    "\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, csv_file, model_name='all-MiniLM-L6-v2'):\n",
    "        self.csv_file = csv_file\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.df = self.load_csv()\n",
    "\n",
    "    def load_csv(self):\n",
    "        print(\"Loading CSV...\")\n",
    "        df = pd.read_csv(self.csv_file)\n",
    "        print(f\"before literal eval - type= {type(df['Embeddings'][0])}\", df['Embeddings'][0])\n",
    "        df['Embeddings'] = df['Embeddings'].apply(literal_eval).apply(np.array)\n",
    "        print(f\"after literal eval - type= {type(df['Embeddings'][0])}\", df['Embeddings'][0])\n",
    "        print(\"CSV Loaded.\")\n",
    "        return df\n",
    "\n",
    "    def save_csv(self):\n",
    "        print(\"Saving CSV...\")\n",
    "        self.df['Embeddings'] = self.df['Embeddings'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "        self.df.to_csv(self.csv_file, index=False)\n",
    "        print(\"CSV Saved.\")\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        print(f\"Encoding text: {text[:30]}...\")  # Print the first 30 characters of the text\n",
    "        embedding = self.model.encode([text], convert_to_tensor=True).cpu().numpy()[0]\n",
    "        embedding = np.ascontiguousarray(embedding, dtype=np.float32)\n",
    "        print(f\"Text encoded. Embedding shape: {embedding.shape}\")\n",
    "        return embedding\n",
    "\n",
    "    def save_entry(self, text):\n",
    "        print(f\"Saving new entry: {text[:30]}...\")  # Print the first 30 characters of the text\n",
    "        new_embedding = self.encode_text(text)\n",
    "        new_entry = pd.DataFrame([{'Text': text, 'Embeddings': new_embedding.tolist()}])\n",
    "        self.df = pd.concat([self.df, new_entry], ignore_index=True)\n",
    "        self.save_csv()\n",
    "        print(\"Entry saved to CSV.\")\n",
    "\n",
    "    def retrieve_entries(self, query, k=10, metric='euclidean'):\n",
    "        print(f\"Retrieving entries using {metric} metric for query: {query[:30]}...\")  # Print the first 30 characters of the query\n",
    "        query_embedding = self.encode_text(query)\n",
    "        embeddings = np.vstack(self.df['Embeddings'].values)\n",
    "        if metric == 'cosine':\n",
    "            faiss.normalize_L2(embeddings)\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "            index = faiss.IndexFlatIP(embeddings.shape[1]) \n",
    "        elif metric == 'mmr':\n",
    "            return self.mmr(query_embedding, top_k=k)\n",
    "        else:\n",
    "            index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        \n",
    "        index.add(embeddings)\n",
    "        D, I = index.search(query_embedding.reshape(1, -1), k)\n",
    "        top_k_texts = [self.df.iloc[i]['Text'] for i in I[0]]\n",
    "        print(f\"Retrieved {len(top_k_texts)} entries.\")\n",
    "        return top_k_texts\n",
    "\n",
    "    def update_entry(self, query, k=1):\n",
    "        print(f\"Updating entry for query: {query[:30]} with new text: {new_text[:30]}...\")  # Print the first 30 characters\n",
    "        top_k_texts = self.retrieve_entries(query, k, metric='euclidean')\n",
    "        print(\"Top 1 similar entry:\")\n",
    "        print(top_k_texts[0])\n",
    "        \n",
    "        confirmation = input(\"Do you want to update this entry? (yes/no): \").strip().lower()\n",
    "        if confirmation == 'yes':\n",
    "            index_to_update = self.df[self.df['Text'] == top_k_texts[0]].index[0]\n",
    "            updated_embedding = self.encode_text(query)\n",
    "            self.df.at[index_to_update, 'Text'] = query\n",
    "            self.df.at[index_to_update, 'Embeddings'] = updated_embedding.tolist()\n",
    "            self.save_csv()\n",
    "            print(\"Entry updated in CSV.\")\n",
    "        else:\n",
    "            print(\"Update cancelled.\")\n",
    "\n",
    "    def delete_entry(self, query, k=1):\n",
    "        print(f\"Deleting entry for query: {query[:30]}...\")  # Print the first 30 characters of the query\n",
    "        top_k_texts = self.retrieve_entries(query, k, metric='euclidean')\n",
    "        print(\"Top 1 similar entry:\")\n",
    "        print(top_k_texts[0])\n",
    "        \n",
    "        confirmation = input(\"Do you want to delete this entry? (yes/no): \").strip().lower()\n",
    "        if confirmation == 'yes':\n",
    "            index_to_remove = self.df[self.df['Text'] == top_k_texts[0]].index[0]\n",
    "            self.df = self.df.drop(index_to_remove).reset_index(drop=True)\n",
    "            self.save_csv()\n",
    "            print(\"Entry deleted from CSV.\")\n",
    "        else:\n",
    "            print(\"Deletion cancelled.\")\n",
    "\n",
    "    def mmr(self, query_embedding, top_k=5, lambda_param=0.5):\n",
    "        print(\"Running MMR...\")\n",
    "        doc_embeddings = np.vstack(self.df['Embeddings'].values)\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        doc_embeddings = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        sim_scores = np.dot(doc_embeddings, query_embedding.T).flatten()\n",
    "        \n",
    "        selected = []\n",
    "        selected_scores = []\n",
    "        \n",
    "        while len(selected) < top_k:\n",
    "            if not selected:\n",
    "                selected_idx = np.argmax(sim_scores)\n",
    "            else:\n",
    "                mmr_scores = []\n",
    "                for idx in range(len(doc_embeddings)):\n",
    "                    if idx not in selected:\n",
    "                        diversity = max([np.dot(doc_embeddings[idx], doc_embeddings[sel_idx]) for sel_idx in selected])\n",
    "                        mmr_score = lambda_param * sim_scores[idx] - (1 - lambda_param) * diversity\n",
    "                        mmr_scores.append((mmr_score, idx))\n",
    "                selected_idx = max(mmr_scores)[1]\n",
    "            \n",
    "            selected.append(selected_idx)\n",
    "            selected_scores.append(sim_scores[selected_idx])\n",
    "        \n",
    "        print(\"MMR completed.\")\n",
    "        return [self.df.iloc[i]['Text'] for i in selected]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"/Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/all_chunks_and_embeddings.csv\"\n",
    "    manager = EmbeddingManager(csv_file)\n",
    "    \n",
    "    while True:\n",
    "        operation = input(\"Enter operation (save, retrieve, update, delete, exit): \").strip().lower()\n",
    "        \n",
    "        if operation == 'save':\n",
    "            new_text = input(\"Enter the text to save: \")\n",
    "            manager.save_entry(new_text)\n",
    "        elif operation == 'retrieve':\n",
    "            query = input(\"Enter the query text to retrieve similar entries: \")\n",
    "            metric = input(\"Enter the metric (cosine, euclidean, mmr): \").strip().lower()\n",
    "            results = manager.retrieve_entries(query, k=10, metric=metric)\n",
    "            print(\"Retrieved entries:\")\n",
    "            for result in results:\n",
    "                print(result)\n",
    "        elif operation == 'update':\n",
    "            query = input(\"Enter the query text to find the entry to update: \")\n",
    "            manager.update_entry(query)\n",
    "        elif operation == 'delete':\n",
    "            query = input(\"Enter the query text to find the entry to delete: \")\n",
    "            manager.delete_entry(query)\n",
    "        elif operation == 'exit':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid operation. Please enter one of the following: save, retrieve, update, delete, exit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV...\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/TakeHomeAssignments/personal_assistant_LLM/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[17], line 3\u001b[0m\n    manager = EmbeddingManager(csv_file)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[16], line 12\u001b[0m in \u001b[1;35m__init__\u001b[0m\n    self.df = self.load_csv()\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[16], line 17\u001b[0m in \u001b[1;35mload_csv\u001b[0m\n    df['Embeddings'] = df['Embeddings'].apply(eval).apply(np.array)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/TakeHomeAssignments/personal_assistant_LLM/.venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m in \u001b[1;35mapply\u001b[0m\n    ).apply()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/TakeHomeAssignments/personal_assistant_LLM/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/TakeHomeAssignments/personal_assistant_LLM/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m in \u001b[1;35mapply_standard\u001b[0m\n    mapped = obj._map_values(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/TakeHomeAssignments/personal_assistant_LLM/.venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m in \u001b[1;35m_map_values\u001b[0m\n    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/TakeHomeAssignments/personal_assistant_LLM/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m in \u001b[1;35mmap_array\u001b[0m\n    return lib.map_infer(values, mapper, convert=convert)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32mlib.pyx:2972\u001b[0;36m in \u001b[0;35mpandas._libs.lib.map_infer\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [-6.13210090e-02 -9.63450372e-02  1.17624560e-02 -5.55313937e-02\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"/Users/rohithsiddharthareddy/Desktop/TakeHomeAssignments/personal_assistant_LLM/rohithsiddhartha/extracted_contents/all_chunks_and_embeddings.csv\"\n",
    "    manager = EmbeddingManager(csv_file)\n",
    "    \n",
    "    while True:\n",
    "        operation = input(\"Enter operation (save, retrieve, update, delete, exit): \").strip().lower()\n",
    "        \n",
    "        if operation == 'save':\n",
    "            new_text = input(\"Enter the text to save: \")\n",
    "            manager.save_entry(new_text)\n",
    "        elif operation == 'retrieve':\n",
    "            query = input(\"Enter the query text to retrieve similar entries: \")\n",
    "            metric = input(\"Enter the metric (cosine, euclidean, mmr): \").strip().lower()\n",
    "            results = manager.retrieve_entries(query, k=10, metric=metric)\n",
    "            print(\"Retrieved entries:\")\n",
    "            for result in results:\n",
    "                print(result)\n",
    "        elif operation == 'update':\n",
    "            query = input(\"Enter the query text to find the entry to update: \")\n",
    "            new_text = input(\"Enter the updated text: \")\n",
    "            manager.update_entry(query, new_text)\n",
    "        elif operation == 'delete':\n",
    "            query = input(\"Enter the query text to find the entry to delete: \")\n",
    "            manager.delete_entry(query)\n",
    "        elif operation == 'exit':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid operation. Please enter one of the following: save, retrieve, update, delete, exit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
